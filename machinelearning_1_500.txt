PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	PG	WC	SC	GA	UT	PMJ	Breiman, L				Breiman, L			Random forests	MACHINE LEARNING												Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.																	0885-6125	1573-0565				OCT	2001	45	1					5	32		10.1023/A:1010933404324						WOS:000170489900001		J	CORTES, C; VAPNIK, V				CORTES, C; VAPNIK, V			SUPPORT-VECTOR NETWORKS	MACHINE LEARNING												The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.																	0885-6125					SEP	1995	20	3					273	297		10.1023/A:1022627411411						WOS:A1995RX35400003		J	Burges, CJC				Burges, CJC			A tutorial on Support Vector Machines for pattern recognition	DATA MINING AND KNOWLEDGE DISCOVERY												The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.																	1384-5810					JUN	1998	2	2					121	167		10.1023/A:1009715923555						WOS:000076132400002		J	Haykin, S				Haykin, S			Cognitive radio: Brain-empowered wireless communications	IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS												Cognitive radio is viewed as a novel approach for improving the utilization of a precious natural resource: the radio electromagnetic spectrum. The cognitive radio, built on a software-defined radio, is defined as an intelligent wireless communication system that is aware of M environment and uses the methodology of understanding-by-building to learn from the environment and adapt to statistical variations in the input stimuli, with two primary objectives in mind: highly reliable communication whenever and wherever needed; efficient utilization of the radio spectrum. Following the discussion of interference temperature as a new metric for the quantification and management of interference, the paper addresses three fundamental cognitive tasks. 1) Radio-scene analysis. 2) Channel-state estimation and predictive modeling. 3) Transmit-power control and dynamic spectrum management. This paper also discusses the emergent behavior of cognitive radio.																	0733-8716	1558-0008				FEB	2005	23	2					201	220		10.1109/JSAC.2004.839380						WOS:000226943900002		J	Fawcett, T				Fawcett, Tom			An introduction to ROC analysis	PATTERN RECOGNITION LETTERS												Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research. (c) 2005 Elsevier B.V. All rights reserved.				Exarchos, Konstantinos/B-5978-2008; Proenca Jr., Mario Lemes/B-8340-2016	Proenca Jr., Mario Lemes/0000-0002-0492-322X												0167-8655	1872-7344				JUN	2006	27	8					861	874		10.1016/j.patrec.2005.10.010						WOS:000237462800002		J	Phillips, SJ; Anderson, RP; Schapire, RE				Phillips, SJ; Anderson, RP; Schapire, RE			Maximum entropy modeling of species geographic distributions	ECOLOGICAL MODELLING												The availability of detailed environmental data, together with inexpensive and powerful computers, has fueled a rapid increase in predictive modeling of species environmental requirements and geographic distributions. For some species, detailed presence/absence occurrence data are available, allowing the use of a variety of standard statistical techniques. However, absence data are not available for most species. In this paper, we introduce the use of the maximum entropy method (Maxent) for modeling species geographic distributions with presence-only data. Maxent is a general-purpose machine learning method with a simple and precise mathematical formulation, and it has a number of aspects that make it well-suited for species distribution modeling. In order to investigate the efficacy of the method, here we perform a continental-scale case study using two Neotropical mammals: a lowland species of sloth, Bradypus variegatus, and a small montane murid rodent, Microryzomys minutus. We compared Maxent predictions with those of a commonly used presence-only modeling method, the Genetic Algorithm for Rule-Set Prediction (GARP). We made predictions on 10 random subsets of the occurrence records for both species, and then used the remaining localities for testing. Both algorithms provided reasonable estimates of the species' range, far superior to the shaded outline maps available in field guides. All models were significantly better than random in both binomial tests of omission and receiver operating characteristic (ROC) analyses. The area under the ROC curve (AUC) was almost always higher for Maxent, indicating better discrimination of suitable versus unsuitable areas for the species. The Maxent modeling approach can be used in its present form for many applications with presence-only datasets, and merits further research and development. (c) 2005 Elsevier B.V. All rights reserved.																	0304-3800	1872-7026				JAN 25	2006	190	3-4					231	259		10.1016/j.ecolmodel.2005.03.026						WOS:000233859600001		B	Rasmussen, CE; Williams, CKI				Rasmussen, CE; Williams, CKI			Gaussian Processes for Machine Learning	GAUSSIAN PROCESSES FOR MACHINE LEARNING	Adaptive Computation and Machine Learning																														978-0-262-18253-9				2005							1	247								WOS:000279925100014		J	Elith, J; Graham, CH; Anderson, RP; Dudik, M; Ferrier, S; Guisan, A; Hijmans, RJ; Huettmann, F; Leathwick, JR; Lehmann, A; Li, J; Lohmann, LG; Loiselle, BA; Manion, G; Moritz, C; Nakamura, M; Nakazawa, Y; Overton, JM; Peterson, AT; Phillips, SJ; Richardson, K; Scachetti-Pereira, R; Schapire, RE; Soberon, J; Williams, S; Wisz, MS; Zimmermann, NE				Elith, J; Graham, CH; Anderson, RP; Dudik, M; Ferrier, S; Guisan, A; Hijmans, RJ; Huettmann, F; Leathwick, JR; Lehmann, A; Li, J; Lohmann, LG; Loiselle, BA; Manion, G; Moritz, C; Nakamura, M; Nakazawa, Y; Overton, JM; Peterson, AT; Phillips, SJ; Richardson, K; Scachetti-Pereira, R; Schapire, RE; Soberon, J; Williams, S; Wisz, MS; Zimmermann, NE			Novel methods improve prediction of species' distributions from occurrence data	ECOGRAPHY												Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.				Soberon, Jorge/N-7444-2015; Williams, Stephen/A-7250-2008; Graham, Catherine/A-9560-2011; Lehmann, Anthony/B-1544-2010; Moritz, Craig/A-7755-2012; Lohmann, Lucia/C-9492-2013; Peterson, A. Townsend/I-5697-2013; Wisz, Mary/J-7826-2013; Ferrier, Simon/C-1490-2009; Guisan, Antoine/A-1057-2011; Zimmermann, Niklaus/A-4276-2008; Elith, Jane/F-2022-2015; Hijmans, Robert/N-3299-2016; Loiselle, Bette/O-7106-2016; IB/USP, Botanica/Q-7627-2016	Soberon, Jorge/0000-0003-2160-4148; Williams, Stephen/0000-0002-2510-7408; Lohmann, Lucia/0000-0003-4960-0587; Peterson, A. Townsend/0000-0003-0243-2379; Ferrier, Simon/0000-0001-7884-2388; Guisan, Antoine/0000-0002-3998-4815; Zimmermann, Niklaus/0000-0003-3099-9604; Elith, Jane/0000-0002-8706-0326; Loiselle, Bette/0000-0003-1434-4173; Lehmann, Anthony/0000-0002-8279-8567; Wisz, Mary/0000-0002-5357-6367; Hijmans, Robert/0000-0001-5872-2872												0906-7590					APR	2006	29	2					129	151		10.1111/j.2006.0906-7590.04596.x						WOS:000236767000001		J	Lecun, Y; Bottou, L; Bengio, Y; Haffner, P				Lecun, Y; Bottou, L; Bengio, Y; Haffner, P			Gradient-based learning applied to document recognition	PROCEEDINGS OF THE IEEE												Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, arch as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so; as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.																	0018-9219	1558-2256				NOV	1998	86	11					2278	2324		10.1109/5.726791						WOS:000076557300010		J	Demsar, J				Demsar, J			Statistical comparisons of classifiers over multiple data sets	JOURNAL OF MACHINE LEARNING RESEARCH												While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD ( critical difference) diagrams.																	1532-4435					JAN	2006	7						1	30								WOS:000236331400001		J	DePristo, MA; Banks, E; Poplin, R; Garimella, KV; Maguire, JR; Hartl, C; Philippakis, AA; del Angel, G; Rivas, MA; Hanna, M; McKenna, A; Fennell, TJ; Kernytsky, AM; Sivachenko, AY; Cibulskis, K; Gabriel, SB; Altshuler, D; Daly, MJ				DePristo, Mark A.; Banks, Eric; Poplin, Ryan; Garimella, Kiran V.; Maguire, Jared R.; Hartl, Christopher; Philippakis, Anthony A.; del Angel, Guillermo; Rivas, Manuel A.; Hanna, Matt; McKenna, Aaron; Fennell, Tim J.; Kernytsky, Andrew M.; Sivachenko, Andrey Y.; Cibulskis, Kristian; Gabriel, Stacey B.; Altshuler, David; Daly, Mark J.			A framework for variation discovery and genotyping using next-generation DNA sequencing data	NATURE GENETICS												Recent advances in sequencing technology make it possible to comprehensively catalog genetic variation in population samples, creating a foundation for understanding human disease, ancestry and evolution. The amounts of raw data produced are prodigious, and many computational steps are required to translate this output into high-quality variant calls. We present a unified analytic framework to discover and genotype variation among multiple samples simultaneously that achieves sensitive and specific results across five sequencing technologies and three distinct, canonical experimental designs. Our process includes (i) initial read mapping; (ii) local realignment around indels; (iii) base quality score recalibration; (iv) SNP discovery and genotyping to find all potential variants; and (v) machine learning to separate true segregating variation from machine artifacts common to next-generation sequencing technologies. We here discuss the application of these tools, instantiated in the Genome Analysis Toolkit, to deep whole-genome, whole-exome capture and multi-sample low-pass (similar to 4x) 1000 Genomes Project datasets.				Sincan, Murat /A-3794-2010; Altshuler, David/A-4476-2009	Altshuler, David/0000-0002-7250-4107												1061-4036	1546-1718				MAY	2011	43	5					491	+		10.1038/ng.806						WOS:000289972600023	21478889	J	Jain, AK; Duin, RPW; Mao, JC				Jain, AK; Duin, RPW; Mao, JC			Statistical pattern recognition: A review	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning. selection of training and lest samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining. web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.																	0162-8828	1939-3539				JAN	2000	22	1					4	37		10.1109/34.824819						WOS:000085472300002		S	Viola, P; Jones, M		Jacobs, A; Baldwin, T		Viola, P; Jones, M			Rapid object detection using a boosted cascade of simple features	2001 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, VOL 1, PROCEEDINGS	PROCEEDINGS - IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION				Conference on Computer Vision and Pattern Recognition	DEC 08-14, 2001	KAUAI, HI	IEEE Comp Soc				This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "Integral Image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[5]. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.				anzhi, yue/A-8609-2012													1063-6919		0-7695-1272-0				2001							511	518								WOS:000184694200067		J	Belkin, M; Niyogi, P				Belkin, M; Niyogi, P			Laplacian eigenmaps for dimensionality reduction and data representation	NEURAL COMPUTATION												One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.																	0899-7667					JUN	2003	15	6					1373	1396		10.1162/089976603321780317						WOS:000182530600005		J	Huang, GB; Zhu, QY; Siew, CK				Huang, Guang-Bin; Zhu, Qin-Yu; Siew, Chee-Kheong			Extreme learning machine: Theory and applications	NEUROCOMPUTING					8th Brazilian Symposium on Neural Networks	SEP 29-OCT 01, 2004	Sao Luis, BRAZIL	Brazilian Comp Soc, Int Neural Network Soc, SIG INNS Brazil Special Interest Grp				It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.(I) (c) 2006 Elsevier B.V. All rights reserved.				Huang, Guang-Bin/A-5035-2011; Siew, CK/A-5082-2011	Huang, Guang-Bin/0000-0002-2480-4965; 												0925-2312	1872-8286				DEC	2006	70	1-3					489	501		10.1016/j.neucom.2005.12.126						WOS:000242602300047		J	Hinton, GE; Osindero, S; Teh, YW				Hinton, Geoffrey E.; Osindero, Simon; Teh, Yee-Whye			A fast learning algorithm for deep belief nets	NEURAL COMPUTATION												We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.				Teh, Yee Whye/C-3400-2008; anzhi, yue/A-8609-2012													0899-7667					JUL	2006	18	7					1527	1554		10.1162/neco.2006.18.7.1527						WOS:000237698100002	16764513	J	Chang, CC; Lin, CJ				Chang, Chih-Chung; Lin, Chih-Jen			LIBSVM: A Library for Support Vector Machines	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY												LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.					Lin, Chih-Jen/0000-0003-4684-8747												2157-6904	2157-6912					2011	2	3			SI				27	10.1145/1961189.1961199						WOS:000208617000010		J	Smola, AJ; Scholkopf, B				Smola, AJ; Scholkopf, B			A tutorial on support vector regression	STATISTICS AND COMPUTING												In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.				Scholkopf, Bernhard/A-7570-2013													0960-3174	1573-1375				AUG	2004	14	3					199	222		10.1023/B:STCO.0000035301.49549.88						WOS:000222770200003		J	Sebastiani, F				Sebastiani, F			Machine learning in automated text categorization	ACM COMPUTING SURVEYS												The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.				Sebastiani, Fabrizio/C-9501-2015	Sebastiani, Fabrizio/0000-0003-4221-6427												0360-0300	1557-7341				MAR	2002	34	1					1	47		10.1145/505282.505283						WOS:000175267600001		J	Tipping, ME				Tipping, ME			Sparse Bayesian learning and the relevance vector machine	JOURNAL OF MACHINE LEARNING RESEARCH												This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classification tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the 'relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art 'support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages. These include the benefits of probabilistic predictions, automatic estimation of 'nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-'Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We offer some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning.																	1532-4435					SUM	2001	1	3					211	244		10.1162/15324430152748236						WOS:000173336900003		J	Pedregosa, F; Varoquaux, G; Gramfort, A; Michel, V; Thirion, B; Grisel, O; Blondel, M; Prettenhofer, P; Weiss, R; Dubourg, V; Vanderplas, J; Passos, A; Cournapeau, D; Brucher, M; Perrot, M; Duchesnay, E				Pedregosa, Fabian; Varoquaux, Gaeel; Gramfort, Alexandre; Michel, Vincent; Thirion, Bertrand; Grisel, Olivier; Blondel, Mathieu; Prettenhofer, Peter; Weiss, Ron; Dubourg, Vincent; Vanderplas, Jake; Passos, Alexandre; Cournapeau, David; Brucher, Matthieu; Perrot, Matthieu; Duchesnay, Edouard			Scikit-learn: Machine Learning in Python	JOURNAL OF MACHINE LEARNING RESEARCH												Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.				Louppe, Gilles/D-1923-2017	Louppe, Gilles/0000-0002-2082-3106; Varoquaux, Gael/0000-0003-1076-5122												1532-4435					OCT	2011	12						2825	2830								WOS:000298103200003		J	Friedman, J; Hastie, T; Tibshirani, R				Friedman, J; Hastie, T; Tibshirani, R			Additive logistic regression: A statistical view of boosting	ANNALS OF STATISTICS												Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted Versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.				anzhi, yue/A-8609-2012													0090-5364					APR	2000	28	2					337	374		10.1214/aos/1016218223						WOS:000089669700001		J	Fan, RE; Chang, KW; Hsieh, CJ; Wang, XR; Lin, CJ				Fan, Rong-En; Chang, Kai-Wei; Hsieh, Cho-Jui; Wang, Xiang-Rui; Lin, Chih-Jen			LIBLINEAR: A Library for Large Linear Classification	JOURNAL OF MACHINE LEARNING RESEARCH												LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.					Lin, Chih-Jen/0000-0003-4684-8747												1532-4435					AUG	2008	9						1871	1874								WOS:000262636800009		J	Kaelbling, LP; Littman, ML; Moore, AW				Kaelbling, LP; Littman, ML; Moore, AW			Reinforcement learning: A survey	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ''reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.																	1076-9757						1996	4						237	285								WOS:A1996UJ31200001		J	Krogan, NJ; Cagney, G; Yu, HY; Zhong, GQ; Guo, XH; Ignatchenko, A; Li, J; Pu, SY; Datta, N; Tikuisis, AP; Punna, T; Peregrin-Alvarez, JM; Shales, M; Zhang, X; Davey, M; Robinson, MD; Paccanaro, A; Bray, JE; Sheung, A; Beattie, B; Richards, DP; Canadien, V; Lalev, A; Mena, F; Wong, P; Starostine, A; Canete, MM; Vlasblom, J; Wu, S; Orsi, C; Collins, SR; Chandran, S; Haw, R; Rilstone, JJ; Gandi, K; Thompson, NJ; Musso, G; St Onge, P; Ghanny, S; Lam, MHY; Butland, G; Altaf-Ui, AM; Kanaya, S; Shilatifard, A; O'Shea, E; Weissman, JS; Ingles, CJ; Hughes, TR; Parkinson, J; Gerstein, M; Wodak, SJ; Emili, A; Greenblatt, JF				Krogan, NJ; Cagney, G; Yu, HY; Zhong, GQ; Guo, XH; Ignatchenko, A; Li, J; Pu, SY; Datta, N; Tikuisis, AP; Punna, T; Peregrin-Alvarez, JM; Shales, M; Zhang, X; Davey, M; Robinson, MD; Paccanaro, A; Bray, JE; Sheung, A; Beattie, B; Richards, DP; Canadien, V; Lalev, A; Mena, F; Wong, P; Starostine, A; Canete, MM; Vlasblom, J; Wu, S; Orsi, C; Collins, SR; Chandran, S; Haw, R; Rilstone, JJ; Gandi, K; Thompson, NJ; Musso, G; St Onge, P; Ghanny, S; Lam, MHY; Butland, G; Altaf-Ui, AM; Kanaya, S; Shilatifard, A; O'Shea, E; Weissman, JS; Ingles, CJ; Hughes, TR; Parkinson, J; Gerstein, M; Wodak, SJ; Emili, A; Greenblatt, JF			Global landscape of protein complexes in the yeast Saccharomyces cerevisiae	NATURE												Identification of protein - protein interactions often provides insight into protein function, and many cellular processes are performed by stable protein complexes. We used tandem affinity purification to process 4,562 different tagged proteins of the yeast Saccharomyces cerevisiae. Each preparation was analysed by both matrix-assisted laser desorption/ ionization - time of flight mass spectrometry and liquid chromatography tandem mass spectrometry to increase coverage and accuracy. Machine learning was used to integrate the mass spectrometry scores and assign probabilities to the protein - protein interactions. Among 4,087 different proteins identified with high confidence by mass spectrometry from 2,357 successful purifications, our core data set ( median precision of 0.69) comprises 7,123 protein - protein interactions involving 2,708 proteins. A Markov clustering algorithm organized these interactions into 547 protein complexes averaging 4.9 subunits per complex, about half of them absent from the MIPS database, as well as 429 additional interactions between pairs of complexes. The data ( all of which are available online) will help future studies on individual proteins as well as functional genomics and systems biology.				Parkinson, John/A-4424-2008; Cagney, Gerard/A-4648-2009; Haw, Robin/D-1393-2009; Robinson, Mark/A-6432-2015	Parkinson, John/0000-0001-9815-1189; Haw, Robin/0000-0002-2013-7835; Robinson, Mark/0000-0002-3048-5518; Peregrin-Alvarez, Jose M/0000-0002-7184-832X; Collins, Sean/0000-0002-4276-5840; Cagney, Gerard/0000-0001-7189-9496; Ignatchenko, Alexandr/0000-0002-6083-941X; Tikuisis, Aaron/0000-0001-7326-382X												0028-0836					MAR 30	2006	440	7084					637	643		10.1038/nature04670						WOS:000236350400036	16554755	J	Xu, R; Wunsch, D				Xu, R; Wunsch, D			Survey of clustering algorithms	IEEE TRANSACTIONS ON NEURAL NETWORKS												Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.																	1045-9227	1941-0093				MAY	2005	16	3					645	678		10.1109/TNN.2005.845141						WOS:000228909900013	15940994	J	Muller, KR; Mika, S; Ratsch, G; Tsuda, K; Scholkopf, B				Muller, KR; Mika, S; Ratsch, G; Tsuda, K; Scholkopf, B			An introduction to kernel-based learning algorithms	IEEE TRANSACTIONS ON NEURAL NETWORKS												This paper provides an introduction to support vector machines (SVMs), kernel Fisher discriminant analysis, and kernel principal component analysis (PCA), as examples for successful kernel-based learning methods, We first give a short background about Vapnik-Chervonenkis (VC) theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by finally discussing applications such as optical character recognition (OCR) and DNA analysis.				Ratsch, Gunnar/B-8182-2009; Muller, Klaus/C-3196-2013; Scholkopf, Bernhard/A-7570-2013													1045-9227	1941-0093				MAR	2001	12	2					181	201		10.1109/72.914517						WOS:000167886700001	18244377	J	Vapnik, VN				Vapnik, VN			An overview of statistical learning theory	IEEE TRANSACTIONS ON NEURAL NETWORKS												Statistical learning theory was introduced in the late 1960's, Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory mere proposed, This made statistical learning theory not only a tool for the theoretical analysis hut also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. ih more detailed overview of the theory (without proofs) can be found in Vapnik (1995), In Vapnik (1998) one can find detailed description of the theory (including proofs).				Karayel, Bora/E-2173-2017	Karayel, Bora/0000-0001-5997-1424												1045-9227					SEP	1999	10	5					988	999		10.1109/72.788640						WOS:000082372100002	18252602	J	COOPER, GF; HERSKOVITS, E				COOPER, GF; HERSKOVITS, E			A BAYESIAN METHOD FOR THE INDUCTION OF PROBABILISTIC NETWORKS FROM DATA	MACHINE LEARNING												This paper presents a Bayesian method for constructing probabilistic networks from databases. In particular, we focus on constructing Bayesian belief networks. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabilistic expert systems. We extend the basic method to handle missing data and hidden (latent) variables. We show how to perform probabilistic inference by averaging over the inferences of multiple belief networks. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. Finally, we relate the methods in this paper to previous work, and we discuss open problems.																	0885-6125					OCT	1992	9	4					309	347		10.1023/A:1022649401552						WOS:A1992JQ51100002		J	Rowley, HA; Baluja, S; Kanade, T				Rowley, HA; Baluja, S; Kanade, T			Neural network-based face detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.				anzhi, yue/A-8609-2012													0162-8828	1939-3539				JAN	1998	20	1					23	38		10.1109/34.655647						WOS:000071872400003		S	Witten, IH; Frank, E; Hall, MA				Witten, IH; Frank, E; Hall, MA			Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition	DATA MINING: PRACTICAL MACHINE LEARNING TOOLS AND TECHNIQUES, 3RD EDITION	Morgan Kaufmann Series in Data Management Systems																												1046-1698		978-0-08-089036-4; 978-0-12-374856-0				2011							1	629								WOS:000316848800019		J	Bradley, AP				Bradley, AP			The use of the area under the roc curve in the evaluation of machine learning algorithms	PATTERN RECOGNITION												In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six ''real world'' medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for ''single number'' evaluation of machine learning algorithms. (C) 1997 Pattern Recognition Society.				Bradley, Andrew/C-5685-2009	Bradley, Andrew/0000-0003-0109-6844												0031-3203					JUL	1997	30	7					1145	1159		10.1016/S0031-3203(96)00142-2						WOS:A1997XE56500009		J	Yang, MH; Kriegman, DJ; Ahuja, N				Yang, MH; Kriegman, DJ; Ahuja, N			Detecting faces in images: A survey	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Images containing faces are essential to intelligent vision-based human computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation, and expression recognition, However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face regardless of its three-dimensional position, orientation, and lighting conditions, Such a problem is challenging because faces a-re nonrigid and have a high degree of variability in size, shape, color, and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics, and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research.				Pang, Angeline/C-7710-2011													0162-8828	1939-3539				JAN	2002	24	1					34	58		10.1109/34.982883						WOS:000172960300003		J	Everingham, M; Van Gool, L; Williams, CKI; Winn, J; Zisserman, A				Everingham, Mark; Van Gool, Luc; Williams, Christopher K. I.; Winn, John; Zisserman, Andrew			The Pascal Visual Object Classes (VOC) Challenge	INTERNATIONAL JOURNAL OF COMPUTER VISION												The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.																	0920-5691	1573-1405				JUN 10	2010	88	2			SI		303	338		10.1007/s11263-009-0275-4						WOS:000275955400009		J	Shipp, MA; Ross, KN; Tamayo, P; Weng, AP; Kutok, JL; Aguiar, RCT; Gaasenbeek, M; Angelo, M; Reich, M; Pinkus, GS; Ray, TS; Koval, MA; Last, KW; Norton, A; Lister, TA; Mesirov, J; Neuberg, DS; Lander, ES; Aster, JC; Golub, TR				Shipp, MA; Ross, KN; Tamayo, P; Weng, AP; Kutok, JL; Aguiar, RCT; Gaasenbeek, M; Angelo, M; Reich, M; Pinkus, GS; Ray, TS; Koval, MA; Last, KW; Norton, A; Lister, TA; Mesirov, J; Neuberg, DS; Lander, ES; Aster, JC; Golub, TR			Diffuse large B-cell lymphoma outcome prediction by gene-expression profiling and supervised machine learning	NATURE MEDICINE												Diffuse large B-cell lymphoma (DLBCL), the most common lymphoid malignancy in adults, is curable in less than 50% of patients. Prognostic models based on pre-treatment characteristics, such as the International Prognostic Index (IPI), are currently used to predict outcome in DLBCL. However, clinical outcome models identify neither the molecular basis of clinical heterogeneity, nor specific therapeutic targets. We analyzed the expression of 6,817 genes in diagnostic tumor specimens from DLBCL patients who received cyclophosphamide, adriamycin, vincristine and prednisone (CHOP)-based chemotherapy, and applied a supervised learning prediction method to identify cured versus fatal or refractory disease. The algorithm classified two categories of patients with very different five-year overall survival rates (70% versus 12%). The model also effectively delineated patients within specific IPI risk categories who were likely to be cured or to die of their disease. Genes implicated in DLBCL outcome included some that regulate responses to B-cell-receptor signaling, critical serine/threonine phosphorylation pathways and apoptosis. Our data indicate that supervised learning classification techniques can predict outcome in DLBCL and identify rational targets for intervention.				Weng, Andrew/I-5015-2014													1078-8956					JAN	2002	8	1					68	74		10.1038/nm0102-68						WOS:000173056900033	11786909	J	Brown, MPS; Grundy, WN; Lin, D; Cristianini, N; Sugnet, CW; Furey, TS; Ares, M; Haussler, D				Brown, MPS; Grundy, WN; Lin, D; Cristianini, N; Sugnet, CW; Furey, TS; Ares, M; Haussler, D			Knowledge-based analysis of microarray gene expression data by using support vector machines	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA												We introduce a method of functionally classifying genes by using gene expression data from DNA microarray hybridization experiments. The method is based on the theory of support vector machines (SVMs). SVMs are considered a supervised computer learning method because they exploit prior knowledge of gene function to identify unknown genes of similar function from expression data. SVMs avoid several problems associated with unsupervised clustering methods, such as hierarchical clustering and self-organizing maps. SVMs have many mathematical features that make them attractive for gene expression analysis, including their flexibility in choosing a similarity function, sparseness of solution when dealing with large data sets, the ability to handle large feature spaces, and the ability to identify outliers. We test several SVMs that use different similarity metrics, as well as some other supervised learning methods, and find that the SVMs best identify sets of genes with a common function using expression data. Finally, we use SVMs to predict functional roles for uncharacterized yeast ORFs based on their expression data.					Ares, Manny/0000-0002-2552-9168												0027-8424					JAN 4	2000	97	1					262	267		10.1073/pnas.97.1.262						WOS:000084624500049	10618406	J	Dudoit, S; Fridlyand, J; Speed, TP				Dudoit, S; Fridlyand, J; Speed, TP			Comparison of discrimination methods for the classification of tumors using gene expression data	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION												A reliable and precise classification of tumors is essential for successful diagnosis and treatment of cancer. cDNA microarrays and high-density oligonucleotide chips are novel biotechnologies increasingly used in cancer research. By allowing the monitoring of expression levels in cells for thousands of genes simultaneously, microarray experiments may lead to a more complete understanding of the molecular variations among tumors and hence to a finer and more informative classification. The ability to successfully distinguish between tumor classes (already known or yet to be discovered) using gene expression data is an important aspect of this novel approach to cancer classification. This article compares the performance of different discrimination methods for the classification of tumors based on gene expression data. The methods include nearest-neighbor classifiers, linear discriminant analysis, and classification trees. Recent machine learning approaches, such as bagging and boosting, are also considered. The discrimination methods are applied to datasets from three recently published cancer gene expression studies.				Speed, Terence /B-8085-2009	Speed, Terence /0000-0002-5403-7998												0162-1459					MAR	2002	97	457					77	87		10.1198/016214502753479248						WOS:000173997500008		J	Saeys, Y; Inza, I; Larranaga, P				Saeys, Yvan; Inza, Inaki; Larranaga, Pedro			A review of feature selection techniques in bioinformatics	BIOINFORMATICS												Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.				Saeys, Yvan/C-1311-2009; Larranaga, Pedro/F-9293-2013	Saeys, Yvan/0000-0002-0415-1506; Larranaga, Pedro/0000-0003-0652-9872; INZA CANO, INAKI/0000-0003-4674-1755												1367-4803					OCT 1	2007	23	19					2507	2517		10.1093/bioinformatics/btm344						WOS:000250673800001	17720704	J	Kanungo, T; Mount, DM; Netanyahu, NS; Piatko, CD; Silverman, R; Wu, AY				Kanungo, T; Mount, DM; Netanyahu, NS; Piatko, CD; Silverman, R; Wu, AY			An efficient k-means clustering algorithm: Analysis and implementation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												In k-means clustering, we are given a set of n data points in d-dimensional space R-d and an integer k and the problem is to determine a set of k points in R-d, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's algorithm. In this paper, we present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation.				gong, junie/G-5880-2010; Piatko, Christine/H-3422-2013													0162-8828					JUL	2002	24	7					881	892		10.1109/TPAMI.2002.1017616						WOS:000176446100002		J	Elith, J; Phillips, SJ; Hastie, T; Dudik, M; Chee, YE; Yates, CJ				Elith, Jane; Phillips, Steven J.; Hastie, Trevor; Dudik, Miroslav; Chee, Yung En; Yates, Colin J.			A statistical explanation of MaxEnt for ecologists	DIVERSITY AND DISTRIBUTIONS												MaxEnt is a program for modelling species distributions from presence-only species records. This paper is written for ecologists and describes the MaxEnt model from a statistical perspective, making explicit links between the structure of the model, decisions required in producing a modelled distribution, and knowledge about the species and the data that might affect those decisions. To begin we discuss the characteristics of presence-only data, highlighting implications for modelling distributions. We particularly focus on the problems of sample bias and lack of information on species prevalence. The keystone of the paper is a new statistical explanation of MaxEnt which shows that the model minimizes the relative entropy between two probability densities (one estimated from the presence data and one, from the landscape) defined in covariate space. For many users, this viewpoint is likely to be a more accessible way to understand the model than previous ones that rely on machine learning concepts. We then step through a detailed explanation of MaxEnt describing key components (e.g. covariates and features, and definition of the landscape extent), the mechanics of model fitting (e.g. feature selection, constraints and regularization) and outputs. Using case studies for a Banksia species native to south-west Australia and a riverine fish, we fit models and interpret them, exploring why certain choices affect the result and what this means. The fish example illustrates use of the model with vector data for linear river segments rather than raster (gridded) data. Appropriate treatments for survey bias, unprojected data, locally restricted species, and predicting to environments outside the range of the training data are demonstrated, and new capabilities discussed. Online appendices include additional details of the model and the mathematical links between previous explanations and this one, example code and data, and further information on the case studies.				Yates, Colin/B-5972-2011; Elith, Jane/F-2022-2015	Elith, Jane/0000-0002-8706-0326; Chee, Yung En/0000-0002-8478-373X												1366-9516	1472-4642				JAN	2011	17	1					43	57		10.1111/j.1472-4642.2010.00725.x						WOS:000285246700005		J	GEMAN, S; BIENENSTOCK, E; DOURSAT, R				GEMAN, S; BIENENSTOCK, E; DOURSAT, R			NEURAL NETWORKS AND THE BIAS VARIANCE DILEMMA	NEURAL COMPUTATION												Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feed-forward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.																	0899-7667	1530-888X				JAN	1992	4	1					1	58		10.1162/neco.1992.4.1.1						WOS:A1992HD97700001		J	Griffiths, TL; Steyvers, M				Griffiths, TL; Steyvers, M			Finding scientific topics	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA					Colloquium on Mapping Knowledge Domains	MAY 09-11, 2003	Natl Acad Sci & Engn, Arnold & Mable Beckman Ctr, Irvine, CA		Natl Acad Sci & Engn, Arnold & Mable Beckman Ctr			A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. & Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying "hot topics" by examining temporal dynamics and tagging abstracts to illustrate semantic content.																	0027-8424					APR 6	2004	101			1			5228	5235		10.1073/pnas.0307752101						WOS:000220823000008	14872004	J	Furey, TS; Cristianini, N; Duffy, N; Bednarski, DW; Schummer, M; Haussler, D				Furey, TS; Cristianini, N; Duffy, N; Bednarski, DW; Schummer, M; Haussler, D			Support vector machine classification and validation of cancer tissue samples using microarray expression data	BIOINFORMATICS												Motivation: DNA microarray experiments generating thousands of gene expression measurements, are being used to gather information from tissue and cell samples regarding gene expression differences that will be useful in diagnosing disease. We have developed a new method to analyse this kind of data using support vector machines (SVMs). This analysis consists of both classification of the tissue samples, and an exploration of the data for mis-labeled or questionable tissue results. Results: We demonstrate the method in detail on samples consisting of ovarian cancer tissues, normal ovarian tissues, and other normal tissues. The dataset consists of expression experiment results for 97 802 cDNAs for each tissue. As a result of computational analysis, a tissue sample is discovered and confirmed to be wrongly labeled Upon correction of this mistake and the removal of an outlier perfect classification of tissues is achieved, but not with high confidence. We identify and analyse a subset of genes from the ovarian dataset whose expression is highly differentiated between the types of tissues. To show robustness of the SVM method, two previously published datasets from other types of tissues or cells are analysed The results are comparable to those previously obtained. We show that other machine learning methods also perform comparably to the SVM on many of those datasets.																	1367-4803					OCT	2000	16	10					906	914		10.1093/bioinformatics/16.10.906						WOS:000165954000006	11120680	J	Rahm, E; Bernstein, PA				Rahm, E; Bernstein, PA			A survey of approaches to automatic schema matching	VLDB JOURNAL												Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.																	1066-8888	0949-877X				DEC	2001	10	4					334	350		10.1007/s007780100057						WOS:000173202400005		J	Belkin, M; Niyogi, P; Sindhwani, V				Belkin, Mikhail; Niyogi, Partha; Sindhwani, Vikas			Manifold regularization: A geometric framework for learning from labeled and unlabeled examples	JOURNAL OF MACHINE LEARNING RESEARCH												We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result ( in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.																	1532-4435					NOV	2006	7						2399	2434								WOS:000245390700005		J	Elith, J; Leathwick, JR; Hastie, T				Elith, J.; Leathwick, J. R.; Hastie, T.			A working guide to boosted regression trees	JOURNAL OF ANIMAL ECOLOGY												1. Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2. This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3. Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4. The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.				Elith, Jane/F-2022-2015	Elith, Jane/0000-0002-8706-0326												0021-8790	1365-2656				JUL	2008	77	4					802	813		10.1111/j.1365-2656.2008.01390.x						WOS:000256539800020	18397250	J	Huang, GB; Zhou, HM; Ding, XJ; Zhang, R				Huang, Guang-Bin; Zhou, Hongming; Ding, Xiaojian; Zhang, Rui			Extreme Learning Machine for Regression and Multiclass Classification	IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART B-CYBERNETICS												Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the "generalized" single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.				Huang, Guang-Bin/A-5035-2011	Huang, Guang-Bin/0000-0002-2480-4965												1083-4419	1941-0492				APR	2012	42	2			SI		513	529		10.1109/TSMCB.2011.2168604						WOS:000302097000018	21984515	J	Blum, AL; Langley, P				Blum, AL; Langley, P			Selection of relevant features and examples in machine learning	ARTIFICIAL INTELLIGENCE												In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples, We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area, (C) 1997 Elsevier Science B.V.																	0004-3702					DEC	1997	97	1-2					245	271		10.1016/S0004-3702(97)00063-5						WOS:000071321500008		J	ACKLEY, DH; HINTON, GE; SEJNOWSKI, TJ				ACKLEY, DH; HINTON, GE; SEJNOWSKI, TJ			A LEARNING ALGORITHM FOR BOLTZMANN MACHINES	COGNITIVE SCIENCE																													0364-0213						1985	9	1					147	169								WOS:A1985AFZ2900007		J	He, HB; Garcia, EA				He, Haibo; Garcia, Edwardo A.			Learning from Imbalanced Data	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.																	1041-4347	1558-2191				SEP	2009	21	9					1263	1284		10.1109/TKDE.2008.239						WOS:000268062400003		J	Pan, SJ; Yang, QA				Pan, Sinno Jialin; Yang, Qiang			A Survey on Transfer Learning	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.				PAN, Sinno Jialin/P-6696-2014; Hajra, Suvadeep/L-8460-2015	PAN, Sinno Jialin/0000-0001-6565-3836; Yang, Qiang/0000-0001-5059-8360												1041-4347					OCT	2010	22	10					1345	1359		10.1109/TKDE.2009.191						WOS:000281000500001		J	de Castro, LN; Von Zuben, FJ				de Castro, LN; Von Zuben, FJ			Learning and optimization using the clonal selection principle	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION												The clonal selection principle is used to explain the basic features of an adaptive immune response to an antigenic stimulus. It establishes the idea that only those cells that recognize the antigens (Ag's) are selected to proliferate. The selected cells are subject to an affinity maturation process, which improves their affinity to the selective Ag's. This paper proposes a computational implementation of the clonal selection principle that explicitly takes into account the affinity maturation of the immune response. The general algorithm, named CLONALG, is derived primarily to perform machine-learning and pattern-recognition tasks and then it is adapted to solve optimization problems, emphasizing multimodal and combinatorial optimization. Two versions of the algorithm are derived, their computational cost per iteration is presented, and a sensitivity analysis in relation to the user-defined parameters is given. CLONALG is also contrasted with evolutionary algorithms. Several benchmark problems are considered to evaluate the performance of CLONALG and it is also compared to a niching method for multimodal function optimization.				de Castro, Leandro/D-7531-2015	de Castro, Leandro/0000-0003-3409-4589												1089-778X					JUN	2002	6	3					239	251	PII S1089-778X(02)06065-4	10.1109/TEVC.2002.1011539						WOS:000176340500003		J	Amari, S				Amari, S			Natural gradient works efficiently in learning	NEURAL COMPUTATION												When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.				Amari, Shun-ichi/A-5901-2016													0899-7667	1530-888X				FEB 15	1998	10	2					251	276		10.1162/089976698300017746						WOS:000071831400001		J	Plamondon, R; Srihari, SN				Plamondon, R; Srihari, SN			On-line and off-line handwriting recognition: A comprehensive survey	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction of new technologies. Given its ubiquity in human transactions, machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, in postal addresses on envelopes, in amounts in bank checks, in handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms. Bath the on-line case (which pertains to the availability of trajectory data during writing) and the off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Other fields of application, like signature verification, writer authentification, handwriting learning tools are also considered.				Plamondon, Rejean/O-3214-2015	Plamondon, Rejean/0000-0002-4903-7539												0162-8828	1939-3539				JAN	2000	22	1					63	84		10.1109/34.824821						WOS:000085472300004		S	Dietterich, TG		Kittler, J; Roli, F		Dietterich, TG			Ensemble methods in machine learning	MULTIPLE CLASSIFIER SYSTEMS	LECTURE NOTES IN COMPUTER SCIENCE				1st International Workshop on Multiple Classifier Systems (MCS 2000)	JUN 21-23, 2000	CAGLIARI, ITALY	Univ Cagliari, Dept Elect & Electr Engn, Int Assoc Pattern Recognit				Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.																	0302-9743		3-540-67704-6				2000	1857						1	15								WOS:000165404700001		J	Cutler, DR; Edwards, TC; Beard, KH; Cutler, A; Hess, KT				Cutler, D. Richard; Edwards, Thomas C., Jr.; Beard, Karen H.; Cutler, Adele; Hess, Kyle T.			Random forests for classification in ecology	ECOLOGY												Classification procedures are some of the most widely used statistical methods in ecology. Random forests (RF) is a new and powerful statistical classifier that is well established in other disciplines but is relatively unknown in ecology. Advantages of RF compared to other statistical classifiers include (1) very high classification accuracy; (2) a novel method of determining variable importance; (3) ability to model complex interactions among predictor variables; (4) flexibility to perform several types of statistical data analysis, including regression, classification, survival analysis, and unsupervised learning; and (5) an algorithm for imputing missing values. We compared the accuracies of RF and four other commonly used statistical classifiers using data on invasive plant species presence in Lava Beds National Monument, California, USA, rare lichen species presence in the Pacific Northwest, USA, and nest sites for cavity nesting birds in the Uinta Mountains, Utah, USA. We observed high classification accuracy in all applications as measured by cross-validation and, in the case of the lichen data, by independent test data, when comparing RF to other common classification methods. We also observed that the variables that RF identified as most important for classifying invasive plant species coincided with expectations based on the literature.				Beard, Karen/B-7177-2011													0012-9658					NOV	2007	88	11					2783	2792		10.1890/07-0539.1						WOS:000251067900015	18051647	J	Pagliarini, DJ; Calvo, SE; Chang, B; Sheth, SA; Vafai, SB; Ong, SE; Walford, GA; Sugiana, C; Boneh, A; Chen, WK; Hill, DE; Vidal, M; Evans, JG; Thorburn, DR; Carr, SA; Mootha, VK				Pagliarini, David J.; Calvo, Sarah E.; Chang, Betty; Sheth, Sunil A.; Vafai, Scott B.; Ong, Shao-En; Walford, Geoffrey A.; Sugiana, Canny; Boneh, Avihu; Chen, William K.; Hill, David E.; Vidal, Marc; Evans, James G.; Thorburn, David R.; Carr, Steven A.; Mootha, Vamsi K.			A mitochondrial protein compendium elucidates complex I disease biology	CELL												Mitochondria are complex organelles whose dysfunction underlies a broad spectrum of human diseases. Identifying all of the proteins resident in this organelle and understanding how they integrate into pathways represent major challenges in cell biology. Toward this goal, we performed mass spectrometry, GFP tagging, and machine learning to create a mitochondrial compendium of 1098 genes and their protein expression across 14 mouse tissues. We link poorly characterized proteins in this inventory to known mitochondrial pathways by virtue of shared evolutionary history. Using this approach, we predict 19 proteins to be important for the function of complex I (CI) of the electron transport chain. Wevalidate a subset of these predictions using RNAi, including C8orf38, which we further show harbors an inherited mutation in a lethal, infantile CI deficiency. Our results have important implications for understanding CI function and pathogenesis and, more generally, illustrate how our compendium can serve as a foundation for systematic investigations of mitochondria.				Hill, David/B-6617-2011; Thorburn, David/G-6266-2013	thorburn, david/0000-0002-7725-9470; Pagliarini, Dave/0000-0002-0001-0087												0092-8674					JUL 11	2008	134	1					112	123		10.1016/j.cell.2008.06.016						WOS:000257513100018	18614015	J	Pawlak, Z; Skowron, A				Pawlak, Zdzislaw; Skowron, Andrzej			Rudiments of rough sets	INFORMATION SCIENCES												Worldwide, there has been a rapid growth in interest in rough set theory and its applications in recent years. Evidence of this can be found in the increasing number of high-quality articles on rough sets and related topics that have been published in a variety of international journals, symposia, workshops, and international conferences in recent years. In addition, many international workshops and conferences have included special sessions on the theory and applications of rough sets in their programs. Rough set theory has led to many interesting applications and extensions. It seems that the rough set approach is fundamentally important in artificial intelligence and cognitive sciences, especially in research areas such as machine learning, intelligent systems, inductive reasoning, pattern recognition, mereology, knowledge discovery, decision analysis, and expert systems. In the article, we present the basic concepts of rough set theory and point out some rough set-based research directions and applications. (c) 2006 Elsevier Inc. All rights reserved.				Smith, Barry/A-9525-2011	Smith, Barry/0000-0003-1384-116X												0020-0255	1872-6291				JAN 1	2007	177	1					3	27		10.1016/j.ins.2006.06.003						WOS:000242194600002		J	Maass, W; Natschlager, T; Markram, H				Maass, W; Natschlager, T; Markram, H			Real-time computing without stable states: A new framework for neural computation based on perturbations	NEURAL COMPUTATION												A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.																	0899-7667					NOV	2002	14	11					2531	2560		10.1162/089976602760407955						WOS:000178882900001	12433288	J	Lanckriet, GRG; Cristianini, N; Bartlett, P; El Ghaoui, L; Jordan, MI				Lanckriet, GRG; Cristianini, N; Bartlett, P; El Ghaoui, L; Jordan, MI			Learning the kernel matrix with semidefinite programming	JOURNAL OF MACHINE LEARNING RESEARCH												Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space-classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm- using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem.																	1532-4435					JAN	2004	5						27	72								WOS:000236326900001		J	LeCun, Y; Bengio, Y; Hinton, G				LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey			Deep learning	NATURE												Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.																	0028-0836	1476-4687				MAY 28	2015	521	7553					436	444		10.1038/nature14539						WOS:000355286600030	26017442	J	Kall, L; Krogh, A; Sonnhammer, ELL				Kall, L; Krogh, A; Sonnhammer, ELL			A combined transmembrane topology and signal peptide prediction method	JOURNAL OF MOLECULAR BIOLOGY												An inherent problem in transmembrane protein topology prediction and signal peptide prediction is the high similarity between the hydrophobic regions of a transmembrane helix and that of a signal peptide, leading to cross-reaction between the two types of predictions. To improve predictions further, it is therefore important to make a predictor that aims to discriminate between the two classes. In addition, topology information can be gained when successfully predicting a signal Peptide leading a trans' membrane protein since it dictates that the N terminus of the mature protein must be on the non-cytoplasmic side of the membrane. Here, we present Phobius, a combined transmembrane protein topology and signal peptide predictor. The predictor is based on a hidden Markov model (HMM) that models the different sequence regions of a signal peptide and the different regions of a transmembrane protein in a series of interconnected states. Training was done on a newly assembled and curated dataset. Compared to TMHMM and SignalP, errors coming from cross-prediction between transmembrane segments and signal peptides were reduced substantially by Phobius. False classifications of signal peptides were reduced from 26.1% to 3.9% and false classifications of transmembrane helices were reduced from 19.0%, to 7.7%. Phobius was applied to the proteomes of Honzo sapiens and Escherichia coli. Here we also noted a drastic reduction of false classifications compared to TMHMM/SignalP, suggesting that Phobius is well suited for whole-genome annotation of signal peptides and transmembrane regions. The method is available at http://phobius.cgb.ki.se/ as well as at http://phobius.binf.ku.dk/ (C) 2004 Elsevier Ltd. All rights reserved.				Kall, Lukas/A-7334-2009; Krogh, Anders/M-1541-2014	Kall, Lukas/0000-0001-5689-9797; Krogh, Anders/0000-0002-5147-6282												0022-2836					MAY 14	2004	338	5					1027	1036		10.1016/j.jmb.2004.03.016						WOS:000221305200014	15111065	J	Fayyad, U; PiatetskyShapiro, G; Smyth, P				Fayyad, U; PiatetskyShapiro, G; Smyth, P			From data mining to knowledge discovery in databases	AI MAGAZINE												Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field.																	0738-4602					FAL	1996	17	3					37	54								WOS:A1996VJ67100006		J	Hofmann, T				Hofmann, T			Unsupervised learning by probabilistic latent semantic analysis	MACHINE LEARNING												This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.																	0885-6125					JAN	2001	42	1-2					177	196		10.1023/A:1007617005950						WOS:000166114100008		J	Zhang, J; Marszalek, M; Lazebnik, S; Schmid, C				Zhang, J.; Marszalek, M.; Lazebnik, S.; Schmid, C.			Local features and kernels for classification of texture and object categories: A comprehensive study	INTERNATIONAL JOURNAL OF COMPUTER VISION												Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover's Distance and the chi(2) distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on four texture and five object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the influence of background correlations on recognition performance via extensive tests on the PASCAL database, for which ground-truth object localization information is available. Our experiments demonstrate that image representations based on distributions of local features are surprisingly effective for classification of texture and objectimages under challenging real-world conditions, including significant intra-class variations and substantial background clutter.					zhang, jianguo/0000-0001-9317-0268												0920-5691					JUN	2007	73	2					213	238		10.1007/s11263-006-9794-4						WOS:000244943500006		J	Huang, GB; Chen, L; Siew, CK				Huang, Guang-Bin; Chen, Lei; Siew, Chee-Kheong			Universal approximation using incremental constructive feedforward networks with random hidden nodes	IEEE TRANSACTIONS ON NEURAL NETWORKS												According to conventional neural network theories, single-hidden-layer feedforward networks (SLFNs) with additive or radial basis function (RBF) hidden nodes are universal approximators when all the parameters of the networks are allowed adjustable. However, as observed in most neural network implementations, tuning all the parameters of the networks may cause learning complicated and inefficient, and it may be difficult to train networks with nondifferential activation functions such as threshold networks. Unlike conventional neural network theories, this paper proves in an incremental constructive method that in order to let SLFNs work as universal approximators, one may simply randomly choose hidden nodes and then only need to adjust the output weights linking the hidden layer and the output layer. In such SLFNs implementations, the activation functions for additive nodes can be any bounded nonconstant piecewise continuous functions g : R -> R and the activation functions for RBF nodes can be any integrable piecewise continuous functions g : R -> Rand f(R) g(x)dx not equal 0. The proposed incremental method is efficient not only for SFLNs with continuous (including nondifferentiable) activation functions but also for SLFNs with piecewise continuous (such as threshold) activation functions. Compared to other popular methods such a new network is fully automatic and users need not intervene the learning process by manually tuning control parameters.				Huang, Guang-Bin/A-5035-2011; Siew, CK/A-5082-2011	Huang, Guang-Bin/0000-0002-2480-4965; 												1045-9227					JUL	2006	17	4					879	892		10.1109/TNN.2006.875977						WOS:000238865200004	16856652	J	Chen, MS; Han, JW; Yu, PS				Chen, MS; Han, JW; Yu, PS			Data mining: An overview from a database perspective	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information providing services, such as data warehousing and on-line services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided, and to increase the business opportunities. In response to such a demand, this article is to provide a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided, and a comparative study of such techniques is presented.				Yu, Philip/A-2815-2012	Chen, Ming-Syan/0000-0002-0711-8197												1041-4347					DEC	1996	8	6					866	883		10.1109/69.553155						WOS:A1996WC05700002		J	Mairal, J; Bach, F; Ponce, J; Sapiro, G				Mairal, Julien; Bach, Francis; Ponce, Jean; Sapiro, Guillermo			Online Learning for Matrix Factorization and Sparse Coding	JOURNAL OF MACHINE LEARNING RESEARCH												Sparse coding-that is, modelling data vectors as sparse linear combinations of basis elements-is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.																	1532-4435					JAN	2010	11						19	60								WOS:000277186400002		J	Dormann, CF; Elith, J; Bacher, S; Buchmann, C; Carl, G; Carre, G; Marquez, JRG; Gruber, B; Lafourcade, B; Leitao, PJ; Munkemuller, T; McClean, C; Osborne, PE; Reineking, B; Schroder, B; Skidmore, AK; Zurell, D; Lautenbach, S				Dormann, Carsten F.; Elith, Jane; Bacher, Sven; Buchmann, Carsten; Carl, Gudrun; Carre, Gabriel; Garcia Marquez, Jaime R.; Gruber, Bernd; Lafourcade, Bruno; Leitao, Pedro J.; Muenkemueller, Tamara; McClean, Colin; Osborne, Patrick E.; Reineking, Bjoern; Schroeder, Boris; Skidmore, Andrew K.; Zurell, Damaris; Lautenbach, Sven			Collinearity: a review of methods to deal with it and a simulation study evaluating their performance	ECOGRAPHY												Collinearity refers to the non independence of predictor variables, usually in a regression-type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold-based pre-selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor-response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine-learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold-based pre-selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold-based pre-selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the folk lore'-thresholds of correlation coefficients between predictor variables of |r| >0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre-analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them.				Skidmore, Andrew/C-7441-2011; Zurell, Damaris/E-2439-2012; Schroder, Boris/B-7211-2009; Lautenbach, Sven/C-1235-2010; Faculty of ITC, Dep Nat. Resources/C-4295-2014; Leitao, Pedro/B-7422-2009; Elith, Jane/F-2022-2015; Bacher, Sven/F-5431-2010	Skidmore, Andrew/0000-0002-7446-8429; Zurell, Damaris/0000-0002-4628-3558; Schroder, Boris/0000-0002-8577-7980; Lautenbach, Sven/0000-0003-1825-9996; Leitao, Pedro/0000-0003-3038-9531; Elith, Jane/0000-0002-8706-0326; Bacher, Sven/0000-0001-5147-7165; Gruber, Bernd/0000-0003-0078-8179												0906-7590	1600-0587				JAN	2013	36	1					27	46		10.1111/j.1600-0587.2012.07348.x						WOS:000315892600003		B	Murphy, KP				Murphy, KP			Machine Learning: A Probabilistic Perspective	MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE																															978-0-262-01802-9				2012							1	1067								WOS:000310531500030		J	Daw, ND; O'Doherty, JP; Dayan, P; Seymour, B; Dolan, RJ				Daw, Nathaniel D.; O'Doherty, John P.; Dayan, Peter; Seymour, Ben; Dolan, Raymond J.			Cortical substrates for exploratory decisions in humans	NATURE												Decision making in an uncertain environment poses a conflict between the opposing demands of gathering and exploiting information. In a classic illustration of this 'exploration-exploitation' dilemma(1), a gambler choosing between multiple slot machines balances the desire to select what seems, on the basis of accumulated experience, the richest option, against the desire to choose a less familiar option that might turn out more advantageous ( and thereby provide information for improving future decisions). Far from representing idle curiosity, such exploration is often critical for organisms to discover how best to harvest resources such as food and water. In appetitive choice, substantial experimental evidence, underpinned by computational reinforcement learning(2) (RL) theory, indicates that a dopaminergic(3,4), striatal(5-9) and medial prefrontal network mediates learning to exploit. In contrast, although exploration has been well studied from both theoretical(1) and ethological(10) perspectives, its neural substrates are much less clear. Here we show, in a gambling task, that human subjects' choices can be characterized by a computationally well-regarded strategy for addressing the explore/exploit dilemma. Furthermore, using this characterization to classify decisions as exploratory or exploitative, we employ functional magnetic resonance imaging to show that the frontopolar cortex and intraparietal sulcus are preferentially active during exploratory decisions. In contrast, regions of striatum and ventromedial prefrontal cortex exhibit activity characteristic of an involvement in value-based exploitative decision making. The results suggest a model of action selection under uncertainty that involves switching between exploratory and exploitative behavioural modes, and provide a computationally precise characterization of the contribution of key decision-related brain systems to each of these functions.				O'Doherty, John/F-1204-2013	Seymour, Ben/0000-0003-1724-5832; Dolan, Ray/0000-0001-9356-761X												0028-0836					JUN 15	2006	441	7095					876	879		10.1038/nature04766						WOS:000238254100043	16778890	S	Osuna, E; Freund, R; Girosi, F			IEEE COMP SOC	Osuna, E; Freund, R; Girosi, F			Training support vector machines: an application to face detection	1997 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS	PROCEEDINGS / CVPR, IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION				1997 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 97)	JUN 17-19, 1997	SAN JUAN, PR	IEEE Comp Soc, Tech Comm Pattern Anal & Machine Intelligence				We investigate the application of Support Vector Machines (SVMs) in computet vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs.) that can be seen as a new method for training polynomial, neural network, or Radical Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.																	1063-6919		0-7803-4236-4				1997							130	136		10.1109/CVPR.1997.609310						WOS:A1997BJ30G00021		J	Ji, SH; Xue, Y; Carin, L				Ji, Shihao; Xue, Ya; Carin, Lawrence			Bayesian compressive sensing	IEEE TRANSACTIONS ON SIGNAL PROCESSING												The data of interest are assumed to be represented as N-dimensional real vectors, and these vectors are compressible in some linear basis B, implying that the signal can be reconstructed accurately using only a small number M << N of basis-function coefficients associated with B. Compressive sensing is a framework whereby one does not measure one of the aforementioned N-dimensional signals directly, but rather a set of related measurements, with the new measurements a linear combination of the original underlying N-dimensional signal. The number of required compressive-sensing measurements is typically much smaller than N, offering the potential to simplify the sensing system. Let f denote the unknown underlying N-dimensional signal, and g a vector of compressive-sensing measurements, then one may approximate f accurately by utilizing knowledge of the (under-determined) linear relationship between f and g, in addition to knowledge of the fact that f is compressible in B. In this paper we employ a Bayesian formalism for estimating the underlying signal f based on compressive-sensing measurements g. The proposed framework has the following properties: i) in addition to estimating the underlying signal f, "error bars" are also estimated, these giving a measure of confidence in the inverted signal; ii) using knowledge of the error bars, a principled means is provided for determining when a sufficient number of compressive-sensing measurements have been performed; iii) this setting lends itself naturally to a framework whereby the compressive sensing measurements are optimized adaptively and hence not determined randomly; and iv) the framework accounts for additive noise in the compressive-sensing measurements and provides an estimate of the noise variance. In this paper we present the underlying theory, an associated algorithm, example results, and provide comparisons to other compressive-sensing inversion algorithms in the literature.																	1053-587X					JUN	2008	56	6					2346	2356		10.1109/TSP.2007.914345						WOS:000256153800016		J	Simpson, TW; Peplinski, JD; Koch, PN; Allen, JK				Simpson, TW; Peplinski, JD; Koch, PN; Allen, JK			Metamodels for computer-based engineering design: survey and recommendations	ENGINEERING WITH COMPUTERS												The use of statistical techniques to build approximations of expensive computer analysis codes pervades much of today's engineering design. These statistical approximations, or metamodels, are used to replace the actual expensive computer analyses, facilitating multidisciplinary, multiobjective optimization and concept exploration. In this paper, we review several of these techniques, including design of experiments, response surface methodology, Taguchi methods, neural networks, inductive learning and kriging. We survey their existing application in engineering design, and then address the dangers of applying traditional statistical techniques to approximate deterministic computer analysis codes. We conclude with recommendations for the appropriate use of statistical approximation techniques in given situations, and how common pitfalls can be avoided.				UF, MDO/G-8720-2012													0177-0667						2001	17	2					129	150		10.1007/PL00007198						WOS:000170149900004		J	LIN, CT; LEE, CSG				LIN, CT; LEE, CSG			NEURAL-NETWORK-BASED FUZZY-LOGIC CONTROL AND DECISION SYSTEM	IEEE TRANSACTIONS ON COMPUTERS												A general neural-network (connectionist) model for fuzzy logic control and decision/diagnosis systems is proposed. The proposed connectionist model can be contrasted with the traditional fuzzy logic control and decision system in their network structure and learning ability. Such fuzzy control/decision networks can be constructed from training examples by machine learning techniques, and the connectionist structure can be trained to develop fuzzy logic rules and find optimal input/output membership functions. By combining both unsupervised (self-organized) and supervised learning schemes, the learning speed converges much faster than the original backpropagation learning algorithm. This connectionist model also provides human-understandable meaning to the normal feedforward multilayer neural network in which the internal units are always opaque to the users. The connectionist structure also avoids the rule-matching time of the inference engine in the traditional fuzzy logic system. Two examples are presented to illustrate the performance and applicability of the proposed connectionist model.				Lin, Chin-Teng (CT)/G-8129-2017	Lin, Chin-Teng (CT)/0000-0001-8371-8197												0018-9340					DEC	1991	40	12					1320	1336		10.1109/12.106218						WOS:A1991GU89000002		J	Zhou, ZH; Wu, JX; Tang, W				Zhou, ZH; Wu, JX; Tang, W			Ensembling neural networks: Many could be better than all	ARTIFICIAL INTELLIGENCE					IJCAI 2001 Conference	2001	SEATTLE, WASHINGTON	IJCAI				Neural network ensemble is a learning paradigm where many neural networks are jointly used to solve a problem. In this paper, the relationship between the ensemble and its component neural networks is analyzed from the context of both regression and classification, which reveals that it may be better to ensemble many instead of all of the neural networks at hand. This result is interesting because at present, most approaches ensemble all the available neural networks for prediction. Then, in order to show that the appropriate neural networks for composing an ensemble can be effectively selected from a set of available neural networks, an approach named GASEN is presented. GASEN trains a number of neural networks at first. Then it assigns random weights to those networks and employs genetic algorithm to evolve the weights so that they can characterize to some extent the fitness of the neural networks in constituting an ensemble. Finally it selects some neural networks based on the evolved weights to make up the ensemble. A large empirical study shows that, compared with some popular ensemble approaches such as Bagging and Boosting, GASEN can generate neural network ensembles with far smaller sizes but stronger generalization ability. Furthermore, in order to understand the working mechanism of GASEN, the bias-variance decomposition of the error is provided in this paper, which shows that the success of GASEN may lie in that it can significantly reduce the bias as well as the variance. (C) 2002 Elsevier Science B.V. All rights reserved.				Wu, Jianxin/A-3700-2011; Wu, Jianxin/B-8539-2012													0004-3702					MAY	2002	137	1-2					239	263	PII S0004-3702(02)00190-X	10.1016/S0004-3702(02)00190-X						WOS:000176015900007		J	HOLTE, RC				HOLTE, RC			VERY SIMPLE CLASSIFICATION RULES PERFORM WELL ON MOST COMMONLY USED DATASETS	MACHINE LEARNING												This article reports an empirical investigation of the accuracy of rules that classify examples on the basis of a single attribute. On most datasets studied, the best of these very simple rules is as accurate as the rules induced by the majority of machine learning systems. The article explores the implications of this finding for machine learning research and applications.																	0885-6125	1573-0565				APR	1993	11	1					63	91		10.1023/A:1022631118932						WOS:A1993LG46600004		J	Sachs, K; Perez, O; Pe'er, D; Lauffenburger, DA; Nolan, GP				Sachs, K; Perez, O; Pe'er, D; Lauffenburger, DA; Nolan, GP			Causal protein-signaling networks derived from multiparameter single-cell data	SCIENCE												Machine learning was applied for the automated derivation of causal influences in cellular signaling networks. This derivation relied on the simultaneous measurement of multiple phosphorylated protein and phospholipid components in thousands of individual primary human immune system cells. Perturbing these cells with molecular interventions drove the ordering of connections between pathway components, wherein Bayesian network computational methods automatically elucidated most of the traditionally reported signaling relationships and predicted novel interpathway network causalities, which we verified experimentally. Reconstruction of network models from physiologically relevant primary single cells might be applied to understanding native-state tissue signaling biology, complex drug actions, and dysfunctional signaling in diseased cells.																	0036-8075					APR 22	2005	308	5721					523	529		10.1126/science.1105809						WOS:000228810500045	15845847	J	Dietterich, TG; Lathrop, RH; LozanoPerez, T				Dietterich, TG; Lathrop, RH; LozanoPerez, T			Solving the multiple instance problem with axis-parallel rectangles	ARTIFICIAL INTELLIGENCE												The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.				Lozano-Perez, Tomas/J-9374-2012	Lozano-Perez, Tomas/0000-0002-8657-2450												0004-3702					JAN	1997	89	1-2					31	71		10.1016/S0004-3702(96)00034-3						WOS:A1997WM88500002		J	Oliver, NM; Rosario, B; Pentland, AP				Oliver, NM; Rosario, B; Pentland, AP			A Bayesian computer vision system for modeling human interactions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task [1]. The system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach [2]. We propose and compare two different state-based learning architectures, namely, HMMs and CHMMs for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately. Finally, to deal with the problem of limited training data, a synthetic "Alife-style" training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training.				Oliver, Nuria/D-6724-2016	Oliver, Nuria/0000-0001-5985-691X												0162-8828	1939-3539				AUG	2000	22	8					831	843		10.1109/34.868684						WOS:000089321500009		J	Bengio, Y; Courville, A; Vincent, P				Bengio, Yoshua; Courville, Aaron; Vincent, Pascal			Representation Learning: A Review and New Perspectives	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.																	0162-8828					AUG	2013	35	8					1798	1828		10.1109/TPAMI.2013.50						WOS:000320381400002	23787338	J	Di Caro, G; Dorigo, M				Di Caro, G; Dorigo, M			AntNet: Distributed stigmergetic control for communications networks	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												This paper introduces AntNet, a novel approach to the adaptive learning of routing tables in communications networks. AntNet is a distributed, mobile agents based Monte Carlo system that was inspired by recent work on the ant colony metaphor for solving optimization problems. AntNet's agents concurrently explore the network and exchange collected information. The communication among the agents is indirect and asynchronous, mediated by the network itself. This form of communication is typical of social insects and is called stigmergy. We compare our algorithm with six state-of-the-art routing algorithms coming from the telecommunications and machine learning fields. The algorithms' performance is evaluated over a set of realistic testbeds. We run many experiments over real and artificial IP datagram networks with increasing number of nodes and under several paradigmatic spatial and temporal traffic distributions. Results are very encouraging. AntNet showed superior performance under all the experimental conditions with respect to its competitors. We analyze the main characteristics of the algorithm and try to explain the reasons for its superiority.				Dorigo, Marco/B-5664-2013	Dorigo, Marco/0000-0002-3971-0507												1076-9757	1943-5037					1998	9						317	365								WOS:000077370300001		J	Barnard, K; Duygulu, P; Forsyth, D; de Freitas, N; Blei, DM; Jordan, MI				Barnard, K; Duygulu, P; Forsyth, D; de Freitas, N; Blei, DM; Jordan, MI			Matching words and pictures	JOURNAL OF MACHINE LEARNING RESEARCH					Workshop on Machine Learning Methods for Text and Images	2001	VANCOUVER, CANADA					We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et at.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data.				Duygulu, Pinar/N-2707-2013	Duygulu, Pinar/0000-0002-6420-2838; Barnard, Kobus/0000-0002-8568-9518												1532-4435					AUG 15	2003	3	6					1107	1135		10.1162/153244303322533214						WOS:000186002400005		J	FARMER, JD; PACKARD, NH; PERELSON, AS				FARMER, JD; PACKARD, NH; PERELSON, AS			THE IMMUNE-SYSTEM, ADAPTATION, AND MACHINE LEARNING	PHYSICA D																													0167-2789					OCT-NOV	1986	22	1-3					187	204		10.1016/0167-2789(86)90240-X						WOS:A1986F340700014		J	Srivastava, N; Hinton, G; Krizhevsky, A; Sutskever, I; Salakhutdinov, R				Srivastava, Nitish; Hinton, Geoffrey; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan			Dropout: A Simple Way to Prevent Neural Networks from Overfitting	JOURNAL OF MACHINE LEARNING RESEARCH												Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.																	1532-4435					JUN	2014	15						1929	1958								WOS:000344638300002		J	Ingolia, NT; Lareau, LF; Weissman, JS				Ingolia, Nicholas T.; Lareau, Liana F.; Weissman, Jonathan S.			Ribosome Profiling of Mouse Embryonic Stem Cells Reveals the Complexity and Dynamics of Mammalian Proteomes	CELL												The ability to sequence genomes has far outstripped approaches for deciphering the information they encode. Here we present a suite of techniques, based on ribosome profiling (the deep sequencing of ribosome-protected mRNA fragments), to provide genome-wide maps of protein synthesis as well as a pulse-chase strategy for determining rates of translation elongation. We exploit the propensity of harringtonine to cause ribosomes to accumulate at sites of translation initiation together with a machine learning algorithm to define protein products systematically. Analysis of translation in mouse embryonic stem cells reveals thousands of strong pause sites and unannotated translation products. These include amino-terminal extensions and truncations and upstream open reading frames with regulatory potential, initiated at both AUG and non-AUG codons, whose translation changes after differentiation. We also define a class of short, polycistronic ribosome-associated coding RNAs (sprcRNAs) that encode small proteins. Our studies reveal an unanticipated complexity to mammalian proteomes.																	0092-8674					NOV 11	2011	147	4					789	802		10.1016/j.cell.2011.10.002						WOS:000296902300013	22056041	J	Julenius, K; Molgaard, A; Gupta, R; Brunak, S				Julenius, K; Molgaard, A; Gupta, R; Brunak, S			Prediction, conservation analysis, and structural characterization of mammalian mucin-type O-glycosylation sites	GLYCOBIOLOGY												O-GalNAc-glycosylation is one of the main types of glycosylation in mammalian cells. No consensus recognition sequence for the O-glycosyltransferases is known, making prediction methods necessary to bridge the gap between the large number of known protein sequences and the small number of proteins experimentally investigated with regard to glycosylation status. From O-GLYCBASE a total of 86 mammalian proteins experimentally investigated for in vivo O-GalNAc sites were extracted. Mammalian protein homolog comparisons showed that a glycosylated serine or threonine is less likely to be precisely conserved than a nonglycosylated one. The Protein Data Bank was analyzed for structural information, and 12 glycosylated structures were obtained. All positive sites were found in coil or turn regions. A method for predicting the location for mucin-type glycosylation sites was trained using a neural network approach. The best overall network used as input amino acid composition, averaged surface accessibility predictions together with substitution matrix profile encoding of the sequence. To improve prediction on isolated (single) sites, networks were trained on isolated sites only. The final method combines predictions from the best overall network and the best isolated site network; this prediction method correctly predicted 76% of the glycosylated residues and 93% of the nonglycosylated residues. NetOGlyc 3.1 can predict sites for completely new proteins without losing its performance. The fact that the sites could be predicted from averaged properties together with the fact that glycosylation sites are not precisely conserved indicates that mucin-type glycosylation in most cases is a bulk property and not a very site-specific one. NetOGlyc 3.1 is made available at www.cbs.dtu.dk/services/netoglyc.					Gupta, Ramneek/0000-0001-6841-6676												0959-6658					FEB	2005	15	2					153	164		10.1093/glycob/cwh151						WOS:000226199900006	15385431	J	Coifman, RR; Lafon, S				Coifman, Ronald R.; Lafon, Stephane			Diffusion maps	APPLIED AND COMPUTATIONAL HARMONIC ANALYSIS												In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods. (C) 2006 Published by Elsevier Inc.																	1063-5203					JUL	2006	21	1					5	30		10.1016/j.acha.2006.04.006						WOS:000239157900003		J	Weinberger, KQ; Saul, LK				Weinberger, Kilian Q.; Saul, Lawrence K.			Distance Metric Learning for Large Margin Nearest Neighbor Classification	JOURNAL OF MACHINE LEARNING RESEARCH												The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.																	1532-4435					FEB	2009	10						207	244								WOS:000270824200005		J	Tsochantaridis, I; Joachims, T; Hofmann, T; Altun, Y				Tsochantaridis, I; Joachims, T; Hofmann, T; Altun, Y			Large margin methods for structured and interdependent output variables	JOURNAL OF MACHINE LEARNING RESEARCH												Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.																	1532-4435					SEP	2005	6						1453	1484								WOS:000236330100008		J	Jordan, MI; Ghahramani, Z; Jaakkola, TS; Saul, LK				Jordan, MI; Ghahramani, Z; Jaakkola, TS; Saul, LK			An introduction to variational methods for graphical models	MACHINE LEARNING												This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.																	0885-6125					NOV	1999	37	2					183	233		10.1023/A:1007665907178						WOS:000083346800004		S	Huang, GB; Zhu, QY; Siew, CK			ieee	Huang, GB; Zhu, QY; Siew, CK			Extreme learning machine: A new learning scheme of feedforward neural networks	2004 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, PROCEEDINGS	IEEE International Joint Conference on Neural Networks (IJCNN)				IEEE International Joint Conference on Neural Networks (IJCNN)	JUL 25-29, 2004	Budapest, HUNGARY	IEEE, IEEE Neural Networks Soc, Hungarian Acad Sci, Comp & Automat Res Inst, Katholieke Univ Leuven, Republic Hungary, Natl Commun & Informat Council				It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: 1) the slow gradient-based learning algorithms are extensively used to train neural networks, and 2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these traditional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses the input weights and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide the best generalization performance at extremely fast learning speed. The experimental results based on real-world benchmarking function approximation and classification problems including large complex applications show that the new algorithm can produce best generalization performance in some cases and can learn much faster than traditional popular learning algorithms for feedforward neural networks.				Huang, Guang-Bin/A-5035-2011	Huang, Guang-Bin/0000-0002-2480-4965												1098-7576		0-7803-8359-1				2004							985	990								WOS:000224941900171		J	Huang, C; Davis, LS; Townshend, JRG				Huang, C; Davis, LS; Townshend, JRG			An assessment of support vector machines for land cover classification	INTERNATIONAL JOURNAL OF REMOTE SENSING												The support vector machine (SVM) is a group of theoretically superior machine learning algorithms. It was found competitive with the best available machine learning algorithms in classifying high-dimensional data sets. This paper gives an introduction to the theoretical development of the SVM and an experimental evaluation of its accuracy, stability and training speed in deriving land cover classifications from satellite images. The SVM was compared to three other popular classifiers, including the maximum likelihood classifier (MLC), neural network classifiers (NNC) and decision tree classifiers (DTC). The impacts of kernel configuration on the performance of the SVM and of the selection of training data and input variables on the four classifiers were also evaluated in this experiment.																	0143-1161					FEB	2002	23	4					725	749		10.1080/01431160110040323						WOS:000173109000009		J	Blankertz, B; Tomioka, R; Lemm, S; Kawanabe, M; Muller, KR				Blankertz, Benjamin; Tomioka, Ryota; Lemm, Steven; Kawanabe, Motoaki; Mueller, Klaus-Robert			Optimizing spatial filters for robust EEG single-trial analysis	IEEE SIGNAL PROCESSING MAGAZINE												Due to the volume conduction multichannel electroencephalogram (EEG) recordings give a rather blurred image of brain activity. Therefore spatial filters are extremely useful in single-trial analysis in order to improve the signal-to-noise ratio. There are powerful methods from machine learning and signal processing that permit the optimization of spatio-temporal filters for each subject in a data dependent fashion beyond the fixed filters based on the sensor geometry, e.g., Laplacians. Here we elucidate the theoretical background of the common spatial pattern (CSP) algorithm, a popular method in brain-computer interface (BCI) research. Apart from reviewing several variants of the basic algorithm, we reveal tricks of the trade for achieving a powerful CSP performance, briefly elaborate on theoretical aspects of CSP, and demonstrate the application of CSP-type preprocessing in our studies of the Berlin BCI (BBCI) project.				Muller, Klaus/C-3196-2013; Magazine, Signal Processing/E-9947-2015													1053-5888					JAN	2008	25	1					41	56		10.1109/MSP.200790.900,9						WOS:000251906900008		J	Faith, JJ; Hayete, B; Thaden, JT; Mogno, I; Wierzbowski, J; Cottarel, G; Kasif, S; Collins, JJ; Gardner, TS				Faith, Jeremiah J.; Hayete, Boris; Thaden, Joshua T.; Mogno, Ilaria; Wierzbowski, Jamey; Cottarel, Guillaume; Kasif, Simon; Collins, James J.; Gardner, Timothy S.			Large-scale mapping and validation of Escherichia coli transcriptional regulation from a compendium of expression profiles	PLOS BIOLOGY												Machine learning approaches offer the potential to systematically identify transcriptional regulatory interactions from a compendium of microarray expression profiles. However, experimental validation of the performance of these methods at the genome scale has remained elusive. Here we assess the global performance of four existing classes of inference algorithms using 445 Escherichia coli Affymetrix arrays and 3,216 known E. coli regulatory interactions from RegulonDB. We also developed and applied the context likelihood of relatedness (CLR) algorithm, a novel extension of the relevance networks class of algorithms. CLR demonstrates an average precision gain of 36% relative to the next-best performing algorithm. At a 60% true positive rate, CLR identifies 1,079 regulatory interactions, of which 338 were in the previously known network and 741 were novel predictions. We tested the predicted interactions for three transcription factors with chromatin immunoprecipitation, confirming 21 novel interactions and verifying our RegulonDB-based performance estimates. CLR also identified a regulatory link providing central metabolic control of iron transport, which we confirmed with real-time quantitative PCR. The compendium of expression data compiled in this study, coupled with RegulonDB, provides a valuable model system for further improvement of network inference algorithms using experimental data.					Hayete, Boris/0000-0002-2521-1889												1544-9173					JAN	2007	5	1					54	66	e8	10.1371/journal.pbio.0050008						WOS:000245243100007	17214507	J	Huang, GB; Wang, DH; Lan, Y				Huang, Guang-Bin; Wang, Dian Hui; Lan, Yuan			Extreme learning machines: a survey	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS												Computational intelligence techniques have been used in wide applications. Out of numerous computational intelligence techniques, neural networks and support vector machines (SVMs) have been playing the dominant roles. However, it is known that both neural networks and SVMs face some challenging issues such as: (1) slow learning speed, (2) trivial human intervene, and/or (3) poor computational scalability. Extreme learning machine (ELM) as emergent technology which overcomes some challenges faced by other techniques has recently attracted the attention from more and more researchers. ELM works for generalized single-hidden layer feedforward networks (SLFNs). The essence of ELM is that the hidden layer of SLFNs need not be tuned. Compared with those traditional computational intelligence techniques, ELM provides better generalization performance at a much faster learning speed and with least human intervene. This paper gives a survey on ELM and its variants, especially on (1) batch learning mode of ELM, (2) fully complex ELM, (3) online sequential ELM, (4) incremental ELM, and (5) ensemble of ELM.				Huang, Guang-Bin/A-5035-2011	Huang, Guang-Bin/0000-0002-2480-4965												1868-8071	1868-808X				JUN	2011	2	2					107	122		10.1007/s13042-011-0019-y						WOS:000209203200006		J	Lewis, DD; Yang, YM; Rose, TG; Li, F				Lewis, DD; Yang, YM; Rose, TG; Li, F			RCV1: A new benchmark collection for text categorization research	JOURNAL OF MACHINE LEARNING RESEARCH												Reuters Corpus Volume I ( RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.																	1532-4435					APR	2004	5						361	397								WOS:000236327400003		J	Bioucas-Dias, JM; Plaza, A; Dobigeon, N; Parente, M; Du, Q; Gader, P; Chanussot, J				Bioucas-Dias, Jose M.; Plaza, Antonio; Dobigeon, Nicolas; Parente, Mario; Du, Qian; Gader, Paul; Chanussot, Jocelyn			Hyperspectral Unmixing Overview: Geometrical, Statistical, and Sparse Regression-Based Approaches	IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING												Imaging spectrometers measure electromagnetic energy scattered in their instantaneous field view in hundreds or thousands of spectral channels with higher spectral resolution than multispectral cameras. Imaging spectrometers are therefore often referred to as hyperspectral cameras (HSCs). Higher spectral resolution enables material identification via spectroscopic analysis, which facilitates countless applications that require identifying materials in scenarios unsuitable for classical spectroscopic analysis. Due to low spatial resolution of HSCs, microscopic material mixing, and multiple scattering, spectra measured by HSCs are mixtures of spectra of materials in a scene. Thus, accurate estimation requires unmixing. Pixels are assumed to be mixtures of a few materials, called endmembers. Unmixing involves estimating all or some of: the number of endmembers, their spectral signatures, and their abundances at each pixel. Unmixing is a challenging, ill-posed inverse problem because of model inaccuracies, observation noise, environmental conditions, endmember variability, and data set size. Researchers have devised and investigated many models searching for robust, stable, tractable, and accurate unmixing algorithms. This paper presents an overview of unmixing methods from the time of Keshava and Mustard's unmixing tutorial [1] to the present. Mixing models are first discussed. Signal-subspace, geometrical, statistical, sparsity-based, and spatial-contextual unmixing algorithms are described. Mathematical problems and potential solutions are described. Algorithm characteristics are illustrated experimentally.				Bioucas-Dias, Jose/C-5479-2009; Plaza, Antonio/C-4455-2008	Bioucas-Dias, Jose/0000-0002-0166-5149; Plaza, Antonio/0000-0002-9613-1659; Dobigeon, Nicolas/0000-0001-8127-350X												1939-1404	2151-1535				APR	2012	5	2			SI		354	379		10.1109/JSTARS.2012.2194696						WOS:000304612300002		J	Papageorgiou, C; Poggio, T				Papageorgiou, C; Poggio, T			A trainable system for object detection	INTERNATIONAL JOURNAL OF COMPUTER VISION												This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system.																	0920-5691	1573-1405				JUN	2000	38	1					15	33		10.1023/A:1008162616689						WOS:000088636500003		J	Zhou, ZL; Licklider, LJ; Gygi, SP; Reed, R				Zhou, ZL; Licklider, LJ; Gygi, SP; Reed, R			Comprehensive proteomic analysis of the human spliceosome	NATURE												The precise excision of introns from pre-messenger RNA is performed by the spliceosome, a macromolecular machine containing five small nuclear RNAs and numerous proteins. Much has been learned about the protein components of the spliceosome from analysis of individual purified small nuclear ribonucleoproteins and salt-stable spliceosome 'core' particles. However, the complete set of proteins that constitutes intact functional spliceosomes has yet to be identified. Here we use maltose-binding protein affinity chromatography to isolate spliceosomes in highly purified and functional form. Using nanoscale microcapillary liquid chromatography tandem mass spectrometry, we identify similar to145 distinct spliceosomal proteins, making the spliceosome the most complex cellular machine so far characterized. Our spliceosomes comprise all previously known splicing factors and 58 newly identified components. The spliceosome contains at least 30 proteins with known or putative roles in gene expression steps other than splicing. This complexity may be required not only for splicing multi-intronic metazoan pre-messenger RNAs, but also for mediating the extensive coupling between splicing and other steps in gene expression.																	0028-0836					SEP 12	2002	419	6903					182	185		10.1038/nature01031						WOS:000177931200045	12226669	J	Andrieu, C; de Freitas, N; Doucet, A; Jordan, MI				Andrieu, C; de Freitas, N; Doucet, A; Jordan, MI			An introduction to MCMC for machine learning	MACHINE LEARNING												This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.																	0885-6125	1573-0565				JAN-FEB	2003	50	1-2					5	43		10.1023/A:1020281327116						WOS:000178037200001		J	Rakotomamonjy, A; Bach, FR; Canu, S; Grandvalet, Y				Rakotomamonjy, Alain; Bach, Francis R.; Canu, Stephane; Grandvalet, Yves			SimpleMKL	JOURNAL OF MACHINE LEARNING RESEARCH												Multiple kernel learning (MKL) aims at simultaneously learning a kernel and the associated predictor in supervised learning settings. For the support vector machine, an efficient and general multiple kernel learning algorithm, based on semi-infinite linear programming, has been recently proposed. This approach has opened new perspectives since it makes MKL tractable for large-scale problems, by iteratively using existing support vector machine code. However, it turns out that this iterative algorithm needs numerous iterations for converging towards a reasonable solution. In this paper, we address the MKL problem through a weighted 2-norm regularization formulation with an additional constraint on the weights that encourages sparse kernel combinations. Apart from learning the combination, we solve a standard SVM optimization problem, where the kernel is defined as a linear combination of multiple kernels. We propose an algorithm, named SimpleMKL, for solving this MKL problem and provide a new insight on MKL algorithms based on mixed-norm regularization by showing that the two approaches are equivalent. We show how SimpleMKL can be applied beyond binary classification, for problems like regression, clustering (one-class classification) or multiclass classification. Experimental results show that the proposed algorithm converges rapidly and that its efficiency compares favorably to other MKL algorithms. Finally, we illustrate the usefulness of MKL for some regressors based on wavelet kernels and on some model selection problems related to multiclass classification problems.																	1532-4435					NOV	2008	9						2491	2521								WOS:000262637600004		J	Lin, CF; Wang, SD				Lin, CF; Wang, SD			Fuzzy support vector machines	IEEE TRANSACTIONS ON NEURAL NETWORKS												A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different constributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).																	1045-9227					MAR	2002	13	2					464	471	PII S1045-9227(02)01807-6	10.1109/72.991432						WOS:000174519400019	18244447	J	Anderson, RP; Lew, D; Peterson, AT				Anderson, RP; Lew, D; Peterson, AT			Evaluating predictive models of species' distributions: criteria for selecting optimal models	ECOLOGICAL MODELLING												The Genetic Algorithm for Rule-Set Prediction (GARP) is one of several current approaches to modeling species' distributions using occurrence records and environmental data. Because of stochastic elements in the algorithm and underdetermination of the system (multiple solutions with the same value for the optimization criterion), no unique solution is produced. Furthermore, current implementations of GARP utilize only presence data-rather than both presence and absence, the more general case. Hence, variability among GARP models, which is typical of genetic algorithms, and complications in interpreting results based on asymmetrical (presence-only) input data make model selection critical. Generally, some locality records are randomly selected to build a distributional model, with others set aside to evaluate it. Here, we use intrinsic and extrinsic measures of model performance to determine whether optimal models can be identified based on objective intrinsic criteria, without resorting to an independent test data set. We modeled potential distributions of two rodents (Heteromys anomalus and Microryzomys minutus) and one passerine bird (Carpodacus mexicanus), creating 20 models for each species. For each model, we calculated intrinsic and extrinsic measures of omission and commission error, as well as composite indices of overall error. Although intrinsic and extrinsic composite measures of overall model performance were sometimes loosely related to each other, none was consistently associated with expert-judged model quality. In contrast, intrinsic and extrinsic measures were highly correlated for both omission and commission in the two widespread species (H. anomalus and C mexicanus). Furthermore, a clear inverse relationship existed between omission and commission there, and the best models were consistently found at low levels of omission and moderate-to-high commission values. In contrast, all models for M. minutus showed low values of both omission and commission. Because models are based only on presence data (and not all areas are adequately sampled), the corm-nission index reflects not only true commission error but also a component that results from undersampled areas that the species actually inhabits. We here propose an operational procedure for determining an optimal region of the omission/commission relationship and thus selecting high-quality GARP models. Our implementation of this technique for H. anomalus gave a much more reasonable estimation of the species' potential distribution than did the original suite of models. These findings are relevant to evaluation of other distributional-modeling techniques based on presence-only data and should also be considered with other machine-learning applications modified for use with asymmetrical input data. (C) 2002 Elsevier Science B.V. All rights reserved.				Peterson, A. Townsend/I-5697-2013	Peterson, A. Townsend/0000-0003-0243-2379												0304-3800	1872-7026				APR 15	2003	162	3					211	232	PII S0304-3800(02)00349-6	10.1016/S0304-3800(02)00349-6						WOS:000182470200003		J	Mountrakis, G; Im, J; Ogole, C				Mountrakis, Giorgos; Im, Jungho; Ogole, Caesar			Support vector machines in remote sensing: A review	ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING												A wide range of methods for analysis or airborne- and satellite-derived imagery continues to be proposed and assessed. In this paper, we review remote sensing implementations of support vector machines (SVMs), a promising machine learning methodology. This review is timely due to the exponentially increasing number of works published in recent years. SVMs are particularly appealing in the remote sensing field due to their ability to generalize well even with limited training samples, a common limitation for remote sensing applications. However, they also suffer from parameter assignment issues that can significantly affect obtained results. A summary of empirical results is provided for various applications of over one hundred published works (as of April, 2010). It is our hope that this survey will provide guidelines for future applications of SVMs and possible areas of algorithm enhancement. (C) 2010 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.					Im, Jungho/0000-0002-4506-6877												0924-2716					MAY	2011	66	3					247	259		10.1016/j.isprsjprs.2010.11.001						WOS:000288641400001		J	Shan, CF; Gong, SG; McOwan, PW				Shan, Caifeng; Gong, Shaogang; McOwan, Peter W.			Facial expression recognition based on Local Binary Patterns: A comprehensive study	IMAGE AND VISION COMPUTING												Automatic facial expression analysis is an interesting and challenging problem, and impacts important applications in many areas such as human-computer interaction and data-driven animation. Deriving an effective facial representation from original face images is a vital step for successful facial expression recognition. In this paper, we empirically evaluate facial representation based on statistical local features, Local Binary Patterns, for person-independent facial expression recognition. Different machine learning methods are systematically examined on several databases. Extensive experiments illustrate that LBP features are effective and efficient for facial expression recognition. We further formulate Boosted-LBP to extract the most discriminant LBP features, and the best recognition performance is obtained by using Support Vector Machine classifiers with Boosted-LBP features. Moreover, we investigate LBP features for low-resolution facial expression recognition, which is a critical problem but seldom addressed in the existing work. We observe in our experiments that LBP features perform stably and robustly over a useful range of low resolutions of face images, and yield promising performance in Compressed low-resolution video sequences captured in real-world environments. (C) 2008 Elsevier B.V. All rights reserved.																	0262-8856					MAY 4	2009	27	6					803	816		10.1016/j.imavis.2008.08.005						WOS:000265807000019		S	Rosten, E; Drummond, T		Leonardis, A; Bischof, H; Pinz, A		Rosten, E; Drummond, T			Machine learning for high-speed corner detection	COMPUTER VISION - ECCV 2006 , PT 1, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE				9th European Conference on Computer Vision (ECCV 2006)	MAY 07-13, 2006	Graz, AUSTRIA	Adv Comp Vis, Graz Univ Technol, Univ Ljubljana				Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7% of the available processing time. By comparison neither the Harris detector (120%) nor the detection stage of SIFT (300%) can operate at full frame rate. Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations[1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.				Drummond, Tom/A-4696-2011	Drummond, Tom/0000-0001-8204-5904												0302-9743		3-540-33832-2				2006	3951		1				430	443								WOS:000237552900034		J	Sonnenburg, S; Ratsch, G; Schafer, C; Scholkopf, B				Sonnenburg, Soeren; Raetsch, Gunnar; Schaefer, Christin; Schoelkopf, Bernhard			Large scale multiple kernel learning	JOURNAL OF MACHINE LEARNING RESEARCH												While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun.				Ratsch, Gunnar/B-8182-2009; Sonnenburg, Soeren/F-2230-2010; Scholkopf, Bernhard/A-7570-2013													1532-4435					JUL	2006	7						1531	1565								WOS:000245388800016		J	Liang, NY; Huang, GB; Saratchandran, P; Sundararajan, N				Liang, Nan-Ying; Huang, Guang-Bin; Saratchandran, P.; Sundararajan, N.			A fast and accurate online sequential learning algorithm for feedforward networks	IEEE TRANSACTIONS ON NEURAL NETWORKS												In this paper, we develop an online sequential learning algorithm for single hidden layer feedforward networks (SLFNs) with. additive or radial basis function (RBF) hidden nodes in a unified framework. The algorithm is referred to as online sequential extreme learning machine (OS-ELM) and can learn data one-by-one or chunk-by-chunk (a block of data) with fixed or varying chunk size. The activation functions for additive nodes in OS-ELM can be any bounded nonconstant piecewise continuous functions and the activation functions for RBF nodes can be any integrable piecewise continuous functions. In OS-ELM, the parameters of hidden nodes (the input weights and biases of additive nodes or the centers and impact factors of RBF nodes) are randomly selected and the output weights are analytically determined based on the sequentially arriving data. The algorithm uses the ideas of ELM of Huang et al. developed for batch learning which has been shown to be extremely fast with generalization performance better than other batch training methods. Apart from selecting the number of hidden nodes, no other control parameters have to be manually chosen. Detailed performance comparison of OS-ELM is done with other popular sequential learning algorithms on benchmark problems drawn from the regression, classification and time series prediction areas. The results show that the OS-ELM is faster than the other sequential algorithms and produces better generalization performance.				Huang, Guang-Bin/A-5035-2011	Huang, Guang-Bin/0000-0002-2480-4965												1045-9227					NOV	2006	17	6					1411	1423		10.1109/TNN.2006.880583						WOS:000241933100006	17131657	J	AGRAWAL, R; IMIELINSKI, T; SWAMI, A				AGRAWAL, R; IMIELINSKI, T; SWAMI, A			DATABASE MINING - A PERFORMANCE PERSPECTIVE	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												We present our perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology. We describe three classes of database mining problems involving classification, associations, and sequences, and argue that these problems can be uniformly viewed as requiring discovery of rules embedded in massive data. We describe a model and some basic operations for the process of rule discovery. We show how the database mining problems we consider map to this model and how they can be solved by using the basic operations we propose. We give an example of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm not only is efficient in discovering classification rules but also has accuracy comparable to ID3, one of the current best classifiers.																	1041-4347					DEC	1993	5	6					914	925		10.1109/69.250074						WOS:A1993MQ44300003		B	Pang, B; Lee, L; Vaithyanathan, S		Hajic, J; Matsumoto, Y		Pang, B; Lee, L; Vaithyanathan, S			Thumbs up? Sentiment classification using machine learning techniques	PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING					Conference on Empirical Methods in Natural Language Processing	JUL 06-07, 2002	Philadelphia, PA	Justsystem Corp, CLAIRVOYANCE Corp				We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.																							2002							79	86								WOS:000223079900011		J	Devasia, S; Eleftheriou, E; Moheimani, SOR				Devasia, Santosh; Eleftheriou, Evangelos; Moheimani, S. O. Reza			A survey of control issues in nanopositioning	IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY												Nanotechnology is the science of understanding matter and the control of matter at dimensions of 100 nm or less. Encompassing nanoscale science, engineering, and technology, nanotechnology involves imaging, measuring, modeling, and manipulation of matter at, this level of precision. An important aspect of research in nanotechnology involves precision control and manipulation of devices and materials at a nanoscale, i.e., nanopositioning. Nanopositioners are precision mechatronic systems designed to move objects over a small range with a resolution down to a fraction of an atomic diameter. The desired attributes of a nanopositioner are extremely high resolution, accuracy, stability, and fast response. The key to successful nanopositioning is accurate position sensing and feedback control of the motion. This paper presents an overview of nanopositioning technologies and devices emphasizing the key role of advanced control techniques in improving precision, accuracy, and speed of operation of these systems.				Moheimani, S. O. Reza/B-3033-2008													1063-6536	1558-0865				SEP	2007	15	5					802	823		10.1109/TCST.2007.903345						WOS:000249161300002		J	Zhang, ML; Zhou, ZH				Zhang, Min-Ling; Zhou, Zhi-Hua			ML-KNN: A lazy learning approach to multi-label leaming	PATTERN RECOGNITION												Multi-label learning originated from the investigation of text categorization problem, where each document may belong to several predefined topics simultaneously. In multi-label learning, the training set is composed of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances through analyzing training instances with known label sets. In this paper, a multi-label lazy learning approach named ML-KNN is presented, which is derived from the traditional K-nearest neighbor (KNN) algorithm. In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances, i.e. the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. Experiments on three different real-world multi-label learning problems, i.e. Yeast gene functional analysis, natural scene classification and automatic web page categorization, show that ML-KNN achieves superior performance to some well-established multi-label learning algorithms. (c) 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.																	0031-3203					JUL	2007	40	7					2038	2048		10.1016/j.patcog.2006.12.019						WOS:000246332300016		J	Pereira, F; Mitchell, T; Botvinick, M				Pereira, Francisco; Mitchell, Tom; Botvinick, Matthew			Machine learning classifiers and fMRI: A tutorial overview	NEUROIMAGE					Workshop on Mathematics in Brain Imaging	JUL 14-25, 2008	Univ Calif Los Angeles, Inst Pure & Appl Math, Los Angeles, CA		Univ Calif Los Angeles, Inst Pure & Appl Math			Interpreting brain image experiments requires analysis of complex, multivariate data. In recent years, one analysis approach that has grown in popularity is the use of machine learning algorithms to train classifiers to decode stimuli, mental states, behaviours and other variables of interest from fMRI data and thereby show the data contain information about them. In this tutorial overview we review some of the key choices faced in using this approach as well as how to derive statistically significant results, illustrating each point from a case study. Furthermore, we show how, in addition to answering the question of 'is there information about a variable of interest' (pattern discrimination), classifiers can be used to tackle other classes of question, namely 'where is the information' (pattern localization) and 'how is that information encoded' (pattern characterization). (C) 2008 Elsevier Inc. All rights reserved.																	1053-8119					MAR	2009	45	1					S199	S209		10.1016/j.neuroimage.2008.11.007						WOS:000263862600018	19070668	J	Liu, Y; Zhang, DS; Lu, GJ; Ma, WY				Liu, Ying; Zhang, Dengsheng; Lu, Guojun; Ma, Wei-Ying			A survey of content-based image retrieval with high-level semantics	PATTERN RECOGNITION												In order to improve the retrieval accuracy of content-based image retrieval systems, research focus has been shifted from designing sophisticated low-level feature extraction algorithms to reducing the 'semantic gap' between the visual features and the richness of human semantics. This paper attempts to provide a comprehensive survey of the recent technical achievements in high-level semantic-based image retrieval. Major recent publications are included in this survey covering different aspects of the research in this area, including low-level image feature extraction, similarity measurement, and deriving high-level semantic features. We identify five major categories of the state-of-the-art techniques in narrowing down the 'semantic gap': (1) using object ontology to define high-level concepts; (2) using machine learning methods to associate low-level features with query concepts; (3) using relevance feedback to learn users' intention; (4) generating semantic template to support high-level image retrieval; (5) fusing the evidences from HTML text and the visual content of images for WWW image retrieval. In addition, some other related issues such as image test bed and retrieval performance evaluation are also discussed. Finally, based on existing technology and the demand from real-world applications, a few promising future research directions are suggested. (c) 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JAN	2007	40	1					262	282		10.1016/j.patcog.2006.04.045						WOS:000241837300020		J	Thrun, S; Montemerlo, M; Dahlkamp, H; Stavens, D; Aron, A; Diebel, J; Fong, P; Gale, J; Halpenny, M; Hoffmann, G; Lau, K; Oakley, C; Palatucci, M; Pratt, V; Stang, P; Strohband, S; Dupont, C; Jendrossek, LE; Koelen, C; Markey, C; Rummel, C; van Niekerk, J; Jensen, E; Alessandrini, P; Bradski, G; Davies, B; Ettinger, S; Kaehler, A; Nefian, A; Mahoney, P				Thrun, Sebastian; Montemerlo, Mike; Dahlkamp, Hendrik; Stavens, David; Aron, Andrei; Diebel, James; Fong, Philip; Gale, John; Halpenny, Morgan; Hoffmann, Gabriel; Lau, Kenny; Oakley, Celia; Palatucci, Mark; Pratt, Vaughan; Stang, Pascal; Strohband, Sven; Dupont, Cedric; Jendrossek, Lars-Erik; Koelen, Christian; Markey, Charles; Rummel, Carlo; van Niekerk, Joe; Jensen, Eric; Alessandrini, Philippe; Bradski, Gary; Davies, Bob; Ettinger, Scott; Kaehler, Adrian; Nefian, Ara; Mahoney, Pamela			Stanley: The robot that won the DARPA Grand Challenge	JOURNAL OF FIELD ROBOTICS												This article describes the robot Stanley, which won the 2005 DARPA Grand Challenge. Stanley was developed for high-speed desert driving without manual intervention. The robot's software system relied predominately on state-of-the-art artificial intelligence technologies, such as machine learning and probabilistic reasoning. This paper describes the major components of this architecture, and discusses the results of the Grand Challenge race. (c) 2006 Wiley Periodicals, Inc.					Fong, Philip/0000-0003-3525-8194												1556-4959					SEP	2006	23	9					661	692		10.1002/rob.20147						WOS:000246146900004		J	Jung, M; Reichstein, M; Ciais, P; Seneviratne, SI; Sheffield, J; Goulden, ML; Bonan, G; Cescatti, A; Chen, JQ; de Jeu, R; Dolman, AJ; Eugster, W; Gerten, D; Gianelle, D; Gobron, N; Heinke, J; Kimball, J; Law, BE; Montagnani, L; Mu, QZ; Mueller, B; Oleson, K; Papale, D; Richardson, AD; Roupsard, O; Running, S; Tomelleri, E; Viovy, N; Weber, U; Williams, C; Wood, E; Zaehle, S; Zhang, K				Jung, Martin; Reichstein, Markus; Ciais, Philippe; Seneviratne, Sonia I.; Sheffield, Justin; Goulden, Michael L.; Bonan, Gordon; Cescatti, Alessandro; Chen, Jiquan; de Jeu, Richard; Dolman, A. Johannes; Eugster, Werner; Gerten, Dieter; Gianelle, Damiano; Gobron, Nadine; Heinke, Jens; Kimball, John; Law, Beverly E.; Montagnani, Leonardo; Mu, Qiaozhen; Mueller, Brigitte; Oleson, Keith; Papale, Dario; Richardson, Andrew D.; Roupsard, Olivier; Running, Steve; Tomelleri, Enrico; Viovy, Nicolas; Weber, Ulrich; Williams, Christopher; Wood, Eric; Zaehle, Soenke; Zhang, Ke			Recent decline in the global land evapotranspiration trend due to limited moisture supply	NATURE												More than half of the solar energy absorbed by land surfaces is currently used to evaporate water(1). Climate change is expected to intensify the hydrological cycle(2) and to alter evapotranspiration, with implications for ecosystem services and feedback to regional and global climate. Evapotranspiration changes may already be under way, but direct observational constraints are lacking at the global scale. Until such evidence is available, changes in the water cycle on land-a key diagnostic criterion of the effects of climate change and variability-remain uncertain. Here we provide a data-driven estimate of global land evapotranspiration from 1982 to 2008, compiled using a global monitoring network(3), meteorological and remote-sensing observations, and a machine-learning algorithm(4). In addition, we have assessed evapotranspiration variations over the same time period using an ensemble of process-based land-surface-models. Our results suggest that global annual evapotranspiration increased on average by 7.1 +/- 1.0 millimetres per year per decade from 1982 to 1997. After that, coincident with the last major El Nino event in 1998, the global evapotranspiration increase seems to have ceased until 2008. This change was driven primarily by moisture limitation in the Southern Hemisphere, particularly Africa and Australia. In these regions, microwave satellite observations indicate that soil moisture decreased from 1998 to 2008. Hence, increasing soil-moisture limitations on evapotranspiration largely explain the recent decline of the global land-evapotranspiration trend. Whether the changing behaviour of evapotranspiration is representative of natural climate variability or reflects a more permanent reorganization of the land water cycle is a key question for earth system science.				Oleson, Keith/A-9328-2008; Chen, Jiquan/D-1955-2009; Mu, Qiaozhen/G-5695-2010; Eugster, Werner/E-5116-2010; Goulden, Michael/B-9934-2008; Mueller, Brigitte/E-2594-2011; roupsard, olivier/C-1219-2008; Seneviratne, Sonia/G-8761-2011; Richardson, Andrew/F-5691-2011; Zhang, Ke/B-3227-2012; Gerten, Dieter/B-2975-2013; Vuichard, Nicolas/A-6629-2011; Gianelle, Damiano/G-9437-2011; Zaehle, Sonke/C-9528-2017; Law, Beverly/G-3882-2010; Reichstein, Markus/A-7494-2011	Eugster, Werner/0000-0001-6067-0741; Mueller, Brigitte/0000-0003-1876-4722; Seneviratne, Sonia/0000-0001-9528-2917; Richardson, Andrew/0000-0002-0148-6714; Zhang, Ke/0000-0001-5288-9372; Gianelle, Damiano/0000-0001-7697-5793; Zaehle, Sonke/0000-0001-5602-7956; Montagnani, Leonardo/0000-0003-2957-9071; Papale, Dario/0000-0001-5170-8648; Law, Beverly/0000-0002-1605-1203; Reichstein, Markus/0000-0001-5736-1112; Dolman, A.J./0000-0003-0099-0457												0028-0836	1476-4687				OCT 21	2010	467	7318					951	954		10.1038/nature09396						WOS:000283254700035	20935626	J	Kumar, A; Agarwal, S; Heyman, JA; Matson, S; Heidtman, M; Piccirillo, S; Umansky, L; Drawid, A; Jansen, R; Liu, Y; Cheung, KH; Miller, P; Gerstein, M; Roeder, GS; Snyder, M				Kumar, A; Agarwal, S; Heyman, JA; Matson, S; Heidtman, M; Piccirillo, S; Umansky, L; Drawid, A; Jansen, R; Liu, Y; Cheung, KH; Miller, P; Gerstein, M; Roeder, GS; Snyder, M			Subcellular localization of the yeast proteome	GENES & DEVELOPMENT												Protein localization data are a valuable information resource helpful in elucidating eukaryotic protein function. Here, we report the first proteome-scale analysis of protein localization within any eukaryote. Using directed topoisomerase I-mediated cloning strategies and genome-wide transposon mutagenesis, we have epitope-tagged 60% of the Saccharomyces cerevisiae proteome. By high-throughput immunolocalization of tagged gene products, we have determined the subcellular localization of 2744 yeast proteins. Extrapolating these data through a computational algorithm employing Bayesian formalism, we define the yeast localizome (the subcellular distribution of all 6100 yeast proteins). We estimate the yeast proteome to encompass -5100 soluble proteins and >1000 transmembrane proteins. Our results indicate that 47% of yeast proteins are cytoplasmic, 13% mitochondrial, 13% exocytic (including proteins of the endoplasmic reticulum and secretory vesicles), and 27% nuclear/nucleolar. A subset of nuclear proteins was further analyzed by immunolocalization using surface-spread preparations of meiotic chromosomes. Of these proteins, 38% were found associated with chromosomal DNA. As determined from phenotypic analyses of nuclear proteins, 34% are essential for spore viability-a percentage nearly twice as great as that observed for the proteome as a whole. In total, this study presents experimentally derived localization data for 955 proteins of previously unknown function: nearly half of all functionally uncharacterized proteins in yeast. To facilitate access to these data, we provide a searchable database featuring 2900 fluorescent micrographs at http://ygae.med.yale.edu.																	0890-9369					MAR 15	2002	16	6					707	719		10.1101/gad.970902						WOS:000174516500007	11914276	J	Evgeniou, T; Pontil, M; Poggio, T				Evgeniou, T; Pontil, M; Poggio, T			Regularization networks and support vector machines	ADVANCES IN COMPUTATIONAL MATHEMATICS												Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples - in particular, the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classification is treated as a special case.																	1019-7168						2000	13	1					1	50		10.1023/A:1018946025316						WOS:000085682400001		J	Bath, J; Turberfield, AJ				Bath, Jonathan; Turberfield, Andrew J.			DNA nanomachines	NATURE NANOTECHNOLOGY												We are learning to build synthetic molecular machinery from DNA. This research is inspired by biological systems in which individual molecules act, singly and in concert, as specialized machines: our ambition is to create new technologies to perform tasks that are currently beyond our reach. DNA nanomachines are made by self-assembly, using techniques that rely on the sequence-specific interactions that bind complementary oligonucleotides together in a double helix. They can be activated by interactions with specific signalling molecules or by changes in their environment. Devices that change state in response to an external trigger might be used for molecular sensing, intelligent drug delivery or programmable chemical synthesis. Biological molecular motors that carry cargoes within cells have inspired the construction of rudimentary DNA walkers that run along self-assembled tracks. It has even proved possible to create DNA motors that move autonomously, obtaining energy by catalysing the reaction of DNA or RNA fuels.				Bath, Jonathan/E-9569-2011	Bath, Jonathan/0000-0002-7144-662X												1748-3387					MAY	2007	2	5					275	284		10.1038/nnano.2007.104						WOS:000246688900010	18654284	J	Markou, M; Singh, S				Markou, M; Singh, S			Novelty detection: a review - part 1: statistical approaches	SIGNAL PROCESSING												Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. Novelty detection is one of the fundamental requirements of a good classification or identification system since sometimes the test data contains information about objects that were not known at the time of training the model. In this paper we provide state-of-the-art review in the area of novelty detection based on statistical approaches. The second part paper details novelty detection using neural networks. As discussed, there are a multitude of applications where novelty detection is extremely important including signal processing, computer vision, pattern recognition, data mining, and robotics. (C) 2003 Elsevier B.V. All rights reserved.																	0165-1684	1879-2677				DEC	2003	83	12					2481	2497		10.1016/j.sigpro.2003.018						WOS:000186346000001		J	Cilibrasi, RL; Vitanyi, PMB				Cilibrasi, Rudi L.; Vitanyi, Paul M. B.			The Google similarity distance	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING					IEEE Information Theory Workshop on Coding and Complexity	AUG-SEP -, 2005	Rotorua, NEW ZEALAND	IEEE				Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers, the equivalent of "society" is "database," and the equivalent of " use" is "a way to search the database." We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts, we use the World Wide Web (WWW) as the database, and Google as the search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the WWW using Google page counts. The WWW is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87 percent with the expert crafted WordNet categories.				Vitanyi, Paul/A-9037-2012	Vitanyi, Paul/0000-0002-5712-7585												1041-4347					MAR	2007	19	3					370	383		10.1109/TKDE.2007.48						WOS:000243504100003		J	Ye, QH; Qin, LX; Forgues, M; He, P; Kim, JW; Peng, AC; Simon, R; Li, Y; Robles, AI; Chen, YD; Ma, ZC; Wu, ZQ; Ye, SL; Liu, YK; Tang, ZY; Wang, XW				Ye, QH; Qin, LX; Forgues, M; He, P; Kim, JW; Peng, AC; Simon, R; Li, Y; Robles, AI; Chen, YD; Ma, ZC; Wu, ZQ; Ye, SL; Liu, YK; Tang, ZY; Wang, XW			Predicting hepatitis B virus-positive metastatic hepatocellular carcinomas using gene expression profiling and supervised machine learning	NATURE MEDICINE												Hepatocellular carcinoma (HCC) is one of the most common and aggressive human malignancies. Its high mortality rate is mainly a result of intra-hepatic metastases. We analyzed the expression profiles of HCC samples without or with intra-hepatic metastases. Using a supervised machine-learning algorithm, we generated for the first time a molecular signature that can classify metastatic HCC patients and identified genes that were relevant to metastasis and patient survival. We found that the gene expression signature of primary HCCs with accompanying metastasis was very similar to that of their corresponding metastases, implying that genes favoring metastasis progression were initiated in the primary tumors. Osteopontin, which was identified as a lead gene in the signature, was over-expressed in metastatic HCC; an osteopontin-specific antibody effectively blocked HCC cell invasion in vitro and inhibited pulmonary metastasis of HCC cells in nude mice. Thus, osteopontin acts as both a diagnostic marker and a potential therapeutic target for metastatic HCC.				Wang, Xin/B-6162-2009	Wang, Xin/0000-0001-9735-606X; Robles, Ana/0000-0001-5019-4374												1078-8956					APR	2003	9	4					416	423		10.1038/nm843						WOS:000181987400031	12640447	J	Stockwell, DRB; Peterson, AT				Stockwell, DRB; Peterson, AT			Effects of sample size on accuracy of species distribution models	ECOLOGICAL MODELLING												Given increasing access to large amounts of biodiversity information, a powerful capability is that of modeling ecological niches and predicting geographic distributions. Because, sampling species' distributions is costly, we explored sample size needs for accurate modeling for three predictive modeling methods via re-sampling of data for well-sampled species, and developed curves of model improvement with increasing sample size. In general, under a coarse surrogate model, and machine-learning methods, average success rate at predicting occurrence of a species at a location, or accuracy, was 90% of maximum within ten sample points, and was near maximal at 50 data points. However, a fine surrogate model and logistic regression model had significantly lower rates of increase in accuracy with increasing sample size, reaching similar maximum accuracy at 100 data points. The choice of environmental variables also produced unpredictable effects on accuracy over the range of sample sizes on the logistic regression method, while the machine-learning method had robust performance throughout. Examining correlates of model performance across species, extent of geographic distribution was the only significant ecological factor. (C) 2002 Elsevier Science B.V. All rights reserved.					Peterson, A. Townsend/0000-0003-0243-2379												0304-3800					FEB 1	2002	148	1					1	13		10.1016/S0304-3800(01)00388-X						WOS:000173626400001		J	Kall, L; Canterbury, JD; Weston, J; Noble, WS; MacCoss, MJ				Kall, Lukas; Canterbury, Jesse D.; Weston, Jason; Noble, William Stafford; MacCoss, Michael J.			Semi-supervised learning for peptide identification from shotgun proteomics datasets	NATURE METHODS												Shotgun proteomics uses liquid chromatography-tandem mass spectrometry to identify proteins in complex biological samples. We describe an algorithm, called Percolator, for improving the rate of confident peptide identifications from a collection of tandem mass spectra. Percolator uses semi-supervised machine learning to discriminate between correct and decoy spectrum identifications, correctly assigning peptides to 17% more spectra from a tryptic Saccharomyces cerevisiae dataset, and up to 77% more spectra from non-tryptic digests, relative to a fully supervised approach.				Kall, Lukas/A-7334-2009	Kall, Lukas/0000-0001-5689-9797												1548-7091					NOV	2007	4	11					923	925		10.1038/NMETH1113						WOS:000250575700016	17952086	J	Turaga, P; Chellappa, R; Subrahmanian, VS; Udrea, O				Turaga, Pavan; Chellappa, Rama; Subrahmanian, V. S.; Udrea, Octavian			Machine Recognition of Human Activities: A Survey	IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY												The past decade has witnessed a rapid proliferation of video cameras in all walks of life and has resulted in a tremendous explosion of video content. Several applications such as content-based video annotation and retrieval, highlight extraction and video summarization require recognition of the activities occurring in the video. The analysis of human activities in videos is an area with increasingly important consequences from security and surveillance to entertainment and personal archiving. Several challenges at various levels of processing-robustness against errors in low-level processing, view and rate-invariant representations at midlevel processing and semantic representation of human activities at higher level processing-make this problem hard to solve. In this review paper, we present a comprehensive survey of efforts in the past couple of decades to address the problems of representation, recognition, and learning of human activities from video and related applications. We discuss the problem at two major levels of complexity: 1) "actions" and 2) "activities." "Actions" are characterized by simple motion patterns typically executed by a single human. "Activities" are more complex and involve coordinated actions among a small number of humans. We will discuss several approaches and classify them according to their ability to handle varying degrees of complexity as interpreted above. We begin with a discussion of approaches to model the simplest of action classes known as atomic or primitive actions that do not require sophisticated dynamical modeling. Then, methods to model actions with more complex dynamics are discussed. The discussion then leads naturally to methods for higher level representation of complex activities.																	1051-8215	1558-2205				NOV	2008	18	11					1473	1488		10.1109/TCSVT.2008.2005594						WOS:000260867100002		J	Argall, BD; Chernova, S; Veloso, M; Browning, B				Argall, Brenna D.; Chernova, Sonia; Veloso, Manuela; Browning, Brett			A survey of robot learning from demonstration	ROBOTICS AND AUTONOMOUS SYSTEMS												We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, anti contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research. (c) 2008 Elsevier B.V. All rights reserved.				Argall, Brenna/G-2543-2011													0921-8890					MAY 31	2009	57	5					469	483		10.1016/j.robot.2008.10.024						WOS:000266122800001		J	Dietterich, TG				Dietterich, TG			Machine-learning research - Four current directions	AI MAGAZINE												Machine-learning research has been making great progress in many directions. This article summarizes four of these directions and discusses some current open problems. The four directions are (1) the improvement of classification accuracy by learning ensembles of classifiers, (2) methods for scaling up supervised learning algorithms, (3) reinforcement learning, and (4) the learning of complex stochastic models.																	0738-4602					WIN	1997	18	4					97	136								WOS:000071153200009		J	Yarkoni, T; Poldrack, RA; Nichols, TE; Van Essen, DC; Wager, TD				Yarkoni, Tal; Poldrack, Russell A.; Nichols, Thomas E.; Van Essen, David C.; Wager, Tor D.			Large-scale automated synthesis of human functional neuroimaging data	NATURE METHODS												The rapid growth of the literature on neuroimaging in humans has led to major advances in our understanding of human brain function but has also made it increasingly difficult to aggregate and synthesize neuroimaging findings. Here we describe and validate an automated brain-mapping framework that uses text-mining, meta-analysis and machine-learning techniques to generate a large database of mappings between neural and cognitive states. We show that our approach can be used to automatically conduct large-scale, high-quality neuroimaging meta-analyses, address long-standing inferential problems in the neuroimaging literature and support accurate 'decoding' of broad cognitive states from brain activity in both entire studies and individual human subjects. Collectively, our results have validated a powerful and generative framework for synthesizing human neuroimaging data on an unprecedented scale.					Nichols, Thomas/0000-0002-4516-5103												1548-7091					AUG	2011	8	8					665	U95		10.1038/NMETH.1635						WOS:000293220600023	21706013	J	Garcia, S; Fernandez, A; Luengo, J; Herrera, F				Garcia, Salvador; Fernandez, Alberto; Luengo, Julian; Herrera, Francisco			Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: Experimental analysis of power	INFORMATION SCIENCES												Experimental analysis of the performance of a proposed method is a crucial and necessary task in an investigation. In this paper, we focus on the use of nonparametric statistical inference for analyzing the results obtained in an experiment design in the field of computational intelligence. We present a case study which involves a set of techniques in classification tasks and we study a set of nonparametric procedures useful to analyze the behavior of a method with respect to a set of algorithms, such as the framework in which a new proposal is developed. Particularly, we discuss some basic and advanced nonparametric approaches which improve the results offered by the Friedman test in some circumstances. A set of post hoc procedures for multiple comparisons is presented together with the computation of adjusted p-values. We also perform an experimental analysis for comparing their power, with the objective of detecting the advantages and disadvantages of the statistical tests described. We found that some aspects such as the number of algorithms, number of data sets and differences in performance offered by the control method are very influential in the statistical tests studied. Our final goal is to offer a complete guideline for the use of nonparametric statistical procedures for performing multiple comparisons in experimental studies. (C) 2009 Elsevier Inc. All rights reserved.				Herrera, Francisco/C-6856-2008; Fernandez, Alberto/G-3827-2014; Luengo, Julian/L-6569-2014; Garcia, Salvador/N-3624-2013; Luengo, Julian/D-1307-2017; Mangan, Rachel/A-8824-2008	Herrera, Francisco/0000-0002-7283-312X; Luengo, Julian/0000-0003-3952-3629; Garcia, Salvador/0000-0003-4494-7565; Luengo, Julian/0000-0003-3952-3629; Mangan, Rachel/0000-0002-8788-9214; Fernandez Hilario, Alberto/0000-0002-6480-8434												0020-0255	1872-6291				MAY 15	2010	180	10			SI		2044	2064		10.1016/j.ins.2009.12.010						WOS:000276708100021		J	Betel, D; Koppal, A; Agius, P; Sander, C; Leslie, C				Betel, Doron; Koppal, Anjali; Agius, Phaedra; Sander, Chris; Leslie, Christina			Comprehensive modeling of microRNA targets predicts functional non-conserved and non-canonical sites	GENOME BIOLOGY												mirSVR is a new machine learning method for ranking microRNA target sites by a down-regulation score. The algorithm trains a regression model on sequence and contextual features extracted from miRanda-predicted target sites. In a large-scale evaluation, miRanda-mirSVR is competitive with other target prediction methods in identifying target genes and predicting the extent of their downregulation at the mRNA or protein levels. Importantly, the method identifies a significant number of experimentally determined non-canonical and non-conserved sites.				sander, chris/H-1452-2011													1474-7596						2010	11	8							R90	10.1186/gb-2010-11-8-r90						WOS:000283777600011	20799968	J	Pawlak, Z; Skowron, A				Pawlak, Zdzislaw; Skowron, Andrzej			Rough sets and Boolean reasoning	INFORMATION SCIENCES												In this article, we discuss methods based on the combination of rough sets and Boolean reasoning with applications in pattern recognition, machine learning, data mining and conflict analysis. (c) 2006 Elsevier Inc. All rights reserved.																	0020-0255					JAN 1	2007	177	1					41	73		10.1016/j.ins.2006.06.007						WOS:000242194600004		J	Kloppel, S; Stonnington, CM; Chu, C; Draganski, B; Scahill, RI; Rohrer, JD; Fox, NC; Jack, CR; Ashburner, J; Frackowiak, RSJ				Kloeppel, Stefan; Stonnington, Cynthia M.; Chu, Carlton; Draganski, Bogdan; Scahill, Rachael I.; Rohrer, Jonathan D.; Fox, Nick C.; Jack, Clifford R., Jr.; Ashburner, John; Frackowiak, Richard S. J.			Automatic classification of MR scans in Alzheimers disease	BRAIN												To be diagnostically useful, structural MRI must reliably distinguish Alzheimers disease (AD) from normal aging in individual scans. Recent advances in statistical learning theory have led to the application of support vector machines to MRI for detection of a variety of disease states. The aims of this study were to assess how successfully support vector machines assigned individual diagnoses and to determine whether data-sets combined from multiple scanners and different centres could be used to obtain effective classification of scans. We used linear support vector machines to classify the grey matter segment of TI-weighted MR scans from pathologically proven AD patients and cognitively normal elderly individuals obtained from two centres with different scanning equipment. Because the clinical diagnosis of mild AD is difficult we also tested the ability of support vector machines to differentiate control scans from patients without post-mortem confirmation. Finally we sought to use these methods to differentiate scans between patients suffering from AD from those with frontotemporal lobar degeneration. Up to 96% of pathologically verified AD patients were correctly classified using whole brain images. Data from different centres were successfully combined achieving comparable results from the separate analyses. Importantly, data from one centre could be used to train a support vector machine to accurately differentiate AD and normal ageing scans obtained from another centre with different subjects and different scanner equipment. Patients with mild, clinically probable AD and age/sex matched controls were correctly separated in 89% of cases which is compatible with published diagnosis rates in the best clinical centres. This method correctly assigned 89% of patients with post-mortem confirmed diagnosis of either AD or frontotemporal lobar degeneration to their respective group. Our study leads to three conclusions: Firstly, support vector machines successfully separate patients with AD from healthy aging subjects. Secondly, they perform well in the differential diagnosis of two different forms of dementia. Thirdly, the method is robust and can be generalized across different centres. This suggests an important role for computer based diagnostic image analysis for clinical practice.				Jack, Clifford/F-2508-2010; Frackowiak, Richard/I-1809-2013; Frackowiak, Richard/H-4383-2011; Fox, Nick/B-1319-2009; Ashburner, John/I-3757-2013; Draganski, Bogdan/C-3096-2016	Jack, Clifford/0000-0001-7916-622X; Frackowiak, Richard/0000-0002-3151-822X; Fox, Nick/0000-0002-6660-657X; Ashburner, John/0000-0001-7605-2518; Draganski, Bogdan/0000-0002-5159-5919; Rohrer, Jonathan/0000-0002-6155-8417												0006-8950					MAR	2008	131		3				681	689		10.1093/brain/awm319						WOS:000253489800011	18202106	J	CARPENTER, GA; GROSSBERG, S; REYNOLDS, JH				CARPENTER, GA; GROSSBERG, S; REYNOLDS, JH			ARTMAP - SUPERVISED REAL-TIME LEARNING AND CLASSIFICATION OF NONSTATIONARY DATA BY A SELF-ORGANIZING NEURAL NETWORK	NEURAL NETWORKS												This article introduces a new neural network architecture, called ARTMAP, that autonomously learns to classify arbitrarily many, arbitrarily ordered vectors into recognition categories based on predictive success. This supervised learning system is built up from a pair of Adaptive Resonance Theory modules (ART(a) and ART(b)) that are capable of self-organizing stable recognition categories in response to arbitrary sequences of input patterns. During training trials, the ART(a) module receives a stream [a(p)] of input patterns, and ART(b) receives a stream [b(p)] of input patterns, where b(p) is the correct prediction given a(p). These ART modules are linked by an associative learning network and an internal controller that ensures autonomous system operation in real time. During test trials, the remaining patterns a(p) are presented without b(p), and their predictions at ART(b) are compared with b(p). Tested on a benchmark machine learning database in both on-line and off-line simulations, the ARTMAP system learns orders of magnitude more quickly, efficiently, and accurately than alternative algorithms, and achieves 100% accuracy after training on less than half the input patterns in the database. It achieves these properties by using an internal controller that conjointly maximizes predictive generalization and minimizes predictive error by linking predictive success to category size on a trial-by-trial basis, using only local operations. This computation increases the vigilance parameter rho-a of ART(a) by the minimal amount needed to correct a predictive error at ART(b). Parameter rho-a calibrates the minimum confidence that ART(a) must have in a category, or hypothesis, activated by an input a(p) in order for ART(a) to accept that category, rather than search for a better one through an automatically controlled process of hypothesis testing. Parameter rho-a is compared with the degree of match between a(p) and the top-down learned expectation, or prototype, that is read-out subsequent to activation of an ART(a) category. Search occurs if the degree of match is less than rho-a. ARTMAP is hereby a type of self-organizing expert system that calibrates the selectivity of its hypotheses based upon predictive success. As a result, rare but important events can be quickly and sharply distinguished even if they are similar to frequent events with different consequences. Between input trials rho-a relaxes to a baseline vigilance rho-aBAR. When rho-aBAR is large, the system runs in a conservative mode, wherein predictions are made only if the system is confident of the outcome. Very few false-alarm errors then occur at any stage of learning, yet the system reaches asymptote with no loss of speed. Because ARTMAP learning is self-stabilizing, it can continue learning one or more databases, without degrading its corpus of memories, until its full memory capacity is utilized.																	0893-6080						1991	4	5					565	588		10.1016/0893-6080(91)90012-T						WOS:A1991GG49600002		J	SAMUEL, AL				SAMUEL, AL			SOME STUDIES IN MACHINE LEARNING USING THE GAME OF CHECKERS	IBM JOURNAL OF RESEARCH AND DEVELOPMENT																													0018-8646						1959	3	3					211	&								WOS:A1959WX53200001		J	Franklin, J				Franklin, J			Predictive vegetation mapping: Geographic modelling of biospatial patterns in relation to environmental gradients	PROGRESS IN PHYSICAL GEOGRAPHY												Predictive vegetation mapping can be defined as predicting the geographic distribution of the vegetation composition across a landscape from mapped environmental variables. Computerized predictive vegetation mapping is made possible by the availability of digital maps of topography and other environmental variables such as soils, geology and climate variables, and geographic information system software for manipulating these data. Especially important to predictive vegetation mapping are interpolated climatic variables related to physiological tolerances, and topographic variables, derived from digital elevation grids, related to site energy and moisture balance. Predictive vegetation mapping is founded in ecological niche theory and gradient analysis, and driven by the need to map vegetation patterns over large areas for resource conservation planning, and to predict the effects of environmental change on vegetation distributions. Predictive vegetation mapping has advanced over the past two decades especially in conjunction with the development of remote sensing-based vegetation mapping and digital geographic information analysis. A number of statistical and, more recently, machine-learning methods have been used to develop and implement predictive vegetation models.																	0309-1333					DEC	1995	19	4					474	499		10.1177/030913339501900403						WOS:A1995TK67900003		J	Yang, J; Frangi, AF; Yang, JY; Zhang, D; Jin, Z				Yang, J; Frangi, AF; Yang, JY; Zhang, D; Jin, Z			KPCA plus LDA: A complete kernel fisher discriminant framework for feature extraction and recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												This paper examines the theory of kernel Fisher discriminant analysis (KFD) in a Hilbert space and develops a two-phase KFD framework, i.e., kernel principal component analysis (KPCA) plus Fisher linear discriminant analysis (LDA). This framework provides novel insights into the nature of KFD. Based on this framework, the authors propose a complete kernel Fisher discriminant analysis (CKFD) algorithm. CKFD can be used to carry out discriminant analysis in "double discriminant subspaces." The fact that, it can make full use of two kinds of discriminant information, regular and irregular, makes CKFD a more powerful discriminator. The proposed algorithm was tested and evaluated using the FERET face database and the CENPARMI handwritten numeral database. The experimental results show that CKFD outperforms other KFD algorithms.				Zhang, David/O-9396-2016; Frangi, Alejandro/C-6500-2008	Zhang, David/0000-0002-5027-5286; Frangi, Alejandro/0000-0002-2675-528X												0162-8828	1939-3539				FEB	2005	27	2					230	244		10.1109/TPAMI.2005.33						WOS:000225689300006	15688560	J	Ding, CHQ; Dubchak, I				Ding, CHQ; Dubchak, I			Multi-class protein fold recognition using support vector machines and neural networks	BIOINFORMATICS												Motivation: Protein fold recognition is an important approach to structure discovery without relying on sequence similarity. We study this approach with new multi-class classification methods and examined many issues important for a practical recognition system. Results: Most current discriminative methods for protein fold prediction use the one-against-others method, which has the well-known 'False Positives' problem. We investigated two new methods: the unique one-against-others and the all-against-all methods. Both improve prediction accuracy by 14-110% on a dataset containing 27 SCOP folds. We used the Support Vector Machine (SVM) and the Neural Network (NN) learning methods as base classifiers. SVMs converges fast and leads to high accuracy. When scores of multiple parameter datasets are combined, majority voting reduces noise and increases recognition accuracy. We examined many issues involved with large number of classes, including dependencies of prediction accuracy on the number of folds and on the number of representatives in a fold. Overall, recognition systems achieve 56% fold prediction accuracy on a protein test dataset, where most of the proteins have below 25% sequence identity with the proteins used in training.																	1367-4803					APR	2001	17	4					349	358		10.1093/bioinformatics/17.4.349						WOS:000168325600008	11301304	J	Scholkopf, B; Sung, KK; Burges, CJC; Girosi, F; Niyogi, P; Poggio, T; Vapnik, V				Scholkopf, B; Sung, KK; Burges, CJC; Girosi, F; Niyogi, P; Poggio, T; Vapnik, V			Comparing support vector machines with Gaussian kernels to radial basis function classifiers	IEEE TRANSACTIONS ON SIGNAL PROCESSING												The support vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights, and threshold that minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by k-means clustering, and the weights are computed using error backpropagation. We consider three machines, namely, a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system. The SV approach is thus not only theoretically well-founded but also superior in a practical application.				Scholkopf, Bernhard/A-7570-2013													1053-587X	1941-0476				NOV	1997	45	11					2758	2765		10.1109/78.650102						WOS:A1997YE90500014		J	Nielsen, H; Brunak, S; von Heijne, G				Nielsen, H; Brunak, S; von Heijne, G			Machine learning approaches for the prediction of signal peptides and other protein sorting signals	PROTEIN ENGINEERING												Prediction of protein sorting signals from the sequence of amino acids has great importance in the field of proteomics today, Recently, the growth of protein databases, combined with machine learning approaches, such as neural networks and hidden Markov models, have made it possible to achieve a level of reliability where practical use in, for example automatic database annotation is feasible. In this review we concentrate on the present status and future perspectives of SignalP, our neural network-based method for prediction of the most well-known sorting signal: the secretory signal peptide. We discuss the problems associated with the use of SignalP on genomic sequences, showing that signal peptide prediction will improve further if integrated with predictions of start codons and transmembrane helices, As a step towards this goal, a hidden Markov model version of SignalP has been developed, making it possible to discriminate between cleaved signal peptides and uncleaved signal anchors. Furthermore, we show how SignalP can be used to characterize putative signal peptides from an archaeon, Methanococcus jannaschii, Finally, we briefly review a few methods for predicting other protein sorting signals and discuss the future of protein sorting prediction in general.				Nielsen, Henrik/D-4128-2011; von Heijne, Gunnar/F-5576-2011	Nielsen, Henrik/0000-0002-9412-9643; von Heijne, Gunnar/0000-0002-4490-8569												0269-2139					JAN	1999	12	1					3	9		10.1093/protein/12.1.3						WOS:000078450700002	10065704	J	Hofmann, T				Hofmann, T			Latent semantic models for collaborative filtering	ACM TRANSACTIONS ON INFORMATION SYSTEMS												Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.																	1046-8188	1558-2868				JAN	2004	22	1					89	115		10.1145/963770.963774						WOS:000188554900004		J	Alcala-Fdez, J; Sanchez, L; Garcia, S; del Jesus, MJ; Ventura, S; Garrell, JM; Otero, J; Romero, C; Bacardit, J; Rivas, VM; Fernandez, JC; Herrera, F				Alcala-Fdez, J.; Sanchez, L.; Garcia, S.; del Jesus, M. J.; Ventura, S.; Garrell, J. M.; Otero, J.; Romero, C.; Bacardit, J.; Rivas, V. M.; Fernandez, J. C.; Herrera, F.			KEEL: a software tool to assess evolutionary algorithms for data mining problems	SOFT COMPUTING												This paper introduces a software tool named KEEL which is a software tool to assess evolutionary algorithms for Data Mining problems of various kinds including as regression, classification, unsupervised learning, etc. It includes evolutionary learning algorithms based on different approaches: Pittsburgh, Michigan and IRL, as well as the integration of evolutionary learning techniques with different pre-processing techniques, allowing it to perform a complete analysis of any learning model in comparison to existing software tools. Moreover, KEEL has been designed with a double goal: research and educational.				Ventura, Sebastian/A-7753-2008; Herrera, Francisco/C-6856-2008; Romero, Cristobal/H-4782-2011; Alcala-Fdez, Jesus/C-6795-2012; Bacardit, Jaume/F-9293-2014; Otero, Jose/K-7636-2014; Sanchez, Luciano/K-8715-2014; Del Jesus, Maria Jose/D-3932-2012; Garcia, Salvador/N-3624-2013; Fernandez, Juan Carlos/K-9840-2014; Mangan, Rachel/A-8824-2008; Rivas Santos, Victor Manuel/L-9937-2014	Ventura, Sebastian/0000-0003-4216-6378; Herrera, Francisco/0000-0002-7283-312X; Romero, Cristobal/0000-0003-4180-4948; Alcala-Fdez, Jesus/0000-0002-6190-3575; Bacardit, Jaume/0000-0002-2692-7205; Otero, Jose/0000-0002-5974-0893; Sanchez, Luciano/0000-0002-2446-1915; Del Jesus, Maria Jose/0000-0002-7891-3059; Garcia, Salvador/0000-0003-4494-7565; Fernandez, Juan Carlos/0000-0001-8849-6036; Mangan, Rachel/0000-0002-8788-9214; Rivas Santos, Victor Manuel/0000-0002-0681-7172												1432-7643	1433-7479				FEB	2009	13	3					307	318		10.1007/s00500-008-0323-y						WOS:000260518500008		J	Xia, JG; Psychogios, N; Young, N; Wishart, DS				Xia, Jianguo; Psychogios, Nick; Young, Nelson; Wishart, David S.			MetaboAnalyst: a web server for metabolomic data analysis and interpretation	NUCLEIC ACIDS RESEARCH												Metabolomics is a newly emerging field of 'omics' research that is concerned with characterizing large numbers of metabolites using NMR, chromatography and mass spectrometry. It is frequently used in biomarker identification and the metabolic profiling of cells, tissues or organisms. The data processing challenges in metabolomics are quite unique and often require specialized (or expensive) data analysis software and a detailed knowledge of cheminformatics, bioinformatics and statistics. In an effort to simplify metabolomic data analysis while at the same time improving user accessibility, we have developed a freely accessible, easy-to-use web server for metabolomic data analysis called MetaboAnalyst. Fundamentally, MetaboAnalyst is a web-based metabolomic data processing tool not unlike many of today's web-based microarray analysis packages. It accepts a variety of input data (NMR peak lists, binned spectra, MS peak lists, compound/concentration data) in a wide variety of formats. It also offers a number of options for metabolomic data processing, data normalization, multivariate statistical analysis, graphing, metabolite identification and pathway mapping. In particular, MetaboAnalyst supports such techniques as: fold change analysis, t-tests, PCA, PLS-DA, hierarchical clustering and a number of more sophisticated statistical or machine learning methods. It also employs a large library of reference spectra to facilitate compound identification from most kinds of input spectra. MetaboAnalyst guides users through a step-by-step analysis pipeline using a variety of menus, information hyperlinks and check boxes. Upon completion, the server generates a detailed report describing each method used, embedded with graphical and tabular outputs. MetaboAnalyst is capable of handling most kinds of metabolomic data and was designed to perform most of the common kinds of metabolomic data analyses. MetaboAnalyst is accessible at http://www.metaboanalyst.ca				Psychogios, Nikolaos/B-6867-2011	Psychogios, Nikolaos/0000-0002-2747-6012; Wishart, David S/0000-0002-3207-2434												0305-1048					JUL 1	2009	37						W652	W660		10.1093/nar/gkp356						WOS:000267889100112	19429898	B	Steinwart, I; Christmann, A	Steinwart, I; Christmann, A			Steinwart, Ingo; Christmann, Andreas	Steinwart, I; Christmann, A		Support Vector Machines Introduction	SUPPORT VECTOR MACHINES	Information Science and Statistics											Overview. The goal of this introduction is to give a gentle and informal overview of what this book is about. In particular, we will discuss key concepts and questions on statistical learning. Furthermore, the underlying ideas of support vector machines are presented, and important questions for understanding their learning mechanisms will be raised. Usage. This introduction serves as a tutorial that presents key concepts and questions of this book. By connecting these to corresponding parts of the book, it is furthermore a guidepost for reading this book.																			978-0-387-77241-7				2008							1	+								WOS:000267334600001		J	Kwatra, V; Schodl, A; Essa, I; Turk, G; Bobick, A				Kwatra, V; Schodl, A; Essa, I; Turk, G; Bobick, A			Graphicut textures: Image and video synthesis using graph cuts	ACM TRANSACTIONS ON GRAPHICS					Annual Symposium of the ACM SIGGRAPH	JUL 27-31, 2003	SAN DIEGO, CALIFORNIA	ACM, SIGGRAPH				In this paper we introduce a new algorithm for image and video texture synthesis. In our approach, patch regions from a sample image or video are transformed and copied to the output and then stitched together along optimal seams to generate a new (and typically larger) output. In contrast to other techniques, the size of the patch is not chosen a-priori, but instead a graph cut technique is used to determine the optimal patch region for any given offset between the input and output texture. Unlike dynamic programming, our graph cut technique for seam optimization is applicable in any dimension. We specifically explore it in 2D and 3D to perform video texture synthesis in addition to regular image synthesis. We present approximative offset search techniques that work well in conjunction with the presented patch size optimization. We show results for synthesizing regular, random, and natural images and videos. We also demonstrate how this method can be used to interactively merge different images to generate new scenes.																	0730-0301					JUL	2003	22	3					277	286		10.1145/882262.882264						WOS:000184291700004		J	Yao, X; Liu, Y				Yao, X; Liu, Y			A new evolutionary system for evolving artificial neural networks	IEEE TRANSACTIONS ON NEURAL NETWORKS												This paper presents a new evolutionary system, i.e., EPNet, for evolving artificial neural networks (ANN's), The evolutionary algorithm used in EPNet is based on Fogel's evolutionary programming (EP), Unlike most previous studies on evolving ANN's, this paper puts its emphasis on evolving ANN's behaviors, This is one of the primary reasons why EP is adopted, Five mutation operators proposed in EPNet reflect such an emphasis on evolving behaviors, Close behavioral links between parents and their offspring are maintained by various mutations, such as partial training and node splitting, EPNet evolves ANN's architectures and connection weights (including biases) simultaneously in order to reduce the noise in fitness evaluation, The parsimony of evolved ANN's is encouraged by preferring node/connection deletion to addition, EPNet has been tested on a number of benchmark problems in machine learning and ANN's, such as the parity problem, the medical diagnosis problems (breast cancer, diabetes, heart disease, and thyroid), the Australian credit card assessment problem, and the Mackey-Glass time series prediction problem, The experimental results show that EPNet can produce very compact ANN's with good generalization ability in comparison with other algorithms.																	1045-9227	1941-0093				MAY	1997	8	3					694	713		10.1109/72.572107						WOS:A1997WX20800020	18255671	J	Hare, S; Saffari, A; Torr, PHS			IEEE	Hare, Sam; Saffari, Amir; Torr, Philip H. S.			Struck: Structured Output Tracking with Kernels	2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)					IEEE International Conference on Computer Vision (ICCV)	NOV 06-13, 2011	Barcelona, SPAIN	IEEE, Toyota, Google, Microsoft Res, Siemens, Technicolor, Adobe, Alcatel Lucent, Gentex Corp, Kooaba Image Recognit, Mitsubishi Elect, Mobileye, Object Video (OV), Toshiba, Xerox, Zeiss, 2d3, SATURNUS				Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (accurate estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we are able to avoid the need for an intermediate classification step. Our method uses a kernelized structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow for real-time application, we introduce a budgeting mechanism which prevents the unbounded growth in the number of support vectors which would otherwise occur during tracking. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased performance.																			978-1-4577-1102-2				2011							263	270								WOS:000300061900034		J	Su, AI; Welsh, JB; Sapinoso, LM; Kern, SG; Dimitrov, P; Lapp, H; Schultz, PG; Powell, SM; Moskaluk, CA; Frierson, HF; Hampton, GM				Su, AI; Welsh, JB; Sapinoso, LM; Kern, SG; Dimitrov, P; Lapp, H; Schultz, PG; Powell, SM; Moskaluk, CA; Frierson, HF; Hampton, GM			Molecular classification of human carcinomas by use of gene expression signatures	CANCER RESEARCH												Classification of human tumors according to their primary anatomical site of origin is fundamental for the optimal treatment of patient's with cancer. Here we describe the use of large-scale RNA profiling and supervised machine learning algorithms to construct a first-generation molecular classification scheme for carcinomas of the prostate, breast, lung, ovary, colorectum, kidney, liver, pancreas, bladder/ureter, and gastroesophagus, which collectively account for similar to 70% of all cancer-related deaths in the United States. The classification scheme was based on identifying gene subsets whose expression typifies each cancer class, and we quantified the extent to which these genes are characteristic of a specific tumor type by accurately and confidently predicting the anatomical site of tumor origin for 90% of 175 carcinomas, including 9 of 12 metastatic lesions. The predictor gene subsets include those whose expression is typical of specific types of normal epithelial differentiation, as well as other genes whose expression is elevated in cancer. This study demonstrates the feasibility of predicting the tissue origin of a carcinoma in the context of multiple cancer classes.				Lapp, Hilmar/A-8275-2009	Su, Andrew I./0000-0002-9859-4104												0008-5472					OCT 15	2001	61	20					7388	7393								WOS:000171707400005	11606367	J	Cohn, DA; Ghahramani, Z; Jordan, MI				Cohn, DA; Ghahramani, Z; Jordan, MI			Active learning with statistical models	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												For many types of machine learning algorithms, one can compute the statistically ''optimal'' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.																	1076-9757	1943-5037					1996	4						129	145								WOS:A1996UD02900001		J	Kubat, M; Holte, RC; Matwin, S				Kubat, M; Holte, RC; Matwin, S			Machine learning for the detection of oil spills in satellite radar images	MACHINE LEARNING												During a project examining the use of machine learning techniques for oil spill detection, we encountered several essential questions that we believe deserve the attention of the research community. We use our particular case study to illustrate such issues as problem formulation, selection of evaluation measures, and data preparation. We relate these issues to properties of the oil spill application, such as its imbalanced class distribution, that are shown to be common to many applications. Our solutions to these issues are implemented in the Canadian Environmental Hazards Detection System (CEHDS), which is about to undergo field testing.				reddy, indra/C-5363-2011	Matwin, Stan/0000-0001-6629-8434												0885-6125					FEB-MAR	1998	30	2-3					195	215		10.1023/A:1007452223027						WOS:000073342100004		J	Tao, DC; Tang, X; Li, XL; Wu, XD				Tao, DC; Tang, X; Li, XL; Wu, XD			Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM's optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM(AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random subspace SVM (RS-SVM). Finally, by integrating AB-SVM and RS-SVM, an asymmetric bagging and random subspace SVM (ABRS-SVM) is built to solve these three problems and further improve the relevance feedback performance.																	0162-8828					JUL	2006	28	7					1088	1099		10.1109/TPAMI.2006.134						WOS:000237424400006	16792098	J	Schmidhuber, J				Schmidhuber, Juergen			Deep learning in neural networks: An overview	NEURAL NETWORKS												In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks. (C) 2014 Published by Elsevier Ltd.																	0893-6080	1879-2782				JAN	2015	61						85	117		10.1016/j.neunet.2014.09.003						WOS:000347595400010	25462637	J	Shotton, J; Sharp, T; Kipman, A; Fitzgibbon, A; Finocchio, M; Blake, A; Cook, M; Moore, R				Shotton, Jamie; Sharp, Toby; Kipman, Alex; Fitzgibbon, Andrew; Finocchio, Mark; Blake, Andrew; Cook, Mat; Moore, Richard			Real-Time Human Pose Recognition in Parts from Single Depth Images	COMMUNICATIONS OF THE ACM												We propose a new method to quickly and accurately - predict human pose-the 3D positions of body joints-from a single depth image, without depending on information from preceding frames. Our approach is strongly rooted in current object recognition strategies. By designing an intermediate - representation in terms of body parts, the difficult pose estimation problem is transformed into a simpler per-pixel classification problem, for which efficient machine learning techniques exist. By using computer graphics to synthesize a very large dataset of training image pairs, one can train a classifier that estimates body part labels from test images invariant to pose, body shape, clothing, and other irrelevances. Finally, we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs in under 5ms on the Xbox 360. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state-of-the-art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.																	0001-0782					JAN	2013	56	1					116	124		10.1145/2398356.2398381						WOS:000312941900034		J	Huang, GB; Chen, L				Huang, Guang-Bin; Chen, Lei			Convex incremental extreme learning machine	NEUROCOMPUTING												Unlike the conventional neural network theories and implementations, Huang et al. [Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Transactions on Neural Networks 17(4) (2006) 879-892] have recently proposed a new theory to show that single-hidden-layer feedforward networks (SLFNs) with randomly generated additive or radial basis function (RBF) hidden nodes (according to any continuous sampling distribution) can work as universal approximators and the resulting incremental extreme learning machine (I-ELM) outperforms many popular learning algorithms. I-ELM randomly generates the hidden nodes and analytically calculates the output weights of SLFNs, however, I-ELM does not recalculate the output weights of all the existing nodes when a new node is added. This paper shows that while retaining the same simplicity, the convergence rate of I-ELM can be further improved by recalculating the output weights of the existing nodes based on a convex optimization method when a new hidden node is randomly added. Furthermore, we show that given a type of piecewise continuous computational hidden nodes (possibly not neural alike nodes), if SLFNs f(n)(x) = Sigma(n)(i=1) beta(i) G(x,a(i),b(i)) can work as universal approximators with adjustable hidden node parameters, V from a function approximation point of view the hidden node parameters of such "generalized" SLFNs (including sigmoid networks, RBF networks, trigonometric networks, threshold networks, fuzzy inference systems, fully complex neural networks, high-order networks, ridge polynomial networks, wavelet networks, etc.) can actually be randomly generated according to any continuous sampling distribution. In theory, the parameters of these SLFNs can be analytically determined by ELM instead of being tuned. (c)., 2007 Elsevier B.V. All rights reserved.				Huang, Guang-Bin/A-5035-2011	Huang, Guang-Bin/0000-0002-2480-4965												0925-2312					OCT	2007	70	16-18					3056	3062		10.1016/j.neucom.2007.02.009						WOS:000249908400047		J	Coifman, RR; Lafon, S; Lee, AB; Maggioni, M; Nadler, B; Warner, F; Zucker, SW				Coifman, RR; Lafon, S; Lee, AB; Maggioni, M; Nadler, B; Warner, F; Zucker, SW			Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA												We provide a framework for structural multiscale geometric organization of graphs and subsets of R-n. We use diffusion semigroups to generate multiscale geometries in order to organize and represent complex structures. We show that appropriately selected eigenfunctions or scaling functions of Markov matrices, which describe local transitions, lead to macroscopic descriptions at different scales. The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration. We provide a unified view of ideas from data analysis, machine learning, and numerical analysis.				Nadler, Boaz/C-7217-2008	Nadler, Boaz/0000-0002-9777-4576												0027-8424					MAY 24	2005	102	21					7426	7431		10.1073/pnas.0500334102						WOS:000229417500007	15899970	J	Mohan, A; Papageorgiou, C; Poggio, T				Mohan, A; Papageorgiou, C; Poggio, T			Example-based object detection in images by components	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												In this paper, we present a general example-based framework for detecting objects in static images by components. The technique is demonstrated by developing a system that locates people in cluttered scenes. The system is structured with four distinct example-based detectors that are trained to separately find the four components of the human body: the head. legs, left arm, and right arm. After ensuring that these components are present in the proper geometric configuration, a second example-based classifier combines the results of the component detectors to classify a pattern as either a "person" or a "nonperson." We call this type of hierarchical architecture, in which learning occurs at multiple stages, an Adaptive Combination of Classifiers (ACC). We present results that show that this system performs significantly better than a similar full-body person detector. This suggests that the improvement in performance is due to the component-based approach and the ACC data classification architecture. The algorithm is also more robust than the full-body person detection method in that it is capable of locating partially occluded Views of people and people whose body parts have little contrast with the background.																	0162-8828	1939-3539				APR	2001	23	4					349	361		10.1109/34.917571						WOS:000168067900002		J	Rosten, E; Porter, R; Drummond, T				Rosten, Edward; Porter, Reid; Drummond, Tom			Faster and Better: A Machine Learning Approach to Corner Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												The repeatability and efficiency of a corner detector determines how likely it is to be useful in a real-world application. The repeatability is important because the same scene viewed from different positions should yield features which correspond to the same real-world 3D locations [1]. The efficiency is important because this determines whether the detector combined with further processing can operate at frame rate. Three advances are described in this paper. First, we present a new heuristic for feature detection and, using machine learning, we derive a feature detector from this which can fully process live PAL video using less than 5 percent of the available processing time. By comparison, most other detectors cannot even operate at frame rate (Harris detector 115 percent, SIFT 195 percent). Second, we generalize the detector, allowing it to be optimized for repeatability, with little loss of efficiency. Third, we carry out a rigorous comparison of corner detectors based on the above repeatability criterion applied to 3D scenes. We show that, despite being principally constructed for speed, on these stringent tests, our heuristic detector significantly outperforms existing feature detectors. Finally, the comparison demonstrates that using machine learning produces significant improvements in repeatability, yielding a detector that is both very fast and of very high quality.				Drummond, Tom/A-4696-2011	Drummond, Tom/0000-0001-8204-5904												0162-8828	1939-3539				JAN	2010	32	1					105	119		10.1109/TPAMI.2008.275						WOS:000271826700009	19926902	J	Carmena, JM; Lebedev, MA; Crist, RE; O'Doherty, JE; Santucci, DM; Dimitrov, DF; Patil, PG; Henriquez, CS; Nicolelis, MAL				Carmena, JM; Lebedev, MA; Crist, RE; O'Doherty, JE; Santucci, DM; Dimitrov, DF; Patil, PG; Henriquez, CS; Nicolelis, MAL			Learning to control a brain-machine interface for reaching and grasping by primates	PLOS BIOLOGY												Reaching and grasping in primates depend on the coordination of neural activity in large frontoparietal ensembles. Here we demonstrate that primates can learn to reach and grasp virtual objects by controlling a robot arm through a closed-loop brain-machine interface [BMIc) that uses multiple mathematical models to extract several motor parameters (i.e., hand position, velocity, gripping force, and the EMGs of multiple arm muscles) from the electrical activity of frontoparietal neuronal ensembles. As single neurons typically contribute to the encoding of several motor parameters, we observed that high BMIc accuracy required recording from large neuronal ensembles. Continuous BMIc operation by monkeys led to significant improvements in both model predictions and behavioral performance. Using visual feedback, monkeys succeeded in producing robot reach-and-grasp movements even when their arms did not move. Learning to operate the BMIc was paralleled by functional reorganization in multiple cortical areas, suggesting that the dynamic properties of the BMIc were incorporated into motor and sensory cortical representations.				O'Doherty, Joseph/D-5116-2009; Patil, Parag/D-1618-2016; Lebedev, Mikhail/H-5066-2016	O'Doherty, Joseph/0000-0001-8175-5699; Lebedev, Mikhail/0000-0003-0355-8723												1544-9173					NOV	2003	1	2					193	208	e42	10.1371/journal.pbio.0000042						WOS:000188835200011		J	Raymond, JL; Lisberger, SG; Mauk, MD				Raymond, JL; Lisberger, SG; Mauk, MD			The cerebellum: A neuronal learning machine?	SCIENCE												Comparison of two seemingly quite different behaviors yields a surprisingly consistent picture of the role of the cerebellum in motor learning, Behavioral and physiological data about classical conditioning of the eyelid response and motor learning in the vestibuloocular reflex suggest that (i) plasticity is distributed between the cerebellar cortex and the deep cerebellar nuclei; (ii) the cerebellar cortex plays a special role in learning the timing of movement; and (iii) the cerebellar cortex guides learning in the deep nuclei, which may allow learning to be transferred from the cortex to the deep nuclei. Because many of the similarities in the data from the two systems typify general features of cerebellar organization, the cerebellar mechanisms of learning in these two systems may represent principles that apply to many motor systems.																	0036-8075	1095-9203				MAY 24	1996	272	5265					1126	1131		10.1126/science.272.5265.1126						WOS:A1996UM88900038	8638157	J	Gonen, M; Alpaydin, E				Gonen, Mehmet; Alpaydin, Ethem			Multiple Kernel Learning Algorithms	JOURNAL OF MACHINE LEARNING RESEARCH												In recent years, several methods have been proposed to combine multiple kernels instead of using a single one. These different kernels may correspond to using different notions of similarity or may be using information coming from multiple sources (different representations or different feature subsets). In trying to organize and highlight the similarities and differences between them, we give a taxonomy of and review several multiple kernel learning algorithms. We perform experiments on real data sets for better illustration and comparison of existing algorithms. We see that though there may not be large differences in terms of accuracy, there is difference between them in complexity as given by the number of stored support vectors, the sparsity of the solution as given by the number of used kernels, and training time complexity. We see that overall, using multiple kernels instead of a single one is useful and believe that combining kernels in a nonlinear or data-dependent way seems more promising than linear combination in fusing information provided by simple linear kernels, whereas linear methods are more reasonable when combining complex Gaussian kernels.				xiankai, chen/A-4624-2009; Gonen, Mehmet/E-8270-2012; ALPAYDIN, ETHEM/E-6127-2013; Gonen, Mehmet/O-7322-2015													1532-4435					JUL	2011	12						2211	2268								WOS:000293757900004		J	la Cour, T; Kiemer, L; Molgaard, A; Gupta, R; Skriver, K; Brunak, S				la Cour, T; Kiemer, L; Molgaard, A; Gupta, R; Skriver, K; Brunak, S			Analysis and prediction of leucine-rich nuclear export signals	PROTEIN ENGINEERING DESIGN & SELECTION												We present a thorough analysis of nuclear export signals and a prediction server, which we have made publicly available. The machine learning prediction method is a significant improvement over the generally used consensus patterns. Nuclear export signals (NESs) are extremely important regulators of the subcellular location of proteins. This regulation has an impact on transcription and other nuclear processes, which are fundamental to the viability of the cell. NESs are studied in relation to cancer, the cell cycle, cell differentiation and other important aspects of molecular biology. Our conclusion from this analysis is that the most important properties of NESs are accessibility and flexibility allowing relevant proteins to interact with the signal. Furthermore, we show that not only the known hydrophobic residues are important in defining a nuclear export signals. We employ both neural networks and hidden Markov models in the prediction algorithm and verify the method on the most recently discovered NESs. The NES predictor (NetNES) is made available for general use at http://www.cbs.dtu.dk/.				Gupta, Ramneek/G-7278-2012; Skriver, Karen/K-9860-2014	Gupta, Ramneek/0000-0001-6841-6676; Skriver, Karen/0000-0003-2225-4012												1741-0126					JUN	2004	17	6					527	536		10.1093/protein/gzh062						WOS:000224704300004	15314210	J	CHENG, B; TITTERINGTON, DM				CHENG, B; TITTERINGTON, DM			NEURAL NETWORKS - A REVIEW FROM A STATISTICAL PERSPECTIVE	STATISTICAL SCIENCE												This paper informs a statistical readership about Artificial Neural Networks (ANNs), points out some of the links with statistical methodology and encourages cross-disciplinary research in the directions most likely to bear fruit. The areas of statistical interest are briefly outlined, and a series of examples indicates the flavor of ANN models. We then treat various topics in more depth. In each case, we describe the neural network architectures and training rules and provide a statistical commentary. The topics treated in this way are perceptrons (from single-unit to multilayer versions), Hopfield-type recurrent networks (including probabilistic versions strongly related to statistical physics and Gibbs distributions) and associative memory networks trained by so-called unsupervised learning rules. Perceptrons are shown to have strong associations with discriminant analysis and regression, and unsupervized networks with cluster analysis. The paper concludes with some thoughts on the future of the interface between neural networks and statistics.																	0883-4237					FEB	1994	9	1					2	30		10.1214/ss/1177010638						WOS:A1994NG28500001		J	Alcala-Fdez, J; Fernandez, A; Luengo, J; Derrac, J; Garcia, S; Sanchez, L; Herrera, F				Alcala-Fdez, J.; Fernandez, A.; Luengo, J.; Derrac, J.; Garcia, S.; Sanchez, L.; Herrera, F.			KEEL Data-Mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis Framework	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING					10th International Conference on Intelligent Data Engineering and Automated Learning (IDEAL 2009)	SEP 23-26, 2009	Burgos, SPAIN	Junta Castilla & Leon, Univ Burgos, Diputac Burgos, Ayuntamiento Burgos, GCI, CSA, FAE, FEC				This work is related to the KEEL1 (Knowledge Extraction based on Evolutionary Learning) tool, an open source software that supports data management and a designer of experiments. KEEL pays special attention to the implementation of evolutionary learning and soft computing based techniques for Data Mining problems including regression, classification, clustering, pattern mining and so on. The aim of this paper is to present three new aspects of KEEL: KEEL-dataset, a data set repository which includes the data set partitions in the KEEL format and shows some results of algorithms in these data sets; some guidelines for including new algorithms in KEEL, helping the researchers to make their methods easily accessible to other authors and to compare the results of many approaches already included within the KEEL software; and a module of statistical procedures developed in order to provide to the researcher a suitable tool to contrast the results obtained in any experimental study. A case of study is given to illustrate a complete case of application within this experimental analysis framework.				Herrera, Francisco/C-6856-2008; Alcala-Fdez, Jesus/C-6795-2012; Fernandez, Alberto/G-3827-2014; Sanchez, Luciano/K-8715-2014; Luengo, Julian/L-6569-2014; Garcia, Salvador/N-3624-2013; Luengo, Julian/D-1307-2017	Herrera, Francisco/0000-0002-7283-312X; Alcala-Fdez, Jesus/0000-0002-6190-3575; Sanchez, Luciano/0000-0002-2446-1915; Luengo, Julian/0000-0003-3952-3629; Garcia, Salvador/0000-0003-4494-7565; Luengo, Julian/0000-0003-3952-3629												1542-3980						2011	17	2-3			SI		255	287								WOS:000288045700008		J	Uijlings, JRR; van de Sande, KEA; Gevers, T; Smeulders, AWM				Uijlings, J. R. R.; van de Sande, K. E. A.; Gevers, T.; Smeulders, A. W. M.			Selective Search for Object Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION												This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/similar to uijlings/SelectiveSearch.html).																	0920-5691					SEP	2013	104	2					154	171		10.1007/s11263-013-0620-5						WOS:000322251800003		J	Statnikov, A; Aliferis, CF; Tsamardinos, I; Hardin, D; Levy, S				Statnikov, A; Aliferis, CF; Tsamardinos, I; Hardin, D; Levy, S			A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis	BIOINFORMATICS												Motivation: Cancer diagnosis is one of the most important emerging clinical applications of gene expression microarray technology. We are seeking to develop a computer system for powerful and reliable cancer diagnostic model creation based on microarray data. To keep a realistic perspective on clinical applications we focus on multicategory diagnosis. To equip the system with the optimum combination of classifier, gene selection and cross-validation methods, we performed a systematic and comprehensive evaluation of several major algorithms for multicategory classification, several gene selection methods, multiple ensemble classifier methods and two cross-validation designs using 11 datasets spanning 74 diagnostic categories and 41 cancer types and 12 normal tissue types. Results: Multicategory support vector machines (MC-SVMs) are the most effective classifiers in performing accurate cancer diagnosis from gene expression data. The MC-SVM techniques by Crammer and Singer, Weston and Watkins and one-versus-rest were found to be the best methods in this domain. MC-SVMs outperform other popular machine learning algorithms, such as k-nearest neighbors, backpropagation and probabilistic neural networks, often to a remarkable degree. Gene selection techniques can significantly improve the classification performance of both MC-SVMs and other non-SVM learning algorithms. Ensemble classifiers do not generally improve performance of the best non-ensemble models. These results guided the construction of a software system GEMS (Gene Expression Model Selector) that automates high-quality model construction and enforces sound optimization and performance estimation procedures. This is the first such system to be informed by a rigorous comparative analysis of the available algorithms and datasets.				Hardin, Douglas/C-3386-2013	Hardin, Douglas/0000-0003-0867-2146												1367-4803					MAR 1	2005	21	5					631	643		10.1093/bioinformatics/bti033						WOS:000227241200010	15374862	J	Kohonen, T; Oja, E; Simula, O; Visa, A; Kangas, J				Kohonen, T; Oja, E; Simula, O; Visa, A; Kangas, J			Engineering applications of the self-organizing map	PROCEEDINGS OF THE IEEE												The self-organizing map (SOM) method is a new, powerful software teal for the visualization of high-dimensional data. It concerts complex, nonlinear statistical relationships between high-dimensional data into simple geometric relationships on a low-dimensional display. As if thereby compresses information while preserving the most important topological and metric relationships of the primary data elements on the display, it may also be thought ro produce some kind of abstractions. These two aspects, visualization and abstraction, occur in a number of complex engineering tasks such as process analysis, machine perception, control, and communication. The term self-organizing map signifies a class of mappings defined by error-theoretic consideration. In practice they result in certain unsupervised, competitive learning processes, computed by simple-looking SOM algorithms, The first SOM algorithms were conceived around 1981-1982, and the popularity of the more advanced SOM methods is growing at a steady pace. Many industries have found the SOM-based software tools useful. The most important property of the SOM, orderliness of the input-output mapping, can be utilized for many tasks: reduction of the amount of training data, speeding up learning, nonlinear interpolation and extrapolation, generalization, and effective compression of information for its transmission.				Visa, Ari/G-4304-2014													0018-9219	1558-2256				OCT	1996	84	10					1358	1384		10.1109/5.537105						WOS:A1996VK62400002		J	Sokolova, M; Lapalme, G				Sokolova, Marina; Lapalme, Guy			A systematic analysis of performance measures for classification tasks	INFORMATION PROCESSING & MANAGEMENT												This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier's evaluation (measure invariance). The result is the Measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification Supplements the discussion with several case studies. (C) 2009 Elsevier Ltd. All rights reserved.																	0306-4573	1873-5371				JUL	2009	45	4					427	437		10.1016/j.ipm.2009.03.002						WOS:000267170100003		J	Loh, WY; Shih, YS				Loh, WY; Shih, YS			Split selection methods for classification trees	STATISTICA SINICA												Classification trees based on exhaustive search algorithms tend to be biased towards selecting variables that afford more splits. As a result, such trees should be interpreted with caution. This article presents an algorithm called QUEST that has negligible bias. its split selection strategy shares similarities with the FACT method, but it yields binary splits and the final tree can be selected by a direct stopping rule or by pruning. Real and simulated data are used to compare QUEST with the exhaustive search approach. QUEST is shown to be substantially faster and the size and classification accuracy of its trees are typically comparable to those of exhaustive search.																	1017-0405					OCT	1997	7	4					815	840								WOS:A1997YF24300001		J	Agarwal, S; Awan, A; Roth, D				Agarwal, S; Awan, A; Roth, D			Learning to detect objects in images via a sparse, part-based representation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE					7th European Conference on Computer Vision (ECCV 2002)	MAY 28-31, 2002	COPENHAGEN, DENMARK	IT Univ Copenhagen, Univ Copenhagen, Lund Univ				We study the problem of detecting objects in still, gray-scale images. Our primary focus is the development of a learning-based approach to the problem that makes use of a sparse, part-based representation. A vocabulary of distinctive object parts is automatically constructed from a set of sample images of the object class of interest; images are then represented using parts from this vocabulary, together with spatial relations observed among the parts. Based on this representation, a learning algorithm is used to automatically learn to detect instances of the object class in new images. The approach can be applied to any object with distinguishable parts in a relatively fixed spatial configuration; it is evaluated here on difficult sets of real-world images containing side views of cars, and is seen to successfully detect objects in varying conditions amidst background clutter and mild occlusion. In evaluating object detection approaches, several important methodological issues arise that have not been satisfactorily addressed in previous work. A secondary focus of this paper is to highlight these issues and to develop rigorous evaluation standards for the object detection problem. A critical evaluation of our approach under the proposed standards is presented.																	0162-8828	1939-3539				NOV	2004	26	11					1475	1490		10.1109/TPAMI.2004.108						WOS:000223737000007	15521495	J	Frank, E; Hall, M; Trigg, L; Holmes, G; Witten, IH				Frank, E; Hall, M; Trigg, L; Holmes, G; Witten, IH			Data mining in bioinformatics using Weka	BIOINFORMATICS												The Weka machine learning workbench provides a general-purpose environment for automatic classification, regression, clustering and feature selection-common data mining problems in bioinformatics research. It contains an extensive collection of machine learning algorithms and data pre-processing methods complemented by graphical user interfaces for data exploration and the experimental comparison of different machine learning techniques on the same problem. Weka can process data given in the form of a single relational table. Its main objectives are to (a) assist users in extracting useful information from data and (b) enable them to easily identify a suitable algorithm for generating an accurate predictive model from it.				Frank, Eibe/A-1434-2008; Witten, Ian/A-3366-2012	Frank, Eibe/0000-0001-6152-7111; Witten, Ian/0000-0001-6428-8988; Holmes, Geoffrey/0000-0003-0433-8925												1367-4803					OCT 12	2004	20	15					2479	2481		10.1093/bioinformatics/bth261						WOS:000224481900024	15073010	J	Biskup, D				Biskup, D			Single-machine scheduling with learning considerations	EUROPEAN JOURNAL OF OPERATIONAL RESEARCH												The focus of this work is to analyze learning in single-machine scheduling problems. It is surprising that the well-known learning effect has never been considered in connection with scheduling problems. It is shown in this paper that even with the introduction of learning to job processing times two important types of single-machine problems remain polynomially solvable. (C) 1999 Elsevier Science B.V. All rights reserved.																	0377-2217					MAY 16	1999	115	1					173	178		10.1016/S0377-2217(98)00246-X						WOS:000079244900011		J	Coskun, A; Banaszak, M; Astumian, RD; Stoddart, JF; Grzybowski, BA				Coskun, Ali; Banaszak, Michal; Astumian, R. Dean; Stoddart, J. Fraser; Grzybowski, Bartosz A.			Great expectations: can artificial molecular machines deliver on their promise?	CHEMICAL SOCIETY REVIEWS												The development and fabrication of mechanical devices powered by artificial molecular machines is one of the contemporary goals of nanoscience. Before this goal can be realized, however, we must learn how to control the coupling/uncoupling to the environment of individual switchable molecules, and also how to integrate these bistable molecules into organized, hierarchical assemblies that can perform significant work on their immediate environment at nano-, micro-and macroscopic levels. In this tutorial review, we seek to draw an all-important distinction between artificial molecular switches which are now ten a penny-or a dime a dozen-in the chemical literature and artificial molecular machines which are few and far between despite the ubiquitous presence of their naturally occurring counterparts in living systems. At the single molecule level, a prevailing perspective as to how machine-like characteristics may be achieved focuses on harnessing, rather than competing with, the ineluctable effects of thermal noise. At the macroscopic level, one of the major challenges inherent to the construction of machine-like assemblies lies in our ability to control the spatial ordering of switchable molecules-e. g., into linear chains and then into muscle-like bundles-and to influence the cross-talk between their switching kinetics. In this regard, situations where all the bistable molecules switch synchronously appear desirable for maximizing mechanical power generated. On the other hand, when the bistable molecules switch "out of phase,'' the assemblies could develop intricate spatial or spatiotemporal patterns. Assembling and controlling synergistically artificial molecular machines housed in highly interactive and robust architectural domains heralds a game-changer for chemical synthesis and a defining moment for nanofabrication.				COSKUN, Ali/C-1045-2008; Stoddart, James /H-1518-2011; Grzybowski, Bartosz/B-7644-2009; Banaszak, Michal /A-9411-2010	COSKUN, Ali/0000-0002-4760-1546; Banaszak, Michal /0000-0003-0106-632X												0306-0012						2012	41	1					19	30		10.1039/c1cs15262a						WOS:000297654700002	22116531	J	Schmidt-Kittler, O; Ragg, T; Daskalakis, A; Granzow, M; Ahr, A; Blankenstein, TJF; Kaufmann, M; Diebold, J; Arnholdt, H; Muller, P; Bischoff, J; Harich, D; Schlimok, G; Riethmuller, G; Eils, R; Klein, CA				Schmidt-Kittler, O; Ragg, T; Daskalakis, A; Granzow, M; Ahr, A; Blankenstein, TJF; Kaufmann, M; Diebold, J; Arnholdt, H; Muller, P; Bischoff, J; Harich, D; Schlimok, G; Riethmuller, G; Eils, R; Klein, CA			From latent disseminated cells to overt metastasis: Genetic analysis of systemic breast cancer progression	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA												According to the present view, metastasis marks the end in a sequence of genomic changes underlying the progression of an epithelial cell to a lethal cancer. Here, we aimed to find out at what stage of tumor development transformed cells leave the primary tumor and whether a defined genotype corresponds to metastatic disease. To this end, we isolated single disseminated cancer cells from bone marrow of breast cancer patients and performed single-cell comparative genomic hybridization. We analyzed disseminated tumor cells from patients after curative resection of the primary tumor (stage MO), as presumptive progenitors of manifest metastasis, and from patients with manifest metastasis (stage M1). Their genomic data were compared with those from microdissected areas of matched primary tumors. Disseminated cells from MO-stage patients displayed significantly fewer chromosomal aberrations than primary tumors or cells from M1-stage patients (P < 0.008 and P < 0.0001, respectively), and their aberrations appeared to be randomly generated. In contrast, primary tumors and M1 cells harbored different and characteristic chromosomal imbalances. Moreover, applying machine-learning methods for the classification of the genotypes, we could correctly identify the presence or absence of metastatic disease in a patient on the basis of a single-cell genome. We suggest that in breast cancer, tumor cells may disseminate in a far less progressed genomic state than previously thought, and that they acquire genomic aberrations typical of metastatic cells thereafter. Thus, our data challenge the widely held view that the precursors of metastasis are derived from the most advanced clone within the primary tumor.				Eils, Roland/B-6121-2009	Eils, Roland/0000-0002-0034-4036												0027-8424					JUN 24	2003	100	13					7737	7742		10.1073/pnas.1331931100						WOS:000183845800060	12808139	J	Tao, DC; Li, XL; Wu, XD; Maybank, SJ				Tao, Dacheng; Li, Xuelong; Wu, Xindong; Maybank, Stephen J.			Geometric Mean for Subspace Selection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Subspace selection approaches are powerful tools in pattern classification and data visualization. One of the most important subspace approaches is the linear dimensionality reduction step in the Fisher's linear discriminant analysis (FLDA), which has been successfully employed in many fields such as biometrics, bioinformatics, and multimedia information management. However, the linear dimensionality reduction step in FLDA has a critical drawback: for a classification task with c classes, if the dimension of the projected subspace is strictly lower than c - 1, the projection to a subspace tends to merge those classes, which are close together in the original feature space. If separate classes are sampled from Gaussian distributions, all with identical covariance matrices, then the linear dimensionality reduction step in FLDA maximizes the mean value of the Kullback-Leibler (KL) divergences between different classes. Based on this viewpoint, the geometric mean for subspace selection is studied in this paper. Three criteria are analyzed: 1) maximization of the geometric mean of the KL divergences, 2) maximization of the geometric mean of the normalized KL divergences, and 3) the combination of 1 and 2. Preliminary experimental results based on synthetic data, UCI Machine Learning Repository, and handwriting digits show that the third criterion is a potential discriminative subspace selection method, which significantly reduces the class separation problem in comparing with the linear dimensionality reduction step in FLDA and its several representative extensions.																	0162-8828					FEB	2009	31	2					260	274		10.1109/TPAMI.2008.70						WOS:000261846800005	19110492	J	Marmion, M; Parviainen, M; Luoto, M; Heikkinen, RK; Thuiller, W				Marmion, Mathieu; Parviainen, Miia; Luoto, Miska; Heikkinen, Risto K.; Thuiller, Wilfried			Evaluation of consensus methods in predictive species distribution modelling	DIVERSITY AND DISTRIBUTIONS												Spatial modelling techniques are increasingly used in species distribution modelling. However, the implemented techniques differ in their modelling performance, and some consensus methods are needed to reduce the uncertainty of predictions. In this study, we tested the predictive accuracies of five consensus methods, namely Weighted Average (WA), Mean(All), Median(All), Median(PCA), and Best, for 28 threatened plant species. North-eastern Finland, Europe. The spatial distributions of the plant species were forecasted using eight state-of-the-art single-modelling techniques providing an ensemble of predictions. The probability values of occurrence were then combined using five consensus algorithms. The predictive accuracies of the single-model and consensus methods were assessed by computing the area under the curve (AUC) of the receiver-operating characteristic plot. The mean AUC values varied between 0.697 (classification tree analysis) and 0.813 (random forest) for the single-models, and from 0.757 to 0.850 for the consensus methods. WA and Mean(All) consensus methods provided significantly more robust predictions than all the single-models and the other consensus methods. Consensus methods based on average function algorithms may increase significantly the accuracy of species distribution forecasts, and thus they show considerable promise for different conservation biological and biogeographical applications.				THUILLER, Wilfried/G-3283-2010; Luoto, Miska/E-6693-2014; Parviainen, Miia/E-1469-2017	THUILLER, Wilfried/0000-0002-5388-5274; Parviainen, Miia/0000-0002-0610-9264												1366-9516					JAN	2009	15	1					59	69		10.1111/j.1472-4642.2008.00491.x						WOS:000261521400006		J	Burbidge, R; Trotter, M; Buxton, B; Holden, S				Burbidge, R; Trotter, M; Buxton, B; Holden, S			Drug design by machine learning: support vector machines for pharmaceutical data analysis	COMPUTERS & CHEMISTRY					Symposium on Artificial Intelligence in Bioinformatic	APR, 2000	BIRMINGHAM, ENGLAND					We show that the support vector machine (SVM) classification algorithm, a recent development from the machine learning community, proves its potential for structure-activity relationship analysis. In a benchmark test, the SVM is compared to several machine learning techniques currently used in the field. The classification task involves predicting the inhibition of dihydrofolate reductase by pyrimidines, using data obtained from the UCI machine learning repository. Three artificial neural networks, a radial basis function network, and a C5.0 decision tree are all outperformed by the SVM. The SVM is significantly better than all of these, bar a manually capacity-controlled neural network, which takes considerably longer to train. (C) 2001 Elsevier Science Ltd. All rights reserved.																	0097-8485					DEC	2001	26	1			SI		5	14		10.1016/S0097-8485(01)00094-8						WOS:000172869700002	11765851	J	Biskup, D				Biskup, Dirk			A state-of-the-art review on scheduling with learning effects	EUROPEAN JOURNAL OF OPERATIONAL RESEARCH												Recently learning effects in scheduling have received considerable attention in the literature. All but one paper are based on the learning-by-doing (or autonomous learning) assumption, even though proactive investments in know how (induced learning) are very important from a practical point of view. In this review we first discuss the questions why and when learning effects in scheduling environments might occur and should be regarded from a planning perspective. Afterwards we give a concise overview on the literature on scheduling with learning effects. (c) 2007 Elsevier B.V. All rights reserved.																	0377-2217					JUL 16	2008	188	2					315	329		10.1016/j.ejor.2007.05.040						WOS:000253183500001		J	Moore, JH; Gilbert, JC; Tsai, CT; Chiang, FT; Holden, T; Barney, N; White, BC				Moore, Jason H.; Gilbert, Joshua C.; Tsai, Chia-Ti; Chiang, Fu-Tien; Holden, Todd; Barney, Nate; White, Bill C.			A flexible computational framework for detecting, characterizing, and interpreting statistical patterns of epistasis in genetic studies of human disease susceptibility	JOURNAL OF THEORETICAL BIOLOGY												Detecting, characterizing, and interpreting gene-gene interactions or epistasis in studies of human disease susceptibility is both a mathematical and a computational challenge. To address this problem, we have previously developed a multifactor dimensionality reduction (MDR) method for collapsing high-dimensional genetic data into a single dimension (i.e. constructive induction) thus permitting interactions to be detected in relatively small sample sizes. In this paper, we describe a comprehensive and flexible framework for detecting and interpreting gene-gene interactions that utilizes advances in information theory for selecting interesting single-nucleotide polymorphisms (SNPs), MDR for constructive induction, machine learning methods for classification, and finally graphical models for interpretation. We illustrate the usefulness of this strategy using artificial datasets simulated from several different two-locus and three-locus epistasis models. We show that the accuracy, sensitivity, specificity, and precision of a naive Bayes classifier are significantly improved when SNPs are selected based on their information gain (i.e. class entropy removed) and reduced to a single attribute using MDR. We then apply this strategy to detecting, characterizing, and interpreting epistatic models in a genetic study (n = 500) of atrial fibrillation and show that both classification and model interpretation are significantly improved. (c) 2005 Elsevier Ltd. All rights reserved.					Tsai, Chia-Ti/0000-0002-4853-8665; CHIANG, FU-TIEN/0000-0003-4936-8968												0022-5193					JUL 21	2006	241	2					252	261		10.1016/j.jtbi.2005.11.036						WOS:000239691300009	16457852	J	Broadhurst, DI; Kell, DB				Broadhurst, David I.; Kell, Douglas B.			Statistical strategies for avoiding false discoveries in metabolomics and related experiments	METABOLOMICS												Many metabolomics, and other high-content or high-throughput, experiments are set up such that the primary aim is the discovery of biomarker metabolites that can discriminate, with a certain level of certainty, between nominally matched 'case' and 'control' samples. However, it is unfortunately very easy to find markers that are apparently persuasive but that are in fact entirely spurious, and there are well-known examples in the proteomics literature. The main types of danger are not entirely independent of each other, but include bias, inadequate sample size (especially relative to the number of metabolite variables and to the required statistical power to prove that a biomarker is discriminant), excessive false discovery rate due to multiple hypothesis testing, inappropriate choice of particular numerical methods, and overfitting (generally caused by the failure to perform adequate validation and cross-validation). Many studies fail to take these into account, and thereby fail to discover anything of true significance (despite their claims). We summarise these problems, and provide pointers to a substantial existing literature that should assist in the improved design and evaluation of metabolomics experiments, thereby allowing robust scientific conclusions to be drawn from the available data. We provide a list of some of the simpler checks that might improve one's confidence that a candidate biomarker is not simply a statistical artefact, and suggest a series of preferred tests and visualisation tools that can assist readers and authors in assessing papers. These tools can be applied to individual metabolites by using multiple univariate tests performed in parallel across all metabolite peaks. They may also be applied to the validation of multivariate models. We stress in particular that classical p-values such as "p < 0.05", that are often used in biomedicine, are far too optimistic when multiple tests are done simultaneously (as in metabolomics). Ultimately it is desirable that all data and metadata are available electronically, as this allows the entire community to assess conclusions drawn from them. These analyses apply to all high-dimensional 'omics' datasets.				Kell, Douglas/E-8318-2011	Kell, Douglas/0000-0001-5838-7963												1573-3882					DEC	2006	2	4					171	196		10.1007/s11306-006-0037-z						WOS:000245261600001		J	Wipf, DP; Rao, BD				Wipf, DP; Rao, BD			Sparse Bayesian learning for basis selection	IEEE TRANSACTIONS ON SIGNAL PROCESSING												Sparse Bayesian learning (SBL) and specifically relevance vector machines have received much attention in the machine learning literature as a means of achieving parsimonious representations in the context of regression and classification. The methodology relies on a parameterized prior that encourages models with few nonzero weights. In this paper, we adapt SBL to the signal processing problem of basis selection from overcomplete dictionaries, proving, several results About the SBL cost function that elucidate its general behavior and provide solid theoretical justification for this application. Specifically, we have shown that SBL retains a desirable property of the l(0)-norm diversity measure (i.e., the global minimum is achieved at the maximally sparse solution) while often possessing a more limited constellation of local minima. We have also demonstrated that the local minima that do exist are achieved at sparse solutions. Later, we provide a novel interpretation of SBL that gives us valuable insight into why it is successful in producing sparse representations. Finally, we include simulation studies comparing sparse Bayesian learning with Basis Pursuit and the more recent FOCal Underdetermined System Solver (FOCUSS) class of basis selection algorithms. These results indicate that our theoretical insights translate directly into improved performance.																	1053-587X					AUG	2004	52	8					2153	2164		10.1109/TSP.2004.831016						WOS:000222760500002		J	Liu, H; Hussain, F; Tan, CL; Dash, M				Liu, H; Hussain, F; Tan, CL; Dash, M			Discretization: An enabling technique	DATA MINING AND KNOWLEDGE DISCOVERY												Discrete values have important roles in data mining and knowledge discovery. They are about intervals of numbers which are more concise to represent and specify, easier to use and comprehend as they are closer to a knowledge-level representation than continuous values. Many studies show induction tasks can benefit from discretization: rules with discrete values are normally shorter and more understandable and discretization can lead to improved predictive accuracy. Furthermore, many induction algorithms found in the literature require discrete features. All these prompt researchers and practitioners to discretize continuous features before or during a machine learning or data mining task. There are numerous discretization methods available in the literature. It is time for us to examine these seemingly different methods for discretization and find out how different they really are, what are the key components of a discretization process, how we can improve the current level of research for new development as well as the use of existing methods. This paper aims at a systematic study of discretization methods with their history of development, effect on classification, and trade-off between speed and accuracy. Contributions of this paper are an abstract description summarizing existing discretization methods, a hierarchical framework to categorize the existing methods and pave the way for further development, concise discussions of representative discretization methods, extensive experiments and their analysis, and some guidelines as to how to choose a discretization method under various circumstances. We also identify some issues yet to solve and future research for discretization.																	1384-5810					OCT	2002	6	4					393	423		10.1023/A:1016304305535						WOS:000176865200003		J	Huang, J; Ling, CX				Huang, J; Ling, CX			Using AUC and accuracy in evaluating learning algorithms	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												The area under the ROC ( Receiver Operating Characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. In this paper, we establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure ( defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications.																	1041-4347					MAR	2005	17	3					299	310		10.1109/TKDE.2005.50						WOS:000226358200001		J	Huang, GB; Chen, L				Huang, Guang-Bin; Chen, Lei			Enhanced random search based incremental extreme learning machine	NEUROCOMPUTING												Recently an incremental algorithm referred to as incremental extreme learning machine (I-ELM) was proposed by Huang et at. [G.-B. Huang, L. Chen, C.-K. Siew, Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Trans. Neural Networks 17(4) (2006) 879-892], which randomly generates hidden nodes and then analytically determines the output weights. Huang et al. [G.-B. Huang, L. Chen, C.-K. Siew, Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Trans. Neural Networks 17(4) (2006) 879-892] have proved in theory that although additive or RBF hidden nodes are generated randomly the network constructed by I-ELM can work as a universal approximator. During our recent study, it is found that some of the hidden nodes in such networks may play a very minor role in the network output and thus may eventually increase the network complexity. In order to avoid this issue and to obtain a more compact network architecture, this paper proposes an enhanced method for I-ELM (referred to as EI-ELM). At each learning step, several hidden nodes are randomly generated and among them the hidden node leading to the largest residual error decreasing will be added to the existing network and the output weight of the network will be calculated in a same simple way as in the original I-ELM. Generally speaking, the proposed enhanced I-ELM works for the widespread type of piecewise continuous hidden nodes. (C) 2007 Elsevier B.V. All rights reserved.				Huang, Guang-Bin/A-5035-2011	Huang, Guang-Bin/0000-0002-2480-4965												0925-2312					OCT	2008	71	16-18			SI		3460	3468		10.1016/j.neucom.2007.10.008						WOS:000260066100047		J	Huang, Z; Chen, HC; Hsu, CJ; Chen, WH; Wu, SS				Huang, Z; Chen, HC; Hsu, CJ; Chen, WH; Wu, SS			Credit rating analysis with support vector machines and neural networks: a market comparative study	DECISION SUPPORT SYSTEMS												Corporate credit rating analysis has attracted lots of research interests in the literature. Recent studies have shown that Artificial Intelligence (AI) methods achieved better performance than, traditional statistical methods. This article introduces a relatively new machine learning technique, support vector machines (SVM), to the problem in attempt to provide a model with better explanatory power. We used backpropagation neural network (BNN) as a benchmark and obtained prediction accuracy around 80% for both BNN and SVM methods for the United States and Taiwan markets. However, only slight improvement of SVM was observed. Another direction of the research is to improve the interpretability of the AI-based models. We applied recent research results in neural network model interpretation and obtained relative importance of the input fmancial variables from the neural network models. Based on these results, we conducted a market comparative analysis on the differences of determining factors in the United States and Taiwan markets. (C) 2003 Elsevier B.V All rights reserved.																	0167-9236					SEP	2004	37	4					543	558		10.1016/S0167-9236(03)00086-1						WOS:000221963500008		S	Ciresan, D; Meier, U; Schmidhuber, J			IEEE	Ciresan, Dan; Meier, Ueli; Schmidhuber, Juergen			Multi-column Deep Neural Networks for Image Classification	2012 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)	IEEE Conference on Computer Vision and Pattern Recognition				IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	JUN 16-21, 2012	Providence, RI	IEEE				Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.																	1063-6919		978-1-4673-1228-8				2012							3642	3649								WOS:000309166203102		J	Kislinger, T; Cox, B; Kannan, A; Chung, C; Hu, PZ; Ignatchenko, A; Scott, MS; Gramolini, AO; Morris, Q; Hallett, MT; Rossant, J; Hughes, TR; Frey, B; Emili, A				Kislinger, T; Cox, B; Kannan, A; Chung, C; Hu, PZ; Ignatchenko, A; Scott, MS; Gramolini, AO; Morris, Q; Hallett, MT; Rossant, J; Hughes, TR; Frey, B; Emili, A			Global survey of organ and organelle protein expression in mouse: Combined proteomic and transcriptomic profiling	CELL												Organs and organelles represent core biological systems in mammals, but the diversity in protein composition remains unclear. Here, we combine subcellular fractionation with exhaustive tandem mass spectrometry-based shotgun sequencing to examine the protein content of four major organellar compartments (cytosol, membranes [microsomes], mitochondria, and nuclei) in six organs (brain, heart, kidney, liver, lung, and placenta) of the laboratory mouse, Mus musculus. Using rigorous statistical filtering and machine-learning methods, the subcellular localization of 3274 of the 4768 proteins identified was determined with high confidence, including 1503 previously uncharacterized factors, while tissue selectivity was evaluated by comparison to previously reported mRNA expression patterns. This molecular compendium, fully accessible via a searchable web-browser interface, serves as a reliable reference of the expressed tissue and organelle proteomes of a leading model mammal.				Kislinger, Thomas/A-5934-2008; Scott, Michelle/C-7445-2013	Scott, Michelle/0000-0001-6231-7714; Hu, Pingzhao/0000-0002-9546-2245; Ignatchenko, Alexandr/0000-0002-6083-941X												0092-8674					APR 7	2006	125	1					173	186		10.1016/j.cell.2006.01.044						WOS:000237314200022	16615898	J	Tuzel, O; Porikli, F; Meer, P				Tuzel, Oncel; Porikli, Fatih; Meer, Peter			Pedestrian detection via classification on Riemannian manifolds	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE					IEEE Conference on Computer Vision and Pattern Recognition	JUN 17-22, 2007	Minneapolis, MN	IEEE, hp invent, INI-GraphicsNet, VIOSO				We present a new algorithm to detect pedestrians in still images utilizing covariance matrices as object descriptors. Since the descriptors do not form a vector space, well-known machine learning techniques are not well suited to learn the classifiers. The space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. The main contribution of the paper is a novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the space. The algorithm is tested on the INRIA and DaimlerChrysler pedestrian data sets where superior detection rates are observed over the previous approaches.																	0162-8828	1939-3539				OCT	2008	30	10					1713	1727		10.1109/TPAMI.2008.75						WOS:000258344900004	18703826	J	Mitchell, TM; Hutchinson, R; Niculescu, RS; Pereira, F; Wang, XR; Just, M; Newman, S				Mitchell, TM; Hutchinson, R; Niculescu, RS; Pereira, F; Wang, XR; Just, M; Newman, S			Learning to decode cognitive states from brain images	MACHINE LEARNING												Over the past decade, functional Magnetic Resonance Imaging ( fMRI) has emerged as a powerful new instrument to collect vast quantities of data about activity in the human brain. A typical fMRI experiment can produce a three-dimensional image related to the human subject's brain activity every half second, at a spatial resolution of a few millimeters. As in other modern empirical sciences, this new instrumentation has led to a flood of new data, and a corresponding need for new data analysis methods. We describe recent research applying machine learning methods to the problem of classifying the cognitive state of a human subject based on fRMI data observed over a single time interval. In particular, we present case studies in which we have successfully trained classifiers to distinguish cognitive states such as ( 1) whether the human subject is looking at a picture or a sentence, ( 2) whether the subject is reading an ambiguous or non-ambiguous sentence, and ( 3) whether the word the subject is viewing is a word describing food, people, buildings, etc. This learning problem provides an interesting case study of classifier learning from extremely high dimensional (10(5) features), extremely sparse ( tens of training examples), noisy data. This paper summarizes the results obtained in these three case studies, as well as lessons learned about how to successfully apply machine learning methods to train classifiers in such settings.					Just, Marcel/0000-0003-1245-3050												0885-6125					OCT-NOV	2004	57	1-2					145	175		10.1023/B:MACH.0000035475.85309.1b						WOS:000222800200007		J	Mourao-Miranda, J; Bokde, ALW; Born, C; Hampel, H; Stetter, M				Mourao-Miranda, J; Bokde, ALW; Born, C; Hampel, H; Stetter, M			Classifying brain states and determining the discriminating activation patterns: Support Vector Machine on functional MRI data	NEUROIMAGE												In the present study, we applied the Support Vector Machine (SVM) algorithm to perform multivariate classification of brain states from whole functional magnetic resonance imaging (fMRI) volumes without prior selection of spatial features. In addition, we did a comparative analysis between the SVM and the Fisher Linear Discriminant (FLD) classifier. We applied the methods to two multisubject attention experiments: a face matching and a location matching task. We demonstrate that SVM outperforms FLD in classification performance as well as in robustness of the spatial maps obtained (i.e. discriminating volumes). In addition, the SVM discrimination maps had greater overlap with the general linear model (GLM) analysis compared to the FLD. The analysis presents two phases: during the training, the classifier algorithm finds the set of regions by which the two brain states can be best distinguished from each other. In the next phase, the test phase, given an fMRI volume from a new subject, the classifier predicts the subject's instantaneous brain state. (c) 2005 Elsevier Inc. All rights reserved.																	1053-8119	1095-9572				DEC	2005	28	4					980	995		10.1016/j.neuroimage.2005.06.070						WOS:000234015300023	16275139	J	Candes, EJ; Plan, Y				Candes, Emmanuel J.; Plan, Yaniv			Matrix Completion With Noise	PROCEEDINGS OF THE IEEE												On the heels of compressed sensing, a new field has very recently emerged. This field addresses a broad range of problems of significant practical interest, namely, the recovery of a data matrix from what appears to be incomplete, and perhaps even corrupted, information. In its simplest form, the problem is to recover a matrix from a small sample of its entries. It comes up in many areas of science and engineering, including collaborative filtering, machine learning, control, remote sensing, and computer vision, to name a few. This paper surveys the novel literature on matrix completion, which shows that under some suitable conditions, one can recover an unknown low-rank matrix from a nearly minimal set of entries by solving a simple convex optimization problem, namely, nuclear-norm minimization subject to data constraints. Further, this paper introduces novel results showing that matrix completion is provably accurate even when the few observed entries are corrupted with a small amount of noise. A typical result is that one can recover an unknown n x n matrix of low rank r from just about nr log(2)n noisy samples with an error that is proportional to the noise level. We present numerical results that complement our quantitative analysis and show that, in practice, nuclear-norm minimization accurately fills in the many missing entries of large low-rank matrices from just a few noisy samples. Some analogies between matrix completion and compressed sensing are discussed throughout.																	0018-9219					JUN	2010	98	6					925	936		10.1109/JPROC.2009.2035722						WOS:000277884900005		J	Bock, JR; Gough, DA				Bock, JR; Gough, DA			Predicting protein-protein interactions from primary structure	BIOINFORMATICS												Motivation: An ambitious goal of proteomics is to elucidate the structure, interactions and functions of ail proteins within cells and organisms. The expectation is that this will provide a fuller appreciation of cellular processes and networks at the protein level, ultimately leading to a better understanding of disease mechanisms and suggesting new means for intervention. This paper addresses the question: can protein-protein interactions be predicted directly from primary structure and associated data? Using a diverse database of known protein interactions, a Support Vector Machine (SVM) learning system was trained to recognize and predict interactions based solely on primary structure and associated physicochemical properties. Results: Inductive accuracy of the trained system, defined here as the percentage of correct protein interaction predictions for previously unseen test sets, averaged 80% for the ensemble of statistical experiments. Future proteomics studies may benefit from this research by proceeding directly from the automated identification of a cell's gene products to prediction of protein interaction pairs.																	1367-4803					MAY	2001	17	5					455	460		10.1093/bioinformatics/17.5.455						WOS:000168750700010	11331240	J	Wang, XW; El Naqa, IM				Wang, Xiaowei; El Naqa, Issam M.			Prediction of both conserved and nonconserved microRNA targets in animals	BIOINFORMATICS												Motivation: MicroRNAs (miRNAs) are involved in many diverse biological processes and they may potentially regulate the functions of thousands of genes. However, one major issue in miRNA studies is the lack of bioinformatics programs to accurately predict miRNA targets. Animal miRNAs have limited sequence complementarity to their gene targets, which makes it challenging to build target prediction models with high specificity. Results: Here we present a new miRNA target prediction program based on support vector machines (SVMs) and a large microarray training dataset. By systematically analyzing public microarray data, we have identified statistically significant features that are important to target downregulation. Heterogeneous prediction features have been non-linearly integrated in an SVM machine learning framework for the training of our target prediction model, MirTarget2. About half of the predicted miRNA target sites in human are not conserved in other organisms. Our prediction algorithm has been validated with independent experimental data for its improved performance on predicting a large number of miRNA down-regulated gene targets.																	1367-4803					FEB 1	2008	24	3					325	332		10.1093/bioinformatics/btm595						WOS:000252903700004	18048393	J	Ito, M				Ito, Masao			Cerebellar circuitry as a neuronal machine	PROGRESS IN NEUROBIOLOGY												Shortly after John Eccles completed his studies of synaptic inhibition in the spinal cord, for which he was awarded the 1963 Nobel Prize in physiology/medicine, he opened another chapter of neuroscience with his work on the cerebellum. From 1963 to 1967, Eccles and his colleagues in Canberra successfully dissected the complex neuronal circuitry in the cerebellar cortex. In the 1967 monograph, "The Cerebellum as a Neuronal Machine", he, in collaboration with Masao Ito and Janos Szentdgothai, presented blue-print-like wiring diagrams of the cerebellar neuronal circuitry. These stimulated worldwide discussions and experimentation on the potential operational mechanisms of the circuitry and spurred theoreticians to develop relevant network models of the machinelike function of the cerebellum. In following decades, the neuronal machine concept of the cerebellum was strengthened by additional knowledge of the modular organization of its structure and memory mechanism, the latter in the form of synaptic plasticity, in particular, long-term depression. Moreover, several types of motor control were established as model systems representing learning mechanisms of the cerebellum. More recently, both the quantitative preciseness of cerebellar analyses and overall knowledge about the cerebellum have advanced considerably at the cellular and molecular levels of analysis. Cerebellar circuitry now includes Lugaro cells and unipolar brush cells as additional unique elements. Other new revelations include the operation of the complex glomerulus structure, intricate signal transduction for synaptic plasticity, silent synapses, irregularity of spike discharges, temporal fidelity of synaptic activation, rhythm generators, a Golgi cell clock circuit, and sensory or motor representation by mossy fibers and climbing fibers. Furthermore, it has become evident that the cerebellum has cognitive functions, and probably also emotion, as well as better-known motor and autonomic functions. Further cerebellar research is required for full understanding of the cerebellum as a broad learning machine for neural control of these functions. (c) 2006 Elsevier Ltd. All rights reserved.																	0301-0082					FEB-APR	2006	78	3-5			SI		272	303		10.1016/j.pneurobio.2006.02.006						WOS:000238916700009	16759785	J	RIPLEY, BD				RIPLEY, BD			NEURAL NETWORKS AND RELATED METHODS FOR CLASSIFICATION	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-METHODOLOGICAL												Feed-forward neural networks are now widely used in classification problems, whereas non-linear methods of discrimination developed in the statistical field are much less widely known. A general framework for classification is set up within which methods from statistics, neural networks, pattern recognition and machine learning can be compared. Neural networks emerge as one of a class of flexible non-linear regression methods which can be used to classify via regression. Many interesting issues remain, including parameter estimation, the assessment of the classifiers and in algorithm development.																	0035-9246						1994	56	3					409	437								WOS:A1994NK40900001		J	Krishnapuram, B; Carin, L; Figueiredo, MAT; Hartemink, AJ				Krishnapuram, B; Carin, L; Figueiredo, MAT; Hartemink, AJ			Sparse multinomial logistic regression: Fast algorithms and generalization bounds	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Recently developed methods for learning sparse classifiers are among the state-of-the-art in supervised learning. These methods learn classifiers that incorporate weighted sums of basis functions with sparsity-promoting priors encouraging the weight estimates to be either significantly large or exactly zero. From a learning-theoretic perspective, these methods control the capacity of the learned classifier by minimizing the number of basis functions used, resulting in better generalization. This paper presents three contributions related to learning sparse classifiers. First, we introduce a true multiclass formulation based on multinomial logistic regression. Second, by combining a bound optimization approach with a component-wise update procedure, we derive fast exact algorithms for learning sparse multiclass classifiers that scale favorably in both the number of training samples and the feature dimensionality, making them applicable even to large data sets in high-dimensional feature spaces. To the best of our knowledge, these are the first algorithms to perform exact multinomial logistic regression with a sparsity-promoting prior. Third, we show how nontrivial generalization bounds can be derived for our classifier in the binary case. Experimental results on standard benchmark data sets attest to the accuracy, sparsity, and efficiency of the proposed methods.				Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745												0162-8828					JUN	2005	27	6					957	968		10.1109/TPAMI.2005.127						WOS:000228334700010	15943426	J	Karaboga, D; Ozturk, C				Karaboga, Dervis; Ozturk, Celal			A novel clustering approach: Artificial Bee Colony (ABC) algorithm	APPLIED SOFT COMPUTING												Artificial Bee Colony (ABC) algorithm which is one of the most recently introduced optimization algorithms, simulates the intelligent foraging behavior of a honey bee swarm. Clustering analysis, used in many disciplines and applications, is an important tool and a descriptive task seeking to identify homogeneous groups of objects based on the values of their attributes. In this work, ABC is used for data clustering on benchmark problems and the performance of ABC algorithm is compared with Particle Swarm Optimization (PSO) algorithm and other nine classification techniques from the literature. Thirteen of typical test data sets from the UCI Machine Learning Repository are used to demonstrate the results of the techniques. The simulation results indicate that ABC algorithm can efficiently be used for multivariate data clustering. (C) 2009 Elsevier B.V. All rights reserved.				Ozturk, Celal/A-8961-2012													1568-4946					JAN	2011	11	1					652	657		10.1016/j.asoc.2009.12.025						WOS:000281591300065		J	Allen, J; Davey, HM; Broadhurst, D; Heald, JK; Rowland, JJ; Oliver, SG; Kell, DB				Allen, J; Davey, HM; Broadhurst, D; Heald, JK; Rowland, JJ; Oliver, SG; Kell, DB			High-throughput classification of yeast mutants for functional genomics using metabolic footprinting	NATURE BIOTECHNOLOGY												Many technologies have been developed to help explain the function of genes discovered by systematic genome sequencing. At present, transcriptome and proteome studies dominate large-scale functional analysis strategies. Yet the metabolome, because it is 'downstream', should show greater effects of genetic or physiological changes and thus should be much closer to the phenotype of the organism. We earlier presented a functional analysis strategy that used metabolic fingerprinting to reveal the phenotype of silent mutations of yeast genes(1). However, this is difficult to scale up for high-throughput screening. Here we present an alternative that has the required throughput (2 min per sample). This 'metabolic footprinting' approach recognizes the significance of 'overflow metabolism' in appropriate media. Measuring intracellular metabolites is time-consuming and subject to technical difficulties caused by the rapid turnover of intracellular metabolites and the need to quench metabolism and separate metabolites from the extracellular space. We therefore focused instead on direct, noninvasive, mass spectrometric monitoring of extracellular metabolites in spent culture medium. Metabolic footprinting can distinguish between different physiological states of wildtype yeast and between yeast single-gene deletion mutants even from related areas of metabolism. By using appropriate clustering and machine learning techniques, the latter based on genetic programming(2-8), we show that metabolic footprinting is an effective method to classify 'unknown' mutants by genetic defect.				Davey, Hazel/C-9055-2009; Kell, Douglas/E-8318-2011	Kell, Douglas/0000-0001-5838-7963												1087-0156					JUN	2003	21	6					692	696		10.1038/nbt823						WOS:000183220800029	12740584	J	Iizuka, N; Oka, M; Yamada-Okabe, H; Nishida, M; Maeda, Y; Mori, N; Takao, T; Tamesa, T; Tangoku, A; Tabuchi, H; Hamada, K; Nakayama, H; Ishitsuka, H; Miyamoto, T; Hirabayashi, A; Uchimura, S; Hamamoto, Y				Iizuka, N; Oka, M; Yamada-Okabe, H; Nishida, M; Maeda, Y; Mori, N; Takao, T; Tamesa, T; Tangoku, A; Tabuchi, H; Hamada, K; Nakayama, H; Ishitsuka, H; Miyamoto, T; Hirabayashi, A; Uchimura, S; Hamamoto, Y			Oligonucleotide microarray for prediction of early intrahepatic recurrence of hepatocellular carcinoma after curative resection	LANCET												Background Hepatocellular carcinoma has a poor prognosis because of the high intrahepatic recurrence rate. There are technological limitations to traditional methods such as TNM staging for accurate prediction of recurrence, suggesting that new techniques are needed. Methods We investigated mRNA expression profiles in tissue specimens from a training set, comprising 33 patients with hepatocellular carcinoma, with high-density oligonucleotide microarrays representing about 6000 genes. We used this training set in a supervised learning manner to construct a predictive system, consisting of 12 genes, with the Fisher linear classifier. We then compared the predictive performance of our system with that of a predictive system with a support vector machine (SVM-based system) on a blinded set of samples from 27 newly enrolled patients. Findings Early intrahepatic recurrence within 1 year after curative surgery occurred in 12 (36%) and eight (30%) patients in the training and blinded sets, respectively. Our system correctly predicted early intrahepatic recurrence or non-recurrence in 25 (93%) of 27 samples in the blinded set and had a positive predictive value of 88% and a negative predictive value of 95%. By contrast, the SVM-based system predicted early intrahepatic recurrence or non-recurrence correctly in only 16 (60%) individuals in the blinded set, and the result yielded a positive predictive value of only 38% and a negative predictive value of 79%. Interpretation Our system predicted early intrahepatic recurrence or non-recurrence for patients with hepatocellular carcinoma much more accurately than the SVM-based system, suggesting that our system could serve as a new method for characterising the metastatic potential of hepatocellular carcinoma.																	0140-6736					MAR 15	2003	361	9361					923	929		10.1016/S0140-6736(03)12775-4						WOS:000181741600011	12648972	J	Fouss, F; Pirotte, A; Renders, JM; Saerens, M				Fouss, Francois; Pirotte, Alain; Renders, Jean-Michel; Saerens, Marco			Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												This work presents a new perspective on characterizing the similarity between elements of a database or, more generally, nodes of a weighted and undirected graph. It is based on a Markov-chain model of random walk through the database. More precisely, we compute quantities (the average commute time, the pseudoinverse of the Laplacian matrix of the graph, etc.) that provide similarities between any pair of nodes, having the nice property of increasing when the number of paths connecting those elements increases and when the "length" of paths decreases. It turns out that the square root of the average commute time is a Euclidean distance and that the pseudoinverse of the Laplacian matrix is a kernel matrix (its elements are inner products closely related to commute times). A principal component analysis (PCA) of the graph is introduced for computing the subspace projection of the node vectors in a manner that preserves as much variance as possible in terms of the Euclidean commute-time distance. This graph PCA provides a nice interpretation to the "Fiedler vector," widely used for graph partitioning. The model is evaluated on a collaborative-recommendation task where suggestions are made about which movies people should watch based upon what they watched in the past. Experimental results on the MovieLens database show that the Laplacian-based similarities perform well in comparison with other methods. The model, which nicely fits into the so-called " statistical relational learning" framework, could also be used to compute document or word similarities, and, more generally, it could be applied to machine-learning and pattern-recognition tasks involving a relational database.																	1041-4347	1558-2191				MAR	2007	19	3					355	369		10.1109/TKDE.2007.46						WOS:000243504100002		J	Tong, S; Koller, D				Tong, S; Koller, D			Support vector machine active learning with applications to text classification	JOURNAL OF MACHINE LEARNING RESEARCH												Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.																	1532-4435					WIN	2002	2	1					45	66								WOS:000173838200003		J	DAYAN, P; HINTON, GE; NEAL, RM; ZEMEL, RS				DAYAN, P; HINTON, GE; NEAL, RM; ZEMEL, RS			THE HELMHOLTZ MACHINE	NEURAL COMPUTATION												Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.																	0899-7667					SEP	1995	7	5					889	904		10.1162/neco.1995.7.5.889						WOS:A1995RT60600002	7584891	J	Jannink, JL; Lorenz, AJ; Iwata, H				Jannink, Jean-Luc; Lorenz, Aaron J.; Iwata, Hiroyoshi			Genomic selection in plant breeding: from theory to practice	BRIEFINGS IN FUNCTIONAL GENOMICS												We intuitively believe that the dramatic drop in the cost of DNA marker information we have experienced should have immediate benefits in accelerating the delivery of crop varieties with improved yield, quality and biotic and abiotic stress tolerance. But these traits are complex and affected by many genes, each with small effect. Traditional marker-assisted selection has been ineffective for such traits. The introduction of genomic selection (GS), however, has shifted that paradigm. Rather than seeking to identify individual loci significantly associated with a trait, GS uses all marker data as predictors of performance and consequently delivers more accurate predictions. Selection can be based on GS predictions, potentially leading to more rapid and lower cost gains from breeding. The objectives of this article are to review essential aspects of GS and summarize the important take-home messages from recent theoretical, simulation and empirical studies. We then look forward and consider research needs surrounding methodological questions and the implications of GS for long-term selection.																	2041-2649					MAR	2010	9	2					166	177		10.1093/bfgp/elq001						WOS:000276191200010	20156985	J	Jain, E; Bairoch, A; Duvaud, S; Phan, I; Redaschi, N; Suzek, BE; Martin, MJ; McGarvey, P; Gasteiger, E				Jain, Eric; Bairoch, Amos; Duvaud, Severine; Phan, Isabelle; Redaschi, Nicole; Suzek, Baris E.; Martin, Maria J.; McGarvey, Peter; Gasteiger, Elisabeth			Infrastructure for the life sciences: design and implementation of the UniProt website	BMC BIOINFORMATICS												Background: The UniProt consortium was formed in 2002 by groups from the Swiss Institute of Bioinformatics ( SIB), the European Bioinformatics Institute (EBI) and the Protein Information Resource (PIR) at Georgetown University, and soon afterwards the website http://www.uniprot.org was set up as a central entry point to UniProt resources. Requests to this address were redirected to one of the three organisations' websites. While these sites shared a set of static pages with general information about UniProt, their pages for searching and viewing data were different. To provide users with a consistent view and to cut the cost of maintaining three separate sites, the consortium decided to develop a common website for UniProt. Following several years of intense development and a year of public beta testing, the http://www.uniprot.org domain was switched to the newly developed site described in this paper in July 2008. Description: The UniProt consortium is the main provider of protein sequence and annotation data for much of the life sciences community. The http://www.uniprot.org website is the primary access point to this data and to documentation and basic tools for the data. These tools include full text and field-based text search, similarity search, multiple sequence alignment, batch retrieval and database identifier mapping. This paper discusses the design and implementation of the new website, which was released in July 2008, and shows how it improves data access for users with different levels of experience, as well as to machines for programmatic access. http://www.uniprot.org/ is open for both academic and commercial use. The site was built with open source tools and libraries. Feedback is very welcome and should be sent to help@uniprot.org. Conclusion: The new UniProt website makes accessing and understanding UniProt easier than ever. The two main lessons learned are that getting the basics right for such a data provider website has huge benefits, but is not trivial and easy to underestimate, and that there is no substitute for using empirical data throughout the development process to decide on what is and what is not working for your users.					Gasteiger, Elisabeth/0000-0003-1829-162X; Bairoch, Amos/0000-0003-2826-6444; Martin, Maria-Jesus/0000-0001-5454-2815; Redaschi, Nicole/0000-0001-8890-2268												1471-2105					MAY 8	2009	10								136	10.1186/1471-2105-10-136						WOS:000266606400002	19426475	J	Cao, LJ; Tay, FEH				Cao, LJ; Tay, FEH			Support vector machine with adaptive parameters in financial time series forecasting	IEEE TRANSACTIONS ON NEURAL NETWORKS												A novel type of learning machine called support vector machine (SVM) has been receiving increasing interest in areas ranging from its original application in pattern recognition to other applications such as regression estimation due to its remarkable generalization performance. This paper deals with the application of SVM in financial time series forecasting. The feasibility of applying SVM in financial forecasting is first examined by comparing it with the multilayer back-propagation (BP) neural network and the regularized radial basis function (RBF) neural network. The variability in performance of SVM with respect to the free parameters is investigated experimentally. Adaptive parameters are then proposed by incorporating the nonstationarity of financial time series into SVM. Five real futures contracts collated from the Chicago Mercantile Market are used as the data sets. The simulation shows that among the three methods, SVM outperforms the BP neural network in financial forecasting, and there are comparable generalization performance between SVM and the regularized RBF neural network. Furthermore, the free parameters of SVM have a great effect on the generalization performance. SVM with adaptive parameters can both achieve higher generalization performance and use fewer support vectors than the standard SVM in financial forecasting.																	1045-9227					NOV	2003	14	6					1506	1518		10.1109/TNN.2003.820556						WOS:000188260400007	18244595	J	Murthy, SK				Murthy, SK			Automatic construction of decision trees from data: A multi-disciplinary survey	DATA MINING AND KNOWLEDGE DISCOVERY												Decision trees have proved to be valuable tools for the description, classification and generalization of data. Work on constructing decision trees from data exists in multiple disciplines such as statistics, pattern recognition, decision theory, signal processing, machine learning and artificial neural networks. Researchers in these disciplines, sometimes working on quite different problems, identified similar issues and heuristics for decision tree construction. This paper surveys existing work on decision tree construction, attempting to identify the important issues involved, directions the work has taken and the current state of the art.																	1384-5810	1573-756X				DEC	1998	2	4					345	389		10.1023/A:1009744630224						WOS:000079520100003		J	Tenenbaum, JB; Kemp, C; Griffiths, TL; Goodman, ND				Tenenbaum, Joshua B.; Kemp, Charles; Griffiths, Thomas L.; Goodman, Noah D.			How to Grow a Mind: Statistics, Structure, and Abstraction	SCIENCE												In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?																	0036-8075					MAR 11	2011	331	6022					1279	1285		10.1126/science.1192788						WOS:000288215200035	21393536	J	Hua, SJ; Sun, ZR				Hua, SJ; Sun, ZR			A novel method of protein secondary structure prediction with high segment overlap measure: Support vector machine approach	JOURNAL OF MOLECULAR BIOLOGY												We have introduced a new method of protein secondary structure prediction which is based on the theory of support vector machine (SVM). SVM represents a new approach to supervised pattern classification which has been successfully applied to a wide range of pattern recognition problems, including object recognition, speaker identification, gene function prediction with microarray expression profile, etc. Ln these cases, the performance of SVM either matches or is significantly better than that of traditional machine learning approaches, including neural networks. The first use of the SVM approach to predict protein secondary structure is described here. Unlike the previous studies, we first constructed several binary classifiers, then assembled a tertiary classifier for three secondary structure states (helix, sheet and coil) based on these binary classifiers. The SVM method achieved a good performance of segment overlap accuracy SOV = 76.2 % through sevenfold cross validation on a database of 513 non-homologous protein chains with multiple sequence alignments, which out-performs existing methods. Meanwhile three-state overall per-residue accuracy Q(3) achieved 73.5%, which is at least comparable to existing single prediction methods. Furthermore a useful "reliability index" for the predictions was developed, hn addition, SVM has many attractive features, including effective avoidance of overfitting, the ability to handle large feature spaces, information condensing of the given data set, etc. The SVM method is conveniently applied to many ether pattern classification tasks in biology. (C) 2001 Academic Press.																	0022-2836					APR 27	2001	308	2					397	407		10.1006/jmbi.2001.4580						WOS:000168552100021	11327775	J	Stone, P; Veloso, M				Stone, P; Veloso, M			Multiagent systems: A survey from a machine learning perspective	AUTONOMOUS ROBOTS												Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goal; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue.																	0929-5593	1573-7527				JUN	2000	8	3					345	383		10.1023/A:1008942012299						WOS:000087607100008		J	Blankertz, B; Lemm, S; Treder, M; Haufe, S; Muller, KR				Blankertz, Benjamin; Lemm, Steven; Treder, Matthias; Haufe, Stefan; Mueller, Klaus-Robert			Single-trial analysis and classification of ERP components - A tutorial	NEUROIMAGE												Analyzing brain states that correspond to event related potentials (ERPs) on a single trial basis is a hard problem due to the high trial-to-trial variability and the unfavorable ratio between signal (ERP) and noise (artifacts and neural background activity). In this tutorial, we provide a comprehensive framework for decoding ERPs, elaborating on linear concepts, namely spatio-temporal patterns and filters as well as linear ERP classification. However, the bottleneck of these techniques is that they require an accurate covariance matrix estimation in high dimensional sensor spaces which is a highly intricate problem. As a remedy, we propose to use shrinkage estimators and show that appropriate regularization of linear discriminant analysis (LDA) by shrinkage yields excellent results for single-trial ERP classification that are far superior to classical LDA classification. Furthermore, we give practical hints on the interpretation of what classifiers learned from the data and demonstrate in particular that the trade-off between goodness-of-fit and model complexity in regularized LDA relates to a morphing between a difference pattern of ERPs and a spatial filter which cancels non task-related brain activity. (C) 2010 Elsevier Inc. All rights reserved.				Muller, Klaus/C-3196-2013													1053-8119					MAY 15	2011	56	2			SI		814	825		10.1016/j.neuroimage.2010.06.048						WOS:000290081900040	20600976	J	Luke, S; Cioffi-Revilla, C; Panait, L; Sullivan, K; Balan, G				Luke, S; Cioffi-Revilla, C; Panait, L; Sullivan, K; Balan, G			MASON: A multiagent simulation environment	SIMULATION-TRANSACTIONS OF THE SOCIETY FOR MODELING AND SIMULATION INTERNATIONAL												MASON is a fast, easily extensible, discrete-event multi-agent simulation toolkit in Java, designed to serve as the basis for a wide range of multi-agent simulation tasks ranging from swarm robotics to machine learning to social complexity environments. MASON carefully delineates between model and visualization, allowing models to be dynamically detached from or attached to visualizers, and to change platforms mid-run. This paper describes the MASON system, its motivation, and its basic architectural design. It then compares MASON to related multi-agent libraries in the public domain, and discusses six applications of the system built over the past year which suggest its breadth of utility.																	0037-5497					JUL	2005	81	7					517	527		10.1177/0037549705058073						WOS:000232138800005		J	Markou, M; Singh, S				Markou, M; Singh, S			Novelty detection: a review - part 2: neural network based approaches	SIGNAL PROCESSING												Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. In this paper we focus on neural network-based approaches for novelty detection. Statistical approaches are covered in Part 1 paper. (C) 2003 Elsevier B.V. All rights reserved.																	0165-1684					DEC	2003	83	12					2499	2521		10.1016/j.sigpro.2003.07.019						WOS:000186346000002		J	Yin, S; Li, XW; Gao, HJ; Kaynak, O				Yin, Shen; Li, Xianwei; Gao, Huijun; Kaynak, Okyay			Data-Based Techniques Focused on Modern Industry: An Overview	IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS												This paper provides an overview of the recent developments in data-based techniques focused on modern industrial applications. As one of the hottest research topics for complicated processes, the data-based techniques have been rapidly developed over the past two decades and widely used in numerous industrial sectors nowadays. The core of data-based techniques is to take full advantage of the huge amounts of available process data, aiming to acquire the useful information within. Compared with the well-developed model-based approaches, data-based techniques provide efficient alternative solutions for different industrial issues under various operating conditions. The main objective of this paper is to review and summarize the recent achievements in data-based techniques, especially for complicated industrial applications, thus providing a referee for further study on the related topics both from academic and practical points of view. This paper begins with a brief evolutionary overview of data-based techniques in the last two decades. Then, the methodologies only based on process measurements and the model-data integrated techniques will be further introduced. The recent developments for modern industrial applications are, respectively, presented mainly from perspectives of monitoring and control. The new trends of data-based technique as well as potential application fields are finally discussed.				Kaynak, Okyay/H-5942-2011; Yin, Shen/I-5855-2014	Kaynak, Okyay/0000-0002-4789-6700; Yin, Shen/0000-0002-3802-9269												0278-0046	1557-9948				JAN	2015	62	1					657	667		10.1109/TIE.2014.2308133						WOS:000346767400068		J	Patcha, A; Park, JM				Patcha, Animesh; Park, Jung-Min			An overview of anomaly detection techniques: Existing solutions and latest technological trends	COMPUTER NETWORKS												As advances in networking technology help to connect the distant corners of the globe and as the Internet continues to expand its influence as a medium for communications and commerce, the threat from spammers, attackers and criminal enterprises has also grown accordingly. It is the prevalence of such threats that has made intrusion detection systems-the cyberspace's equivalent to the burglar alarm join ranks with firewalls as one of the fundamental technologies for network security. However, today's commercially available intrusion detection systems are predominantly signature-based intrusion detection systems that are designed to detect known attacks by utilizing the signatures of those attacks. Such systems require frequent rule-base updates and signature updates, and are not capable of detecting unknown attacks. In contrast, anomaly detection systems, a subset of intrusion detection systems, model the normal system/network behavior which enables them to be extremely effective in finding and foiling both known as well as unknown or "zero day" attacks. While anomaly detection systems are attractive conceptually, a host of technological problems need to be overcome before they can be widely adopted. These problems include: high false alarm rate, failure to scale to gigabit speeds, etc. In this paper, we provide a comprehensive survey of anomaly detection systems and hybrid intrusion detection systems of the recent past and present. We also discuss recent technological trends in anomaly detection and identify open problems and challenges in this area. (c) 2007 Elsevier B.V. All rights reserved.				WANG, HUAN/A-1155-2009; Pagna Disso, Jules Ferdinand/A-6712-2009	Pagna Disso, Jules Ferdinand/0000-0001-8388-0418												1389-1286	1872-7069				AUG 22	2007	51	12					3448	3470		10.1016/j.comnet.2007.02.001						WOS:000248139800009		J	APTE, C; DAMERAU, F; WEISS, SM				APTE, C; DAMERAU, F; WEISS, SM			AUTOMATED LEARNING OF DECISION RULES FOR TEXT CATEGORIZATION	ACM TRANSACTIONS ON INFORMATION SYSTEMS												We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to ''read'' documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67% recall/precision breakeven point to 80.5%. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.																	1046-8188					JUL	1994	12	3					233	251		10.1145/183422.183423						WOS:A1994PH88700002		J	Huang, GB; Ding, XJ; Zhou, HM				Huang, Guang-Bin; Ding, Xiaojian; Zhou, Hongming			Optimization method based extreme learning machine for classification	NEUROCOMPUTING												Extreme learning machine (ELM) as an emergent technology has shown its good performance in regression applications as well as in large dataset (and/or multi-label) classification applications The ELM theory shows that the hidden nodes of the generalized single-hidden layer feedforward networks (SLFNs) which need not be neuron alike can be randomly generated and the universal approximation capability of such SLFNs can be guaranteed This paper further studies ELM for classification in the aspect of the standard optimization method and extends ELM to a specific type of generalized SLFNs support vector network. This paper shows that (1) under the ELM learning framework SVM s maximal margin property and the minimal norm of weights theory of feedforward neural networks are actually consistent (2) from the standard optimization method point of view ELM for classification and SVM are equivalent but ELM has less optimization constraints due to its special separability feature (3) as analyzed in theory and further verified by the simulation results ELM for classification tends to achieve better generalization performance than traditional SVM ELM for classification is less sensitive to user specified parameters and can be implemented easily (C) 2010 Elsevier B V All rights reserved				Huang, Guang-Bin/A-5035-2011	Huang, Guang-Bin/0000-0002-2480-4965												0925-2312					DEC	2010	74	1-3			SI		155	163		10.1016/j.neucom.2010.02.019						WOS:000285805800014		J	Ishibuchi, H; Nakashima, T; Murata, T				Ishibuchi, H; Nakashima, T; Murata, T			Performance evaluation of fuzzy classifier systems for multidimensional pattern classification problems	IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART B-CYBERNETICS												We examine the performance of a fuzzy genetics-based machine learning method for multidimensional pattern classification problems with continuous attributes. In our method, each fuzzy if-then rule is handled as an individual, and a fitness value is assigned to each rule. Thus, our method can be viewed as a classifier system. In this paper, we first describe fuzzy if-then rules and fuzzy reasoning for pattern classification problems. Then we explain a genetics-based machine learning method that automatically generates fuzzy if-then rules for pattern classification problems from numerical data. Because our method uses linguistic values with fixed membership functions as antecedent fuzzy sets, a linguistic interpretation of each fuzzy if-then rule is easily obtained. The fixed membership functions also lead to a simple implementation of our method as a computer program. The simplicity of implementation and the linguistic interpretation of the generated fuzzy if-then rules are the main characteristic features of our method. The performance of our method is evaluated by computer simulations on some well-known test problems. While our method involves no tuning mechanism of membership functions, it works very well in comparison with other classification methods such as nonfuzzy machine learning techniques and neural networks.				Ishibuchi, Hisao/B-3599-2009	Ishibuchi, Hisao/0000-0001-9186-6472												1083-4419	1941-0492				OCT	1999	29	5					601	618		10.1109/3477.790443						WOS:000082666700004	18252338	J	Birbaumer, N; Cohen, LG				Birbaumer, Niels; Cohen, Leonardo G.			Brain-computer interfaces: communication and restoration of movement in paralysis	JOURNAL OF PHYSIOLOGY-LONDON					Journal of Physiology Symposium on Physiology of Brain-Computer Interfaces	OCT   13, 2006	Atlanta, GA					The review describes the status of brain-computer or brain-machine interface research. We focus on non-invasive brain-computer interfaces (BCIs) and their clinical utility for direct brain communication in paralysis and motor restoration in stroke. A large gap between the promises of invasive animal and human BCI preparations and the clinical reality characterizes the literature: while intact monkeys learn to execute more or less complex upper limb movements with spike patterns from motor brain regions alone without concomitant peripheral motor activity usually after extensive training, clinical applications in human diseases such as amyotrophic lateral sclerosis and paralysis from stroke or spinal cord lesions show only limited success, with the exception of verbal communication in paralysed and locked-in patients. BCIs based on electroencephalographic potentials or oscillations are ready to undergo large clinical studies and commercial production as an adjunct or a major assisted communication device for paralysed and locked-in patients. However, attempts to train completely locked-in patients with BCI communication after entering the complete locked-in state with no remaining eye movement failed. We propose that a lack of contingencies between goal directed thoughts and intentions may be at the heart of this problem. Experiments with chronically curarized rats support our hypothesis; operant conditioning and voluntary control of autonomic physiological functions turned out to be impossible in this preparation. In addition to assisted communication, BCIs consisting of operant learning of EEG slow cortical potentials and sensorimotor rhythm were demonstrated to be successful in drug resistant focal epilepsy and attention deficit disorder. First studies of non-invasive BCIs using sensorimotor rhythm of the EEG and MEG in restoration of paralysed hand movements in chronic stroke and single cases of high spinal cord lesions show some promise, but need extensive evaluation in well-controlled experiments. Invasive BMIs based on neuronal spike patterns, local field potentials or electrocorticogram may constitute the strategy of choice in severe cases of stroke and spinal cord paralysis. Future directions of BCI research should include the regulation of brain metabolism and blood flow and electrical and magnetic stimulation of the human brain (invasive and non-invasive). A series of studies using BOLD response regulation with functional magnetic resonance imaging (fMRI) and near infrared spectroscopy demonstrated a tight correlation between voluntary changes in brain metabolism and behaviour.																	0022-3751					MAR 15	2007	579	3					621	636		10.1113/jphysiol.2006.125633						WOS:000244886900010	17234696	J	Lanitis, A; Taylor, CJ; Cootes, TF				Lanitis, A; Taylor, CJ; Cootes, TF			Toward automatic simulation of aging effects on face images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												The process of aging causes significant alterations in the facial appearance of individuals. When compared with other sources of variation in face images, appearance variation due to aging displays some unique characteristics. For example, aging variation is specific to a given individual; it occurs slowly and is affected significantly by other factors, such as health, gender, and lifestyle. Changes in facial appearance due to aging can even affect discriminatory facial features, resulting in deterioration of the ability of humans and machines to identify aged individuals. In this paper, we describe how the effects of aging on facial appearance can be explained using learned age transformations and present experimental results to show that reasonably accurate estimates of age can be made for unseen images. We also show that we can improve our results by taking into account the fact that different individuals age in different ways and by considering the effect of lifestyle. Our proposed framework can be used for simulating aging effects on new face images in order to predict how an individual might look like in the future or how he/she used to look in the past. The methodology presented has also been used for designing a face recognition system, robust to aging variation. In this context, the perceived age of the subjects in the training and test images is normalized before the training and classification procedure so that aging variation is eliminated. Experimental results demonstrate that, when age normalization is used, the performance of our face recognition system can be improved.					Lanitis, Andreas/0000-0001-6841-8065; Cootes, Timothy/0000-0002-2695-9063												0162-8828					APR	2002	24	4					442	455		10.1109/34.993553						WOS:000174574100002		J	Blankertz, B; Dornhege, G; Krauledat, M; Muller, KR; Curio, G				Blankertz, Benjamin; Dornhege, Guido; Krauledat, Matthias; Mueller, Klaus-Robert; Curio, Gabriel			The non-invasive Berlin Brain-Computer Interface: Fast acquisition of effective performance in untrained subjects	NEUROIMAGE												Brain-Computer Interface (BCI) systems establish a direct communication channel from the brain to an output device. These systems use brain signals recorded from the scalp, the surface of the cortex, or from inside the brain to enable users to control a variety of applications. BCI systems that bypass conventional motor output pathways of nerves and muscles can provide novel control options for paralyzed patients. One classical approach to establish EEG-based control is to set up a system that is controlled by a specific EEG feature which is known to be susceptible to conditioning and to let the subjects learn the voluntary control of that feature. In contrast, the Berlin Brain-Computer Interface (BBCI) uses well established motor competencies of its users and a machine learning approach to extract subject-specific patterns from high-dimensional features optimized for detecting the user's intent. Thus the long subject training is replaced by a short calibration measurement (20 min) and machine learning (I min). We report results from a study in which 10 subjects, who had no or little experience with BCI feedback, controlled computer applications by voluntary imagination of limb movements: these intentions led to modulations of spontaneous brain activity specifically, somatotopically matched sensorimotor 7-30 Hz rhythms were diminished over pericentral cortices. The peak information transfer rate was above 35 bits per minute (bpm) for 3 subjects, above 23 bpm for two, and above 12 bpm for 3 subjects, while one subject could achieve no BCI control. Compared to other BCI systems which need longer subject training to achieve comparable results, we propose that the key to quick efficiency in the BBCI system is its flexibility due to complex but physiologically meaningful features and its adaptivity which respects the enormous inter-subject variability. (c) 2007 Elsevier Inc. All rights reserved.				Muller, Klaus/C-3196-2013	Blankertz, Benjamin/0000-0002-2437-4846												1053-8119					AUG 15	2007	37	2					539	550		10.1016/j.neuroimage.2007.01.051						WOS:000248585400017	17475513	J	Friston, KJ				Friston, Karl J.			The free-energy principle: a rough guide to the brain?	TRENDS IN COGNITIVE SCIENCES												This article reviews a free-energy formulation that advances Helmholtz's agenda to find principles of brain function based on conservation laws and neuronal energy. It rests on advances in statistical physics, theoretical biology and machine learning to explain a remarkable range of facts about brain structure and function. We could have just scratched the surface of what this formulation offers; for example, it is becoming clear that the Bayesian brain is just one facet of the free-energy principle and that perception is an inevitable consequence of active exchange with the environment. Furthermore, one can see easily how constructs like memory, attention, value, reinforcement and salience might disclose their simple relationships within this framework.				Friston, Karl/D-9230-2011	Friston, Karl/0000-0001-7984-8909												1364-6613					JUL	2009	13	7					293	301		10.1016/j.tics.2009.04.005						WOS:000268379400005	19559644	J	Agarwal, A; Triggs, B				Agarwal, A; Triggs, B			Recovering 3D human pose from monocular images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												We describe a learning-based method for recovering 3D human body pose from single images and monocular image sequences. Our approach requires neither an explicit body model nor prior labeling of body parts in the image. Instead, it recovers pose by direct nonlinear regression against shape descriptor vectors extracted automatically from image silhouettes. For robustness against local silhouette segmentation errors, silhouette shape is encoded by histogram-of-shape-contexts descriptors. We evaluate several different regression methods: ridge regression, Relevance Vector Machine (RVM) regression, and Support Vector Machine (SVM) regression over both linear and kernel bases. The RVMs provide much sparser regressors without compromising performance, and kernel bases give a small but worthwhile improvement in performance. The loss of depth and limb labeling information often makes the recovery of 3D pose from single silhouettes ambiguous. To handle this, the method is embedded in a novel regressive tracking framework, using dynamics from the previous state estimate together with a learned regression value to disambiguate the pose. We show that the resulting system tracks long sequences stably. For realism and good generalization over a wide range of viewpoints, we train the regressors on images resynthesized from real human motion capture data. The method is demonstrated for several representations of full body pose, both quantitatively on independent but similar test data and qualitatively on real image sequences. Mean angular errors of 4-6 degrees are obtained for a variety of walking motions.																	0162-8828	1939-3539				JAN	2006	28	1					44	58		10.1109/TPAMI.2006.21						WOS:000233172000004	16402618	J	COST, S; SALZBERG, S				COST, S; SALZBERG, S			A WEIGHTED NEAREST NEIGHBOR ALGORITHM FOR LEARNING WITH SYMBOLIC FEATURES	MACHINE LEARNING												In the past, nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values. in such domains, the examples can be treated as points and distance metrics can use standard definitions. In symbolic domains, a more sophisticated treatment of the feature space is required. We introduce a nearest neighbor algorithm for learning in domains with symbolic features. Our algorithm calculates distance tables that allow it to produce real-valued distances between instances. and attaches weights to the instances to further modify the structure of feature space. We show that this technique produces excellent classification accuracy on three problems that have been studied by machine learning researchers: predicting protein secondary structure, identifying DNA promoter sequences, and pronouncing English text. Direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior in all three domains. In addition, our algorithm has advantages in training speed, simplicity, and perspicuity. We conclude that experimental evidence favors the use and continued development of nearest neighbor algorithms for domains such as the ones studied here.				Salzberg, Steven/F-6162-2011	Salzberg, Steven/0000-0002-8859-7432												0885-6125					JAN	1993	10	1					57	78		10.1007/BF00993481						WOS:A1993KL35400002		J	Zhou, ZH; Liu, XY				Zhou, ZH; Liu, XY			Training cost-sensitive neural networks with methods addressing the class imbalance problem	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both oversampling and undersampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassified. Moreover, hard-ensemble and soft-ensemble, i.e., the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one UCI data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that cost-sensitive learning with multiclass tasks is more difficult than with two-class tasks, and a higher degree of class imbalance may increase the difficulty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multiclass tasks. Overall, threshold-moving and soft-ensemble are relatively good choices in training cost-sensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may, in fact, only be effective on learning with imbalanced two-class data sets.																	1041-4347					JAN	2006	18	1					63	77		10.1109/TKDE.2006.17						WOS:000233938200005		J	Chen, YX; Wang, JZ				Chen, YX; Wang, JZ			Image categorization by learning and reasoning with regions	JOURNAL OF MACHINE LEARNING RESEARCH												Designing computer programs to automatically categorize images using low-level features is a challenging research topic in computer vision. In this paper, we present a new learning technique, which extends Multiple-Instance Learning (MIL), and its application to the problem of region-based image categorization. Images are viewed as bags, each of which contains a number of instances corresponding to regions obtained from image segmentation. The standard MIL problem assumes that a bag is labeled positive if at least one of its instances is positive; otherwise, the bag is negative. In the proposed MIL framework, DD-SVM, a bag label is determined by some number of instances satisfying various properties. DD-SVM first learns a collection of instance prototypes according to a Diverse Density (DD) function. Each instance prototype represents a class of instances that is more likely to appear in bags with the specific label than in the other bags. A nonlinear mapping is then defined using the instance prototypes and maps every bag to a point in a new feature space, named the bag feature space. Finally, standard support vector machines are trained in the bag feature space. We provide experimental results on an image categorization problem and a drug activity prediction problem.																	1532-4435					AUG	2004	5						913	939								WOS:000236328000003		J	Geng, X; Zhou, ZH; Smith-Miles, K				Geng, Xin; Zhou, Zhi-Hua; Smith-Miles, Kate			Automatic age estimation based on facial aging patterns	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												While recognition of most facial variations, such as identity, expression, and gender, has been extensively studied, automatic age estimation has rarely been explored. In contrast to other facial variations, aging variation presents several unique characteristics which make age estimation a challenging task. This paper proposes an automatic age estimation method named AGES ( AGing pattErn Subspace). The basic idea is to model the aging pattern, which is defined as the sequence of a particular individual's face images sorted in time order, by constructing a representative subspace. The proper aging pattern for a previously unseen face image is determined by the projection in the subspace that can reconstruct the face image with minimum reconstruction error, while the position of the face image in that aging pattern will then indicate its age. In the experiments, AGES and its variants are compared with the limited existing age estimation methods ( WAS and AAS) and some well- established classification methods (kNN, BP, C4.5, and SVM). Moreover, a comparison with human perception ability on age is conducted. It is interesting to note that the performance of AGES is not only significantly better than that of all the other algorithms, but also comparable to that of the human observers.				Geng, Xin/A-5290-2008; Smith-Miles, Kate/B-7493-2008	Smith-Miles, Kate/0000-0003-2718-7680												0162-8828	1939-3539				DEC	2007	29	12					2234	2240		10.1109/TPAMI.2007.70733						WOS:000250087900014	17934231	J	Kivinen, J; Smola, AJ; Williamson, RC				Kivinen, J; Smola, AJ; Williamson, RC			Online learning with kernels	IEEE TRANSACTIONS ON SIGNAL PROCESSING												Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with, the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.																	1053-587X					AUG	2004	52	8					2165	2176		10.1109/TSP.2004.830991						WOS:000222760500003		J	Ando, RK; Zhang, T				Ando, RK; Zhang, T			A framework for learning predictive structures from multiple tasks and unlabeled data	JOURNAL OF MACHINE LEARNING RESEARCH												One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.																	1532-4435					NOV	2005	6						1817	1853								WOS:000236330700003		J	Panait, L; Luke, S				Panait, L; Luke, S			Cooperative multi-agent learning: The state of the art	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS												Cooperative multi-agent systems (MAS) are ones in which several agents attempt, through their interaction, to jointly solve tasks or to maximize utility. Due to the interactions among the agents, multi-agent problem complexity can rise rapidly with the number of agents or their behavioral sophistication. The challenge this presents to the task of programming solutions to MAS problems has spawned increasing interest in machine learning techniques to automate the search and optimization process. We provide a broad survey of the cooperative multi-agent learning literature. Previous surveys of this area have largely focused on issues common to specific subareas (for example, reinforcement learning, RL or robotics). In this survey we attempt to draw from multi-agent learning work in a spectrum of areas, including RL, evolutionary computation, game theory, complex systems, agent modeling, and robotics. We find that this broad view leads to a division of the work into two categories, each with its own special issues: applying a single learner to discover joint solutions to multi-agent problems (team learning), or using multiple simultaneous learners, often one per agent (concurrent learning). Additionally, we discuss direct and indirect communication in connection with learning, plus open issues in task decomposition, scalability, and adaptive dynamics. We conclude with a presentation of multi-agent learning problem domains, and a list of multi-agent learning resources.				, /B-8889-2012													1387-2532	1573-7454				NOV	2005	11	3					387	434		10.1007/s10458-005-2631-2						WOS:000232377200005		J	De Boer, PT; Kroese, DP; Mannor, S; Rubinstein, RY				De Boer, PT; Kroese, DP; Mannor, S; Rubinstein, RY			A tutorial on the cross-entropy method	ANNALS OF OPERATIONS RESEARCH												The cross-entropy (CE) method is a new generic approach to combinatorial and multi-extremal optimization and rare event simulation. The purpose of this tutorial is to give a gentle introduction to the CE method. We present the CE methodology, the basic algorithm and its modifications, and discuss applications in combinatorial optimization and machine learning.				Kroese, Dirk/B-2029-2008	Kroese, Dirk/0000-0003-2281-9431												0254-5330					JAN	2005	134	1					19	67		10.1007/s10479-005-5724-z						WOS:000228195500002		B	Wan, EA; van der Merwe, R			IEEE; IEEE	Wan, EA; van der Merwe, R			The unscented Kalman Filter for nonlinear estimation	IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS					Symposium on Adaptive Systems for Signal Processing, Communications, and Control (AS-SPCC)	OCT 01-04, 2000	MCMASTER UNIV, LAKE LOUISE, CANADA	IEEE, IEEE, So Alberta Sect, IEEE, Commun Soc, IEEE, Control Syst Soc, IEE, Japanese Neural Network Soc, Int Neural Network Soc, Commun Res Ctr, Def & Civilian Inst Environm Med, IEEE, Canadian Fdn, McMaster Univ, Natl Sci Fdn, Nortel Networks	MCMASTER UNIV			The Extended Kalman Filter (EKF) has become a standard technique used in a number of nonlinear estimation and machine learning applications. These include estimating the state of a nonlinear dynamic system, estimating parameters for nonlinear system identification (e.g., learning the weights of a neural network), and dual estimation (e.g., the Expectation Maximization (EM) algorithm) where both states and parameters are estimated simultaneously. This paper points out the flaws in using the EKF, and introduces an improvement, the Unscented Kalman Filter (UKF), proposed by Julier and Uhlman [5]. A central and vital operation performed in the Kalman Filter is the propagation of a Gaussian random variable (GRV) through the system dynamics. In the EKE the state distribution is approximated by a GRV, which is then propagated analytically through the first-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed GRV, which may lead to sub-optimal performance and sometimes divergence of the filter. The UKF addresses this problem by using a deterministic sampling approach. The state distribution is again approximated by a GRV, but is now represented using a minimal set of carefully chosen sample points. These sample paints completely capture the true mean and covariance of the GRV, and when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to the 3rd order (Taylor series expansion) for any nonlinearity. The EKE in contrast, only achieves first-order accuracy. Remarkably, the computational complexity of the UKF is the same order as that of the EKF. Julier and Uhlman demonstrated the substantial performance gains of the UKF in the context of state-estimation for nonlinear control. Machine learning problems were not considered. We extend the use of the UKF to a broader class of nonlinear estimation problems, including nonlinear system identification, training of neural networks, and dual estimation problems. Our preliminary results were presented in [13]. in this paper, the algorithms are further developed and illustrated with a number of additional examples.																			0-7803-5800-7				2000							153	158		10.1109/ASSPCC.2000.882463						WOS:000165737500027		J	Hoffmann, U; Vesin, JM; Ebrahimi, T; Diserens, K				Hoffmann, Ulrich; Vesin, Jean-Marc; Ebrahimi, Touradj; Diserens, Karin			An efficient P300-based brain-computer interface for disabled subjects	JOURNAL OF NEUROSCIENCE METHODS												A brain-computer interface (130) is a communication system that translates brain-activity into commands for a computer or other devices. In other words, a BCI allows users to act on their environment by using only brain-activity, without using peripheral nerves and muscles. In this paper, we present a BCI that achieves high classification accuracy and high bitrates for both disabled and able-bodied subjects. The system is based on the P300 evoked potential and is tested with five severely disabled and four able-bodied subjects. For four of the disabled subjects classification accuracies of 100% are obtained. The bitrates obtained for the disabled subjects range between 10 and 25 bits/min. The effect of different electrode configurations and machine learning algorithms on classification accuracy is tested. Further factors that are possibly important for obtaining good classification accuracy in P300-based BCI systems for disabled subjects are discussed. (C) 2007 Elsevier B.V. All rights reserved.																	0165-0270					JAN 15	2008	167	1					115	125		10.1016/j.jneumeth.2007.03.005						WOS:000252164300012	17445904	J	Foley, AM; Leahy, PG; Marvuglia, A; McKeogh, EJ				Foley, Aoife M.; Leahy, Paul G.; Marvuglia, Antonino; McKeogh, Eamon J.			Current methods and advances in forecasting of wind power generation	RENEWABLE ENERGY												Wind power generation differs from conventional thermal generation due to the stochastic nature of wind. Thus wind power forecasting plays a key role in dealing with the challenges of balancing supply and demand in any electricity system, given the uncertainty associated with the wind farm power output. Accurate wind power forecasting reduces the need for additional balancing energy and reserve power to integrate wind power. Wind power forecasting tools enable better dispatch, scheduling and unit commitment of thermal generators, hydro plant and energy storage plant and more competitive market trading as wind power ramps up and down on the grid. This paper presents an in-depth review of the current methods and advances in wind power forecasting and prediction. Firstly, numerical wind prediction methods from global to local scales, ensemble forecasting, upscaling and downscaling processes are discussed. Next the statistical and machine learning approach methods are detailed. Then the techniques used for benchmarking and uncertainty analysis of forecasts are overviewed, and the performance of various approaches over different forecast time horizons is examined. Finally, current research activities, challenges and potential future developments are appraised. (C) 2011 Elsevier Ltd. All rights reserved.					Mckeogh, Eamon/0000-0002-7803-6180; Marvuglia, Antonino/0000-0002-8360-8040; Foley, Aoife/0000-0001-6491-2592												0960-1481					JAN	2012	37	1					1	8		10.1016/j.renene.2011.05.033						WOS:000295766000001		J	Hofmann, T; Scholkopf, B; Smola, AJ				Hofmann, Thomas; Schoelkopf, Bernhard; Smola, Alexander J.			Kernel methods in machine learning	ANNALS OF STATISTICS												We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.				Scholkopf, Bernhard/A-7570-2013													0090-5364					JUN	2008	36	3					1171	1220		10.1214/009053607000000677						WOS:000256504400007		J	Gyimothy, T; Ferenc, R; Siket, I				Gyimothy, T; Ferenc, R; Siket, I			Empirical validation of object-oriented metrics on open source software for fault prediction	IEEE TRANSACTIONS ON SOFTWARE ENGINEERING					20th IEEE International Conference on Software Maintenance (ICSM 2004)	SEP 11-14, 2004	Chicago, IL	IEEE Comp Soc				Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.																	0098-5589	1939-3520				OCT	2005	31	10					897	910		10.1109/TSE.2005.112						WOS:000233015300008		J	Nadeau, C; Bengio, Y				Nadeau, C; Bengio, Y			Inference for the generalization error	MACHINE LEARNING												In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich ( 1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.																	0885-6125					SEP	2003	52	3					239	281		10.1023/A:1024068626366						WOS:000183326600003		J	Zhu, QY; Qin, AK; Suganthan, PN; Huang, GB				Zhu, QY; Qin, AK; Suganthan, PN; Huang, GB			Evolutionary extreme learning machine	PATTERN RECOGNITION												Extreme learning machine (ELM) [G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine: a new learning scheme of feedforward neural networks, in: Proceedings of the International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25-29 July 2004], a novel learning algorithm much faster than the traditional gradient-based learning algorithms, was proposed recently for single-hidden-layer feedforward neural networks (SLFNs). However, ELM may need higher number of hidden neurons due to the random determination of the input weights and hidden biases. In this paper, a hybrid learning algorithm is proposed which uses the differential evolutionary algorithm to select the input weights and Moore-Penrose (MP) generalized inverse to analytically determine the output weights. Experimental results show that this approach is able to achieve good generalization performance with much more compact networks. (c) 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.				Huang, Guang-Bin/A-5035-2011;  Suganthan, .Ponnuthurai /A-5023-2011	Huang, Guang-Bin/0000-0002-2480-4965;  Suganthan, .Ponnuthurai /0000-0003-0901-5105												0031-3203					OCT	2005	38	10					1759	1763		10.1016/j.patcog.2005.03.028						WOS:000231291900022		J	Pal, M; Mather, PM				Pal, M; Mather, PM			Support vector machines for classification in remote sensing	INTERNATIONAL JOURNAL OF REMOTE SENSING												Support vector machines (SVM) represent a promising development in machine learning research that is not widely used within the remote sensing community. This paper reports the results of two experiments in which multi-class SVMs are compared with maximum likelihood (ML) and artificial neural network (ANN) methods in terms of classification accuracy. The two land cover classification experiments use multispectral (Landsat-7 ETM+) and hyperspectral (DAIS) data, respectively, for test areas in eastern England and central Spain. Our results show that the SVM achieves a higher level of classification accuracy than either the ML or the ANN classifier, and that the SVM can be used with small training datasets and high-dimensional data.				Pal, Mahesh /P-1136-2014	Pal, Mahesh /0000-0003-1805-2952												0143-1161	1366-5901				MAR 10	2005	26	5					1007	1011		10.1080/01431160512331314083						WOS:000226864100013		J	Dreiseitl, S; Ohno-Machado, L				Dreiseitl, S; Ohno-Machado, L			Logistic regression and artificial neural network classification models: a methodology review	JOURNAL OF BIOMEDICAL INFORMATICS												Logistic regression and artificial neural networks are the models of choice in many medical data classification tasks. In this review, we summarize the differences and similarities of these models from a technical point of view, and compare them with other machine learning algorithms. We provide considerations useful for critically assessing the quality of the models and the results based on these models. Finally, we summarize our findings on how quality criteria for logistic regression and artificial neural network models are met in a sample of papers from the medical literature. (C) 2003 Elsevier Science (USA). All rights reserved.					Ohno-Machado, Lucila/0000-0002-8005-7327												1532-0464					OCT-DEC	2002	35	5-6					352	359		10.1016/S1532-0464(03)00034-0						WOS:000184879000009	12968784	J	Jung, M; Reichstein, M; Margolis, HA; Cescatti, A; Richardson, AD; Arain, MA; Arneth, A; Bernhofer, C; Bonal, D; Chen, JQ; Gianelle, D; Gobron, N; Kiely, G; Kutsch, W; Lasslop, G; Law, BE; Lindroth, A; Merbold, L; Montagnani, L; Moors, EJ; Papale, D; Sottocornola, M; Vaccari, F; Williams, C				Jung, Martin; Reichstein, Markus; Margolis, Hank A.; Cescatti, Alessandro; Richardson, Andrew D.; Arain, M. Altaf; Arneth, Almut; Bernhofer, Christian; Bonal, Damien; Chen, Jiquan; Gianelle, Damiano; Gobron, Nadine; Kiely, Gerald; Kutsch, Werner; Lasslop, Gitta; Law, Beverly E.; Lindroth, Anders; Merbold, Lutz; Montagnani, Leonardo; Moors, Eddy J.; Papale, Dario; Sottocornola, Matteo; Vaccari, Francesco; Williams, Christopher			Global patterns of land-atmosphere fluxes of carbon dioxide, latent heat, and sensible heat derived from eddy covariance, satellite, and meteorological observations	JOURNAL OF GEOPHYSICAL RESEARCH-BIOGEOSCIENCES												We upscaled FLUXNET observations of carbon dioxide, water, and energy fluxes to the global scale using the machine learning technique, model tree ensembles (MTE). We trained MTE to predict site-level gross primary productivity (GPP), terrestrial ecosystem respiration (TER), net ecosystem exchange (NEE), latent energy (LE), and sensible heat (H) based on remote sensing indices, climate and meteorological data, and information on land use. We applied the trained MTEs to generate global flux fields at a 0.5 degrees x 0.5 degrees spatial resolution and a monthly temporal resolution from 1982 to 2008. Cross-validation analyses revealed good performance of MTE in predicting among-site flux variability with modeling efficiencies (MEf) between 0.64 and 0.84, except for NEE (MEf = 0.32). Performance was also good for predicting seasonal patterns (MEf between 0.84 and 0.89, except for NEE (0.64)). By comparison, predictions of monthly anomalies were not as strong (MEf between 0.29 and 0.52). Improved accounting of disturbance and lagged environmental effects, along with improved characterization of errors in the training data set, would contribute most to further reducing uncertainties. Our global estimates of LE (158 +/- 7 J x 10(18) yr(-1)), H (164 +/- 15 J x 10(18) yr(-1)), and GPP (119 +/- 6 Pg C yr(-1)) were similar to independent estimates. Our global TER estimate (96 +/- 6 Pg C yr(-1)) was likely underestimated by 5-10%. Hot spot regions of interannual variability in carbon fluxes occurred in semiarid to semihumid regions and were controlled by moisture supply. Overall, GPP was more important to interannual variability in NEE than TER. Our empirically derived fluxes may be used for calibration and evaluation of land surface process models and for exploratory and diagnostic assessments of the biosphere.				Lindroth, Anders/N-4697-2014; Chen, Jiquan/D-1955-2009; Richardson, Andrew/F-5691-2011; Law, Beverly/G-3882-2010; Sottocornola, Matteo/E-9092-2010; Vaccari, Francesco Primo/C-2123-2009; Moors, Eddy/J-5165-2012; Merbold, Lutz/K-6103-2012; Arneth, Almut/B-2702-2013; Gianelle, Damiano/G-9437-2011; Kiely, Gerard/I-8158-2013; Garmisch-Pa, Ifu/H-9902-2014; Reichstein, Markus/A-7494-2011	Lindroth, Anders/0000-0002-7669-784X; Richardson, Andrew/0000-0002-0148-6714; Law, Beverly/0000-0002-1605-1203; Vaccari, Francesco Primo/0000-0002-5253-2135; Moors, Eddy/0000-0003-2309-2887; Merbold, Lutz/0000-0003-4974-170X; Gianelle, Damiano/0000-0001-7697-5793; Reichstein, Markus/0000-0001-5736-1112; Arain, M. Altaf/0000-0002-1433-5173; Kiely, Gerard/0000-0003-2189-6427; Montagnani, Leonardo/0000-0003-2957-9071; Papale, Dario/0000-0001-5170-8648												0148-0227					SEP 3	2011	116								G00J07	10.1029/2010JG001566						WOS:000294615800001		J	Jensen, R; Shen, Q				Jensen, R; Shen, Q			Semantics-preserving dimensionality reduction: Rough and fuzzy-rough-based approaches	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												Semantics-preserving dimensionality reduction refers to the problem of selecting those input features that are most predictive of a given outcome; a problem encountered in many areas such as machine learning, pattern recognition, and signal processing. This has found successful application in tasks that involve data sets containing huge numbers of features ( in the order of tens of thousands), which would be impossible to process further. Recent examples include text processing and Web content classification. One of the many successful applications of rough set theory has been to this feature selection area. This paper reviews those techniques that preserve the underlying semantics of the data, using crisp and fuzzy rough set-based methodologies. Several approaches to feature selection based on rough set theory are experimentally compared. Additionally, a new area in feature selection, feature grouping, is highlighted and a rough set-based feature grouping technique is detailed.																	1041-4347					DEC	2004	16	12					1457	1471		10.1109/TKDE.2004.96						WOS:000224586800001		J	Galar, M; Fernandez, A; Barrenechea, E; Bustince, H; Herrera, F				Galar, Mikel; Fernandez, Alberto; Barrenechea, Edurne; Bustince, Humberto; Herrera, Francisco			A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches	IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART C-APPLICATIONS AND REVIEWS												Classifier learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most significant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of the results.				Galar, Mikel/H-4846-2011; Herrera, Francisco/C-6856-2008; Bustince, Humberto/H-4868-2011; Fernandez, Alberto/G-3827-2014	Galar, Mikel/0000-0003-2865-6549; Herrera, Francisco/0000-0002-7283-312X; Bustince, Humberto/0000-0002-1279-6195; Barrenechea, Edurne/0000-0001-6657-948X												1094-6977	1558-2442				JUL	2012	42	4					463	484		10.1109/TSMCC.2011.2161285						WOS:000305584800003		J	Mairal, J; Bach, F; Ponce, J				Mairal, Julien; Bach, Francis; Ponce, Jean			Task-Driven Dictionary Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience, and signal processing. For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks. In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools. The same approach has also been used for learning features from data for other purposes, e. g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult. In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem. Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.																	0162-8828					APR	2012	34	4					791	804		10.1109/TPAMI.2011.156						WOS:000300581700012	21808090	J	Miche, Y; Sorjamaa, A; Bas, P; Simula, O; Jutten, C; Lendasse, A				Miche, Yoan; Sorjamaa, Antti; Bas, Patrick; Simula, Olli; Jutten, Christian; Lendasse, Amaury			OP-ELM: Optimally Pruned Extreme Learning Machine	IEEE TRANSACTIONS ON NEURAL NETWORKS												In this brief, the optimally pruned extreme learning machine (OP-ELM) methodology is presented. It is based on the original extreme learning machine (ELM) algorithm with additional steps to make it more robust and generic. The whole methodology is presented in detail and then applied to several regression and classification problems. Results for both computational time and accuracy (mean square error) are compared to the original ELM and to three other widely used methodologies: multilayer perceptron (MLP), support vector machine (SVM), and Gaussian process (GP). As the experiments for both regression and classification illustrate, the proposed OP-ELM methodology performs several orders of magnitude faster than the other algorithms used in this brief, except the original ELM. Despite the simplicity and fast performance, the OP-ELM is still able to maintain an accuracy that is comparable to the performance of the SVM. A toolbox for the OP-ELM is publicly available online.				Miche, Yoan/J-9079-2014	Miche, Yoan/0000-0001-8864-2312; Lendasse, Amaury/0000-0001-5410-4751												1045-9227	1941-0093				JAN	2010	21	1					158	162		10.1109/TNN.2009.2036259						WOS:000273339800012	20007026	J	Middleton, SE; Shadbolt, NR; De Roure, DC				Middleton, SE; Shadbolt, NR; De Roure, DC			Ontological user profiling in recommender systems	ACM TRANSACTIONS ON INFORMATION SYSTEMS												We explore a novel ontological approach to user profiling within recommender systems, working on the problem of recommending on-line academic research papers. Our two experimental systems, Quickstep and Foxtrot, create user profiles from unobtrusively monitored behaviour and relevance feedback, representing the profiles in terms of a research paper topic ontology. A novel profile visualization approach is taken to acquire profile feedback. Research papers are classified using ontological classes and collaborative recommendation algorithms used to recommend papers seen by similar people on their current topics of interest. Two small-scale experiments, with 24 subjects over 3 months, and a large-scale experiment, with 260 subjects over an academic year, are conducted to evaluate different aspects of our approach. Ontological inference is shown to improve user profiling, external ontological knowledge used to successfully bootstrap a recommender system and profile visualization employed to improve profiling accuracy. The overall performance of our ontological recommender systems are also presented and favourably compared to other systems in the literature.				De Roure, David/D-6785-2011	De Roure, David/0000-0001-9074-3016; Middleton, Stuart/0000-0001-8305-8176												1046-8188					JAN	2004	22	1					54	88		10.1145/963770.963773						WOS:000188554900003		J	Blankertz, B; Muller, KR; Krusienski, DJ; Schalk, G; Wolpaw, JR; Schlogl, A; Pfurtscheller, G; Millan, JDR; Schroder, M; Birbaumer, N				Blankertz, B; Muller, KR; Krusienski, DJ; Schalk, G; Wolpaw, JR; Schlogl, A; Pfurtscheller, G; Millan, JDR; Schroder, M; Birbaumer, N			The BCI competition III: Validating alternative approaches to actual BCI problems	IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING					3rd International Meeting on Brain-Computer Interface Technology	JUN, 2005	Rensselaerville Inst, Rensselaerville, NY		Rensselaerville Inst			A brain-computer interface (BCI) is a system that allows its users to control external devices with brain activity. Although the proof-of-concept was given decades ago, the reliable translation of user intent into device control commands is still a major challenge. Success requires the effective interaction of two adaptive controllers: the user's brain, which produces brain activity that encodes intent, and the BCI system, which translates that activity into device control commands. In order to facilitate this interaction, many laboratories are exploring a variety of signal analysis techniques to improve the adaptation of the BCI system to the user. In the literature, many machine learning and pattern classification algorithms have been reported to give impressive results when applied to BCI data in offline analyses. However, it is more difficult to evaluate their relative value for actual online use. BCI data competitions have been organized to provide objective formal evaluations of alternative methods. Prompted by the great interest in the first two BCI Competitions, we organized the third BCI Competition to address several of the most difficult and important analysis problems in BCI research. The paper describes the data sets that were provided to the competitors and gives an overview of the results.				Millan, Jose del R./F-1696-2011; Muller, Klaus/C-3196-2013	Millan, Jose del R./0000-0001-5819-1522; 												1534-4320					JUN	2006	14	2					153	159		10.1109/TNSRE.2006.875642						WOS:000238394700008	16792282	J	Thelwall, M; Buckley, K; Paltoglou, G; Cai, D; Kappas, A				Thelwall, Mike; Buckley, Kevan; Paltoglou, Georgios; Cai, Di; Kappas, Arvid			Sentiment in Short Strength Detection Informal Text	JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY												A huge number of informal messages are posted every day in social network sites, blogs, and discussion forums. Emotions seem to be frequently important in these texts for expressing friendship, showing social support or as part of online arguments. Algorithms to identify sentiment and sentiment strength are needed to help understand the role of emotion in this informal communication and also to identify inappropriate or anomalous affective utterances, potentially associated with threatening behavior to the self or others. Nevertheless, existing sentiment detection algorithms tend to be commercially oriented, designed to identify opinions about products rather than user behaviors. This article partly fills this gap with a new algorithm, SentiStrength, to extract sentiment strength from informal English text, using new methods to exploit the de facto grammars and spelling styles of cyberspace. Applied to MySpace comments and with a lookup table of term sentiment strengths optimized by machine learning, SentiStrength is able to predict positive emotion with 60.6% accuracy and negative emotion with 72.8% accuracy, both based upon strength scales of 1-5. The former, but not the latter, is better than baseline and a wide range of general machine learning approaches.				Thelwall, Mike/C-1449-2013; Kappas, Arvid/F-8224-2013	Thelwall, Mike/0000-0001-6065-205X; Kappas, Arvid/0000-0002-7715-8709												1532-2882					DEC	2010	61	12					2544	2558		10.1002/asi.21416						WOS:000284231100013		J	Shen, JW; Zhang, J; Luo, XM; Zhu, WL; Yu, KQ; Chen, KX; Li, YX; Jiang, HL				Shen, Juwen; Zhang, Jian; Luo, Xiaomin; Zhu, Weiliang; Yu, Kunqian; Chen, Kaixian; Li, Yixue; Jiang, Hualiang			Predictina protein-protein interactions based only on sequences information	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA												Protein-protein interactions (PPIs) are central to most biological processes. Although efforts have been devoted to the development of methodology for predicting PPIs and protein interaction networks, the application of most existing methods is limited because they need information about protein homology or the interaction marks of the protein partners. In the present work, we propose a method for PPI prediction using only the information of protein sequences. This method was developed based on a learning algorithm-support vector machine combined with a kernel function and a conjoint triad feature for describing amino acids. More than 16,000 diverse PPI pairs were used to construct the universal model. The prediction ability of our approach is better than that of other sequence-based PPI prediction methods because it is able to predict PPI networks. Different types of PPI networks have been effectively mapped with our method, suggesting that, even with only sequence information, this method could be applied to the exploration of networks for any newly discovered protein with unknown biological relativity. In addition, such supplementary experimental information can enhance the prediction ability of the method.				Luo, Xiaomin/R-6203-2016	Luo, Xiaomin/0000-0003-0426-3417												0027-8424					MAR 13	2007	104	11					4337	4341		10.1073/pnas.0607879104						WOS:000244972700019	17360525	J	Cheng, B; Yang, JC; Yan, SC; Fu, Y; Huang, TS				Cheng, Bin; Yang, Jianchao; Yan, Shuicheng; Fu, Yun; Huang, Thomas S.			Learning With l(1)-Graph for Image Analysis	IEEE TRANSACTIONS ON IMAGE PROCESSING												The graph construction procedure essentially determines the potentials of those graph-oriented learning algorithms for image analysis. In this paper, we propose a process to build the so-called directed graph, in which the vertices involve all the samples and the ingoing edge weights to each vertex describe its norm driven reconstruction from the remaining samples and the noise. Then, a series of new algorithms for various machine learning tasks, e. g., data clustering, subspace learning, and semi-supervised learning, are derived upon the graphs. Compared with the conventional-nearest-neighbor graph and epsilon-ball graph, the graph possesses the advantages: 1) greater robustness to data noise, 2) automatic sparsity, and 3) adaptive neighborhood for individual datum. Extensive experiments on three real-world datasets show the consistent superiority of graph over those classic graphs in data clustering, subspace learning, and semi-supervised learning tasks.																	1057-7149					APR	2010	19	4					858	866		10.1109/TIP.2009.2038764						WOS:000275662900002	20031500	J	Yu, CS; Lin, CJ; Hwang, JK				Yu, CS; Lin, CJ; Hwang, JK			Predicting subcellular localization of proteins for Gram-negative bacteria by support vector machines based on n-peptide compositions	PROTEIN SCIENCE												Gram-negative bacteria have five major subcellular localization sites: the cytoplasm, the periplasm, the inner membrane, the outer membrane, and the extracellular space. The subcellular location of a protein can provide valuable information about its function. With the rapid increase of sequenced genomic data, the need for an automated and accurate to predict subcellular localization becomes increasingly important. We present an approach to predict subcellular localization for Gram-negative bacteria. This method uses the support vector machines trained by multiple feature vectors based on n-peptide compositions. For a standard data set comprising 1443 proteins, the overall prediction accuracy reaches 89%, which, to the best of our knowledge, is the highest prediction rate ever reported. Our prediction is 14% higher than that of the recently developed multimodular PSORT-B. Because of its simplicity, this approach can be easily extended to other organisms and should be a useful tool for the high-throughput and large-scale analysis of proteomic and genomic data.					Lin, Chih-Jen/0000-0003-4684-8747												0961-8368					MAY	2004	13	5					1402	1406		10.1110/ps.03479604						WOS:000221042200024	15096640	J	PEARLMUTTER, BA				PEARLMUTTER, BA			GRADIENT CALCULATIONS FOR DYNAMIC RECURRENT NEURAL NETWORKS - A SURVEY	IEEE TRANSACTIONS ON NEURAL NETWORKS												We survey learning algorithms for recurrent neural networks with hidden units and put the various techniques into a common framework, We discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture, Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed, In many cases, the unified presentation leads to generalizations of various sorts, We discuss advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continue with some ''tricks of the trade'' for training, using, and simulating continuous time and recurrent neural networks, We present some simulations, and at the end, address issues of computational complexity and learning speed.				Pearlmutter, Barak/M-8791-2014	Pearlmutter, Barak/0000-0003-0521-4553												1045-9227					SEP	1995	6	5					1212	1228		10.1109/72.410363						WOS:A1995RT52300017	18263409	J	Lee, YK; Lin, Y; Wahba, G				Lee, YK; Lin, Y; Wahba, G			Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION												Two-category support vector machines (SVM) have been very popular in the machine learning community for classification problems. Solving multicategory problems by a series of binary classifiers is quite common in the SVM paradigm; however, this approach may fail under various circumstances. We propose the multicategory support vector machine (MSVM), which extends the binary SVM to the multicategory case and has good theoretical properties. The proposed method provides a unifying framework when there are either equal or unequal misclassification costs. As a tuning criterion for the MSVM, an approximate leave-one-out cross-validation function, called Generalized Approximate Cross Validation, is derived. analogous to the binary case. The effectiveness of the MSVM is demonstrated through the applications to cancer classification using microarray data and cloud classification with satellite radiance profiles.				Lee, Yoonkyung/K-4360-2015	Lee, Yoonkyung/0000-0002-5756-6588												0162-1459					MAR	2004	99	465					67	81		10.1198/016214504000000098						WOS:000220638200008		J	TOWELL, GG; SHAVLIK, JW				TOWELL, GG; SHAVLIK, JW			KNOWLEDGE-BASED ARTIFICIAL NEURAL NETWORKS	ARTIFICIAL INTELLIGENCE												Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for accurately classifying examples not seen during training. The challenge of hybrid learning systems is to use the information provided by one source of information to offset information missing from the other source. By so doing, a hybrid learning system should learn more effectively than systems that use only one of the information sources. KBANN (Knowledge-Based Artificial Neural Networks) is a hybrid learning system built on top of connectionist learning techniques. It maps problem-specific ''domain theories'', represented in propositional logic, into neural networks and then refines this reformulated knowledge using backpropagation. KBANN is evaluated by extensive empirical tests on two problems from molecular biology. Among other results, these tests show that the networks created by KBANN generalize better than a wide variety of learning systems, as well as several techniques proposed by biologists.																	0004-3702					OCT	1994	70	1-2					119	165		10.1016/0004-3702(94)90105-8						WOS:A1994PQ99900005		J	Saier, MH; Yen, MR; Noto, K; Tamang, DG; Elkan, C				Saier, Milton H., Jr.; Yen, Ming Ren; Noto, Keith; Tamang, Dorjee G.; Elkan, Charles			The Transporter Classification Database: recent advances	NUCLEIC ACIDS RESEARCH												The Transporter Classification Database ( TCDB), freely accessible at http://www.tcdb.org, is a relational database containing sequence, structural, functional and evolutionary information about transport systems from a variety of living organisms, based on the International Union of Biochemistry and Molecular Biology-approved transporter classification (TC) system. It is a curated repository for factual information compiled largely from published references. It uses a functional/phylogenetic system of classification, and currently encompasses about 5000 representative transporters and putative transporters in more than 500 families. We here describe novel software designed to support and extend the usefulness of TCDB. Our recent efforts render it more user friendly, incorporate machine learning to input novel data in a semiautomatic fashion, and allow analyses that are more accurate and less time consuming. The availability of these tools has resulted in recognition of distant phylogenetic relationships and tremendous expansion of the information available to TCDB users.																	0305-1048					JAN	2009	37						D274	D278		10.1093/nar/gkn862						WOS:000261906200049	19022853	J	Min, JH; Lee, YC				Min, JH; Lee, YC			Bankruptcy prediction using support vector machine with optimal choice of kernel function parameters	EXPERT SYSTEMS WITH APPLICATIONS												Bankruptcy prediction has drawn a lot of research interests in previous literature, and recent studies have shown that machine learning techniques achieved better performance than traditional statistical ones. This paper applies support vector machines (SVMs) to the bankruptcy prediction problem in an attempt to suggest a new model with better explanatory power and stability. To serve this purpose, we use a grid-search technique using 5-fold cross-validation to find out the optimal parameter values of kernel function of SVM. In addition, to evaluate the prediction accuracy of SVM, we compare its performance with those of multiple discriminant analysis (MDA), logistic regression analysis (Logit), and three-layer fully connected back-propagation neural networks (BPNs). The experiment results show that SVM outperforms the other methods. (c) 2005 Published by Elsevier Ltd.																	0957-4174					MAY	2005	28	4					603	614		10.1016/j.eswa.2004.12.008						WOS:000228124200001		J	Chen, YX; Bi, JB; Wang, JZ				Chen, Yixin; Bi, Jinbo; Wang, James Z.			MILES: Multiple-Instance Learning via Embedded instance Selection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Multiple- instance problems arise from the situations where training class labels are attached to sets of samples (named bags), instead of individual samples within each bag (called instances). Most previous multiple- instance learning (MIL) algorithms are developed based on the assumption that a bag is positive if and only if at least one of its instances is positive. Although the assumption works well in a drug activity prediction problem, it is rather restrictive for other applications, especially those in the computer vision area. We propose a learning method, MILES (Multiple- Instance Learning via Embedded instance Selection), which converts the multiple-instance learning problem to a standard supervised learning problem that does not impose the assumption relating instance labels to bag labels. MILES maps each bag into a feature space defined by the instances in the training bags via an instance similarity measure. This feature mapping often provides a large number of redundant or irrelevant features. Hence, 1-norm SVM is applied to select important features as well as construct classifiers simultaneously. We have performed extensive experiments. In comparison with other methods, MILES demonstrates competitive classification accuracy, high computation efficiency, and robustness to labeling uncertainty.																	0162-8828					DEC	2006	28	12					1931	1947		10.1109/TPAMI.2006.248						WOS:000241195700004	17108368	J	Amaldi, E; Kann, V				Amaldi, E; Kann, V			On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems	THEORETICAL COMPUTER SCIENCE												We investigate the computational complexity of two closely related classes of combinatorial optimization problems for linear systems which arise in various fields such as machine learning, operations research and pattern recognition. In the first class (MIN ULR) one wishes, given a possibly infeasible system of linear relations, to find a solution that violates as few relations as possible while satisfying all the others. In the second class (MIN RVLS) the linear system is supposed to be feasible and one looks for a solution with as few nonzero variables as possible. For both MIN ULR and MIN RVLS the four basic types of relational operators =, greater than or equal to, > and not equal are considered. While MIN RVLS with equations was mentioned to be NP-hard in (Garey and Johnson, 1979), we established in (Amaldi; 1992; Amaldi and Kann, 1995) that MIN ULR with equalities and inequalities are NP-hard even when restricted to homogeneous systems with bipolar coefficients. The latter problems have been shown hard to approximate in (Arora et al., 1993). In this paper we determine strong bounds on the approximability of various variants of MIN RVLS and MIN ULR, including constrained ones where the variables are restricted to take binary values or where some relations are mandatory while others are optional. The various NP-hard versions turn out to have different approximability properties depending on the type of relations and the additional constraints, but none of them can be approximated within any constant factor, unless P = NP. Particular attention is devoted to two interesting special cases that occur in discriminant analysis and machine learning. In particular, we disprove a conjecture of van Horn and Martinet (1992) regarding the existence of a polynomial-time algorithm to design linear classifiers (or perceptrons) that involve a close-to-minimum number of features. (C) 1998 Published by Elsevier Science B.V. All rights reserved.																	0304-3975	1879-2294				DEC 6	1998	209	1-2					237	260		10.1016/S0304-3975(97)00115-1						WOS:000076464000012		J	Gabrilovich, E; Markovitch, S		Veloso, MM		Gabrilovich, Evgeniy; Markovitch, Shaul			Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis	20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE					20th International Joint Conference on Artificial Intelligence	JAN 06-12, 2007	Hyderabad, INDIA					Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. We use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of Wikipedia-based concepts. Assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics (e.g., cosine). Compared with the previous state of the art, using ESA results in substantial improvements in correlation of computed relatedness scores with human judgments: from r = 0.56 to 0.75 for individual words and from r = 0.60 to 0.72 for texts. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.																							2007							1606	1611								WOS:000283721000258		J	Moritz, CT; Perlmutter, SI; Fetz, EE				Moritz, Chet T.; Perlmutter, Steve I.; Fetz, Eberhard E.			Direct control of paralysed muscles by cortical neurons	NATURE												A potential treatment for paralysis resulting from spinal cord injury is to route control signals from the brain around the injury by artificial connections. Such signals could then control electrical stimulation of muscles, thereby restoring volitional movement to paralysed limbs(1-3). In previously separate experiments, activity of motor cortex neurons related to actual or imagined movements has been used to control computer cursors and robotic arms(4-10), and paralysed muscles have been activated by functional electrical stimulation(11-13). Here we show that Macaca nemestrina monkeys can directly control stimulation of muscles using the activity of neurons in the motor cortex, thereby restoring goal-directed movements to a transiently paralysed arm. Moreover, neurons could control functional stimulation equally well regardless of any previous association to movement, a finding that considerably expands the source of control signals for brain- machine interfaces. Monkeys learned to use these artificial connections from cortical cells to muscles to generate bidirectional wrist torques, and controlled multiple neuron - muscle pairs simultaneously. Such direct transforms from cortical activity to muscle stimulation could be implemented by autonomous electronic circuitry, creating a relatively natural neuroprosthesis. These results are the first demonstration that direct artificial connections between cortical cells and muscles can compensate for interrupted physiological pathways and restore volitional control of movement to paralysed limbs.					Perlmutter, Steve/0000-0002-1149-4152												0028-0836					DEC 4	2008	456	7222					639	U63		10.1038/nature07418						WOS:000261340000041	18923392	J	Nguyen, TTT; Armitage, G				Nguyen, Thuy T. T.; Armitage, Grenville			A Survey of Techniques for Internet Traffic Classification using Machine Learning	IEEE COMMUNICATIONS SURVEYS AND TUTORIALS												The research community has begun looking for IP traffic classification techniques that do not rely on 'well known' TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.																	1553-877X						2008	10	4					56	76		10.1109/SURV.2008.080406						WOS:000207971900005		J	Evgeniou, T; Micchelli, CA; Pontil, M				Evgeniou, T; Micchelli, CA; Pontil, M			Learning multiple tasks with kernel methods	JOURNAL OF MACHINE LEARNING RESEARCH												We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.																	1532-4435					APR	2005	6						615	637								WOS:000236329600009		J	Moghaddam, B; Yang, MH				Moghaddam, B; Yang, MH			Learning gender with support faces	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Nonlinear Support Vector Machines (SVMs) are investigated for appearance-based gender classification with low-resolution '"thumbnail" faces processed from 1.755 images from the FERET face database. The performance of SVMs (3.4 percent error) is shown to be superior to traditional pattern classifiers (linear, quadratic, Fisher linear discriminant, nearest-neighbor) as well as more modern techniques such as Radial Basis Function (RBF) classifiers and large ensemble-RBF networks, Furthermore, the difference in classification performance with low-resolution "thumbnails" (21-by-12 pixels) and the corresponding higher resolution images (84-by-48 pixels) was found to be only 1 percent, thus demonstrating robustness and stability with respect to scale and degree of facial detail.																	0162-8828	1939-3539				MAY	2002	24	5					707	711		10.1109/34.1000244						WOS:000175187800011		J	Garcia, S; Fernandez, A; Luengo, J; Herrera, F				Garcia, S.; Fernandez, A.; Luengo, J.; Herrera, F.			A study of statistical techniques and performance measures for genetics-based machine learning: accuracy and interpretability	SOFT COMPUTING												The experimental analysis on the performance of a proposed method is a crucial and necessary task to carry out in a research. This paper is focused on the statistical analysis of the results in the field of genetics-based machine Learning. It presents a study involving a set of techniques which can be used for doing a rigorous comparison among algorithms, in terms of obtaining successful classification models. Two accuracy measures for multi-class problems have been employed: classification rate and Cohen's kappa. Furthermore, two interpretability measures have been employed: size of the rule set and number of antecedents. We have studied whether the samples of results obtained by genetics-based classifiers, using the performance measures cited above, check the necessary conditions for being analysed by means of parametrical tests. The results obtained state that the fulfillment of these conditions are problem-dependent and indefinite, which supports the use of non-parametric statistics in the experimental analysis. In addition, non-parametric tests can be satisfactorily employed for comparing generic classifiers over various data-sets considering any performance measure. According to these facts, we propose the use of the most powerful non-parametric statistical tests to carry out multiple comparisons. However, the statistical analysis conducted on interpretability must be carefully considered.				Herrera, Francisco/C-6856-2008; Fernandez, Alberto/G-3827-2014; Luengo, Julian/L-6569-2014; Garcia, Salvador/N-3624-2013; Luengo, Julian/D-1307-2017	Herrera, Francisco/0000-0002-7283-312X; Luengo, Julian/0000-0003-3952-3629; Garcia, Salvador/0000-0003-4494-7565; Luengo, Julian/0000-0003-3952-3629; Fernandez Hilario, Alberto/0000-0002-6480-8434												1432-7643					AUG	2009	13	10					959	977		10.1007/s00500-008-0392-y						WOS:000264874700004		J	Stamatatos, E				Stamatatos, Efstathios			A Survey of Modern Authorship Attribution Methods	JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY												Authorship attribution supported by statistical or computational methods has a long history starting from the 19th century and is marked by the seminal study of Mosteller and Wallace (1964) on the authorship of the disputed "Federalist Papers." During the last decade, this scientific field has been developed substantially, taking advantage of research advances in areas such as machine learning, information retrieval, and natural language processing. The plethora of available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code, etc.) indicates a wide variety of applications of this technology, provided it is able to handle short and noisy text from multiple candidate authors. In this article, a survey of recent advances of the automated approaches to attributing authorship is presented, examining their characteristics for both text representation and text classification. The focus of this survey is on computational requirements and settings rather than on linguistic or literary issues. We also discuss evaluation methodologies and criteria for authorship attribution studies and list open questions that will attract future work in this area.				Stamatatos, Efstathios/F-2927-2012	Stamatatos, Efstathios/0000-0002-1336-9128												1532-2882					MAR	2009	60	3					538	556		10.1002/asi.21001						WOS:000263935100009		J	Fan, Y; Batmanghelich, N; Clark, CM; Davatzikos, C				Fan, Yong; Batmanghelich, Nematollah; Clark, Chris M.; Davatzikos, Christos		Alzheimers Dis Neuroimaging Initia	Spatial patterns of brain atrophy in MCI patients, identified via high-dimensional pattern classification, predict subsequent cognitive decline	NEUROIMAGE												Spatial patterns of brain atrophy in mild cognitive impairment (MCI) and Alzheimer's disease (AD) were measured via methods of computational neuroanatomy. These patterns were spatially complex and involved many brain regions. In addition to the hippocampus and the medial temporal lobe gray matter, a number of other regions displayed significant atrophy, including orbitofrontal and medial-prefrontal grey matter, cingulate (mainly posterior), insula, uncus, and temporal lobe white matter. Approximately 2/3 of the MCI group presented patterns of atrophy that overlapped with AD, whereas the remaining 1/3 overlapped with cognitively normal individuals, thereby indicating that some, but not all, MCI patients have significant and extensive brain atrophy in this cohort of MCI patients. Importantly, the group with AD-like patterns presented much higher rate of MMSE decline in follow-up visits; conversely, pattern classification provided relatively high classification accuracy (87%) of the individuals that presented relatively higher MMSE decline within a year from baseline. High-dimensional pattern classification, a nonlinear multivariate analysis, provided measures of structural abnormality that can potentially be useful for individual patient classification, as well as for predicting progression and examining multivariate relationships in group analyses. (C) 2007 Elsevier Inc. All rights reserved.				Scharre, Douglas/E-4030-2011	Fan, Yong/0000-0001-9869-4685												1053-8119	1095-9572				FEB 15	2008	39	4					1731	1743		10.1016/j.neuroimage.2007.10.031						WOS:000253241800022	18053747	J	Gutierrez-Osuna, R				Gutierrez-Osuna, Ricardo			Pattern Analysis for Machine Olfaction: A Review	IEEE SENSORS JOURNAL												Pattern analysis constitutes a critical building block in the development of gas sensor array instruments capable of detecting, identifying, and measuring volatile compounds, a technology that has been proposed as an artificial substitute of the human olfactory system. The successful design of a pattern analysis system for machine olfaction requires a careful consideration of the various issues involved in processing multivariate data: signal-preprocessing, feature extraction, feature selection, classification, regression, clustering, and validation. A considerable number of methods from statistical pattern recognition, neural networks, chemometrics, machine learning, and biological cybernetics has been used to process electronic nose data. The objective of this review paper is to provide a summary and guidelines for using the most widely used pattern analysis techniques, as well as to identify research directions that are at the frontier of sensor-based machine olfaction.					Gutierrez-Osuna, Ricardo/0000-0003-2817-2085												1530-437X	1558-1748				JUN	2002	2	3					189	202		10.1109/JSEN.2002.800688						WOS:000208326200008		J	Barbet-Massin, M; Jiguet, F; Albert, CH; Thuiller, W				Barbet-Massin, Morgane; Jiguet, Frederic; Albert, Cecile Helene; Thuiller, Wilfried			Selecting pseudo-absences for species distribution models: how, where and how many?	METHODS IN ECOLOGY AND EVOLUTION												1. Species distribution models are increasingly used to address questions in conservation biology, ecology and evolution. The most effective species distribution models require data on both species presence and the available environmental conditions (known as background or pseudo-absence data) in the area. However, there is still no consensus on how and where to sample these pseudo-absences and how many. 2. In this study, we conducted a comprehensive comparative analysis based on simple simulated species distributions to propose guidelines on how, where and how many pseudo-absences should be generated to build reliable species distribution models. Depending on the quantity and quality of the initial presence data (unbiased vs. climatically or spatially biased), we assessed the relative effect of the method for selecting pseudo-absences (random vs. environmentally or spatially stratified) and their number on the predictive accuracy of seven common modelling techniques (regression, classification and machine-learning techniques). 3. When using regression techniques, the method used to select pseudo-absences had the greatest impact on the model's predictive accuracy. Randomly selected pseudo-absences yielded the most reliable distribution models. Models fitted with a large number of pseudo-absences but equally weighted to the presences (i. e. the weighted sum of presence equals the weighted sum of pseudoabsence) produced the most accurate predicted distributions. For classification and machine-learning techniques, the number of pseudo-absences had the greatest impact on model accuracy, and averaging several runs with fewer pseudo-absences than for regression techniques yielded the most predictive models. 4. Overall, we recommend the use of a large number (e. g. 10 000) of pseudo-absences with equal weighting for presences and absences when using regression techniques (e. g. generalised linear model and generalised additive model); averaging several runs (e. g. 10) with fewer pseudo-absences (e. g. 100) with equal weighting for presences and absences with multiple adaptive regression splines and discriminant analyses; and using the same number of pseudo-absences as available presences (averaging several runs if few pseudo-absences) for classification techniques such as boosted regression trees, classification trees and random forest. In addition, we recommend the random selection of pseudo-absences when using regression techniques and the random selection of geographically and environmentally stratified pseudo-absences when using classification and machine-learning techniques.				THUILLER, Wilfried/G-3283-2010; albert, cecile/H-5804-2012	THUILLER, Wilfried/0000-0002-5388-5274; 												2041-210X					APR	2012	3	2					327	338		10.1111/j.2041-210X.2011.00172.x						WOS:000302538500013		J	Friston, KJ; Mattout, J; Trujillo-Barreto, N; Ashburner, J; Penny, W				Friston, Karl J.; Mattout, Jeremie; Trujillo-Barreto, Nelson; Ashburner, John; Penny, Will			Variational free energy and the Laplace approximation	NEUROIMAGE												This note derives the variational free energy under the Laplace approximation, with a focus on accounting for additional model complexity induced by increasing the number of model parameters. This is relevant when using the free energy as an approximation to the log-evidence in Bayesian model averaging and selection. By setting restricted maximum likelihood (ReML) in the larger context of variational learning and expectation maximisation (EM), we show how the ReML objective function can be adjusted to provide an approximation to the log-evidence for a particular model. This means ReML can be used for model selection, specifically to select or compare models with different covariance components. This is useful in the context of hierarchical models because it enables a principled selection of priors that, under simple hyperpriors, can be used for automatic model selection and relevance determination (ARD). Deriving the ReML objective function, from basic variational principles, discloses the simple relationships among Variational Bayes, EM and ReML. Furthermore, we show that EM is formally identical to a full variational treatment when the precisions are linear in the hyperparameters. Finally, we also consider, briefly, dynamic models and how these inform the regularisation of free energy ascent schemes, like EM and ReML. (c) 2006 Elsevier Inc. All rights reserved.				Friston, Karl/D-9230-2011; Trujillo-Barreto, Nelson/G-3764-2013; Trujillo-Barreto, Nelson/I-9402-2014; Ashburner, John/I-3757-2013	Friston, Karl/0000-0001-7984-8909; Trujillo-Barreto, Nelson/0000-0001-6581-7503; Trujillo-Barreto, Nelson/0000-0001-6581-7503; Ashburner, John/0000-0001-7605-2518; Penny, William/0000-0001-9064-1191												1053-8119	1095-9572				JAN 1	2007	34	1					220	234		10.1016/j.neuroimage.2006.08.035						WOS:000242735300022	17055746	J	Allwein, EL; Schapire, RE; Singer, Y				Allwein, EL; Schapire, RE; Singer, Y			Reducing multiclass to binary: A unifying approach for margin classifiers	JOURNAL OF MACHINE LEARNING RESEARCH												We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.																	1532-4435					SPR	2001	1	2					113	141		10.1162/15324430152733133						WOS:000173336800002		J	Orru, G; Pettersson-Yeo, W; Marquand, AF; Sartori, G; Mechelli, A				Orru, Graziella; Pettersson-Yeo, William; Marquand, Andre F.; Sartori, Giuseppe; Mechelli, Andrea			Using Support Vector Machine to identify imaging biomarkers of neurological and psychiatric disease: A critical review	NEUROSCIENCE AND BIOBEHAVIORAL REVIEWS												Standard univariate analysis of neuroimaging data has revealed a host of neuroanatomical and functional differences between healthy individuals and patients suffering a wide range of neurological and psychiatric disorders. Significant only at group level however these findings have had limited clinical translation, and recent attention has turned toward alternative forms of analysis, including Support-Vector-Machine (SVM). A type of machine learning, SVM allows categorisation of an individual's previously unseen data into a predefined group using a classification algorithm, developed on a training data set. In recent years, SVM has been successfully applied in the context of disease diagnosis, transition prediction and treatment prognosis, using both structural and functional neuroimaging data. Here we provide a brief overview of the method and review those studies that applied it to the investigation of Alzheimer's disease, schizophrenia, major depression, bipolar disorder, presymptomatic Huntington's disease, Parkinson's disease and autistic spectrum disorder. We conclude by discussing the main theoretical and practical challenges associated with the implementation of this method into the clinic and possible future directions. (C) 2012 Elsevier Ltd. All rights reserved.				Marquand, Andre/B-6050-2012	Marquand, Andre/0000-0001-5903-203X; Sartori, Giuseppe/0000-0002-6026-0133												0149-7634	1873-7528				APR	2012	36	4					1140	1152		10.1016/j.neubiorev.2012.01.004						WOS:000302970900004		J	Lafon, S; Lee, AB				Lafon, Stephane; Lee, Ann B.			Diffusion maps and coarse-graining: A unified framework for dimensionality reduction, graph partitioning, and data set parameterization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												We provide evidence that nonlinear dimensionality reduction, clustering, and data set parameterization can be solved within one and the same framework. The main idea is to define a system of coordinates with an explicit metric that reflects the connectivity of a given data set and that is robust to noise. Our construction, which is based on a Markov random walk on the data, offers a general scheme of simultaneously reorganizing and subsampling graphs and arbitrarily shaped data sets in high dimensions using intrinsic geometry. We show that clustering in embedding spaces is equivalent to compressing operators. The objective of data partitioning and clustering is to coarse-grain the random walk on the data while at the same time preserving a diffusion operator for the intrinsic geometry or connectivity of the data set up to some accuracy. We show that the quantization distortion in diffusion space bounds the error of compression of the operator, thus giving a rigorous justification fork-means clustering in diffusion space and a precise measure of the performance of general clustering algorithms.																	0162-8828					SEP	2006	28	9					1393	1403		10.1109/TPAMI.2006.184						WOS:000238950800004	16929727	J	Millan, JD; Renkens, F; Mourino, J; Gerstner, W				Millan, JD; Renkens, F; Mourino, J; Gerstner, W			Noninvasive brain-actuated control of a mobile robot by human EEG	IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING												Brain activity recorded noninvasively is sufficient to control a mobile robot if advanced robotics is used in combination with asynchronous electroencephalogram (EEG) analysis and machine learning techniques. Until now brain-actuated control has mainly relied on implanted electrodes, since EEG-based systems have been considered too slow for controlling rapid and complex sequences of movements. We show that two human subjects successfully moved a robot between several rooms by mental control only, using an EEG-based brain-machine interface that recognized three mental states. Mental control was comparable to manual control on the same task with a performance ratio of 0.74.				Millan, Jose del R./F-1696-2011	Millan, Jose del R./0000-0001-5819-1522												0018-9294					JUN	2004	51	6					1026	1033		10.1109/TBME.2004.827086						WOS:000221578000022		J	Zhang, ML; Zhou, ZH				Zhang, Min-Ling; Zhou, Zhi-Hua			Multilabel neural networks with applications to functional genomics and text categorization	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												In multilabel learning, each instance in the training set is associated with a set of labels and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, this problem is addressed in the way that a neural network algorithm named BP-MLL, i.e., Backpropagation for Multilabel Learning, is proposed. It is derived from the popular Backpropogation algorithm through employing a novel error function capturing the characteristics of multilabel learning, i.e., the labels belonging to an instance should be ranked higher than those not belonging to that instance. Applications to two real-world multilabel learning problems, i.e., functional genomics and text categorization, show that the performance of BP-MLL is superior to that of some well-established multilabel learning algorithms.																	1041-4347	1558-2191				OCT	2006	18	10					1338	1351		10.1109/TKDE.2006.162						WOS:000239814600004		J	Bartlett, PL; Jordan, MI; McAuliffe, JD				Bartlett, PL; Jordan, MI; McAuliffe, JD			Convexity, classification, and risk bounds	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION												Many of the classification algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. The convexity makes these algorithms computationally efficient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function-that it satisfies a pointwise form of Fisher consistency for classification. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise. and show that in this case, strictly convex loss functions lead to faster rates of convergence of the risk than would be implied by standard uniform convergence arguments. Finally, we present applications of our results to the estimation of convergence rates in function classes that are scaled convex hulls of a finite-dimensional base class, with a variety of commonly used loss functions.					Bartlett, Peter/0000-0002-8760-3140												0162-1459	1537-274X				MAR	2006	101	473					138	156		10.1198/016214505000000907						WOS:000235958400017		J	Martinez-Meyer, E; Townsend Peterson, A; Hargrove, WW				Martinez-Meyer, E; Townsend Peterson, A; Hargrove, WW			Ecological niches as stable distributional constraints on mammal species, with implications for Pleistocene extinctions and climate change projections for biodiversity	GLOBAL ECOLOGY AND BIOGEOGRAPHY												Aim Theoretical work suggests that species' ecological niches should remain relatively constant over long-term ecological time periods, but empirical tests are few. We present longitudinal studies of 23 extant mammal species, modelling ecological niches and predicting geographical distributions reciprocally between the Last Glacial Maximum and present to test this evolutionary conservatism. Location This study covered distributional shifts in mammal species across the lower 48 states of the United States. Methods We used a machine-learning tool for modelling species' ecological niches, based on known occurrences and electronic maps summarizing ecological dimensions, to assess the ability of ecological niches as modelled in one time period to predict the geographical distribution of the species in another period, and vice versa. Results High intertemporal predictivity between niche models and species' occurrences indicate that niche conservatism is widespread among the taxa studied, particularly when statistical power is considered as a reason for failure of reciprocal predictions. Niche projections to the present for 8 mammal taxa that became extinct at the end of the Pleistocene generally increased in area, and thus do not support the hypothesis of niche collapse as a major driving force in their extinction. Main conclusions Ecological niches represent long-term stable constraints on the distributional potential of species; indeed, this study suggests that mammal species have tracked consistent climate profiles throughout the drastic climate change events that marked the end of the Pleistocene glaciations. Many current modelling efforts focusing on anticipating climate change effects on species' potential geographical distributions will be bolstered by this result - in essence, the first longitudinal demonstration of niche conservatism.				Martinez-Meyer, Enrique/B-1464-2008	Martinez-Meyer, Enrique/0000-0003-1184-9264; Peterson, A. Townsend/0000-0003-0243-2379												0960-7447					JUL	2004	13	4					305	314		10.1111/j.1466-822X.2004.00107.x						WOS:000222175600003		J	Wager, TD; Atlas, LY; Lindquist, MA; Roy, M; Woo, CW; Kross, E				Wager, Tor D.; Atlas, Lauren Y.; Lindquist, Martin A.; Roy, Mathieu; Woo, Choong-Wan; Kross, Ethan			An fMRI-Based Neurologic Signature of Physical Pain	NEW ENGLAND JOURNAL OF MEDICINE												BACKGROUND Persistent pain is measured by means of self-report, the sole reliance on which hampers diagnosis and treatment. Functional magnetic resonance imaging (fMRI) holds promise for identifying objective measures of pain, but brain measures that are sensitive and specific to physical pain have not yet been identified. METHODS In four studies involving a total of 114 participants, we developed an fMRI-based measure that predicts pain intensity at the level of the individual person. In study 1, we used machine-learning analyses to identify a pattern of fMRI activity across brain regions - a neurologic signature - that was associated with heat-induced pain. The pattern included the thalamus, the posterior and anterior insulae, the secondary somatosensory cortex, the anterior cingulate cortex, the periaqueductal gray matter, and other regions. In study 2, we tested the sensitivity and specificity of the signature to pain versus warmth in a new sample. In study 3, we assessed specificity relative to social pain, which activates many of the same brain regions as physical pain. In study 4, we assessed the responsiveness of the measure to the analgesic agent remifentanil. RESULTS In study 1, the neurologic signature showed sensitivity and specificity of 94% or more (95% confidence interval [CI], 89 to 98) in discriminating painful heat from nonpainful warmth, pain anticipation, and pain recall. In study 2, the signature discriminated between painful heat and nonpainful warmth with 93% sensitivity and specificity (95% CI, 84 to 100). In study 3, it discriminated between physical pain and social pain with 85% sensitivity (95% CI, 76 to 94) and 73% specificity (95% CI, 61 to 84) and with 95% sensitivity and specificity in a forced-choice test of which of two conditions was more painful. In study 4, the strength of the signature response was substantially reduced when remifentanil was administered. CONCLUSIONS It is possible to use fMRI to assess pain elicited by noxious heat in healthy persons. Future studies are needed to assess whether the signature predicts clinical pain. (Funded by the National Institute on Drug Abuse and others.)				Roy, Mathieu/H-1963-2017	, Ethan/0000-0002-8664-2711												0028-4793					APR 11	2013	368	15					1388	1397		10.1056/NEJMoa1204471						WOS:000317333600006	23574118	J	Bosco, C; Colli, R; Introini, E; Cardinale, M; Tsarpela, O; Madella, A; Tihanyi, J; Viru, A				Bosco, C; Colli, R; Introini, E; Cardinale, M; Tsarpela, O; Madella, A; Tihanyi, J; Viru, A			Adaptive responses of human skeletal muscle to vibration exposure	CLINICAL PHYSIOLOGY												The aim of this study was to investigate the effects of whole-body vibrations (WBV) on the mechanical behaviour of human skeletal muscle. For this purpose, six female volleyball players at national level were recruited voluntarily. They were tested with maximal dynamic leg press exercise on a slide machine with extra loads of 70, 90, 110 and 130 kg. After the testing, one leg was randomly assigned to the control treatment (C) and the other to the experimental treatment (E) consisting of vibrations. The subjects were then retested at the end of the treatment using the leg press. Results showed remarkable and statistically significant enhancement of the experimental treatment in average velocity (AV), average force (AF) and average power (AP) (P<0.05-0.005). Consequently, the velocity-force and power-force relationship shifted to the right after the treatment. In conclusion, it was affirmed that the enhancement could be caused by neural factors, as athletes were well accustomed to the leg press exercise and the learning effect was minimized.				Cardinale, Marco/B-9235-2009	Cardinale, Marco/0000-0002-2777-8707												0144-5979					MAR	1999	19	2					183	187								WOS:000083616500012	10200901	J	Yang, Q; Wu, XD				Yang, Qiang; Wu, Xindong			10 Challenging problems in data mining research	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING					278th Xiangshan Sciences Forum on Frontier Studies on Data Technology and Knowledge Economy	MAY 22-24, 2006	Beijing, PEOPLES R CHINA	Comm Xiangshan Sci, Western Union Financial Serv				In October 2005, we took an initiative to identify 10 challenging problems in data mining research, by consulting some of the most active researchers in data mining and machine learning for their opinions on what are considered important and worthy topics for future research in data mining. We hope their insights will inspire new research efforts, and give young researchers (including PhD students) a high-level guideline as to where the hot problems are located in data mining. Due to the limited amount of time, we were only able to send out our survey requests to the organizers of the IEEE ICDM and ACM KDD conferences, and we received an overwhelming response. We are very grateful for the contributions provided by these researchers despite their busy schedules. This short article serves to summarize the 10 most challenging problems of the 14 responses we have received from this survey. The order of the listing does not reflect their level of importance.					Yang, Qiang/0000-0001-5059-8360												0219-6220					DEC	2006	5	4					597	604		10.1142/S0219622006002258						WOS:000243318400003		J	Liu, XY; Wu, JX; Zhou, ZH				Liu, Xu-Ying; Wu, Jianxin; Zhou, Zhi-Hua			Exploratory Undersampling for Class-Imbalance Learning	IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART B-CYBERNETICS												Undersampling is a popular method in dealing with class-imbalance problems, which uses only a subset of the majority class and thus is very efficient. The main deficiency is that many majority class examples are ignored. We propose two algorithms to overcome this deficiency. EasyEnsemble samples several subsets from the majority class, trains a learner using each of them, and combines the outputs of those learners. BalanceCascade trains the learners sequentially, where in each step, the majority class examples that are correctly classified by the current trained learners are removed from further consideration. Experimental results show that both methods have higher Area Under the ROC Curve, F-measure, and G-mean values than many existing class-imbalance learning methods. Moreover, they have approximately the same training time as that of undersampling when the same number of weak classifiers is used, which is significantly faster than other methods.				Wu, Jianxin/A-3700-2011; Wu, Jianxin/B-8539-2012													1083-4419					APR	2009	39	2					539	550		10.1109/TSMCB.2008.2007853						WOS:000264630500020	19095540	J	Leslie, CS; Eskin, E; Cohen, A; Weston, J; Noble, WS				Leslie, CS; Eskin, E; Cohen, A; Weston, J; Noble, WS			Mismatch string kernels for discriminative protein classification	BIOINFORMATICS												Motivation: Classification of proteins sequences into functional and structural families based on sequence homology is a central problem in computational biology. Discriminative supervised machine learning approaches provide good performance, but simplicity and computational efficiency of training and prediction are also important concerns. Results: We introduce a class of string kernels, called mismatch kernels, for use with support vector machines (SVMs) in a discriminative approach to the problem of protein classification and remote homology detection. These kernels measure sequence similarity based on shared occurrences of fixed-length patterns in the data, allowing for mutations between patterns. Thus, the kernels provide a biologically well-motivated way to compare protein sequences without relying on family-based generative models such as hidden Markov models. We compute the kernels efficiently using a mismatch tree data structure, allowing us to calculate the contributions of all patterns occurring in the data in one pass while traversing the tree. When used with an SVM, the kernels enable fast prediction on test sequences. We report experiments on two benchmark SCOP datasets, where we show that the mismatch kernel used with an SVM classifier performs competitively with state-of-the-art methods for homology detection, particularly when very few training examples are available. Examination of the highest-weighted patterns learned by the SVM classifier recovers biologically important motifs in protein families and superfamilies.				Eskin, Eleazar/J-9187-2012	Eskin, Eleazar/0000-0003-1149-4758												1367-4803					MAR 1	2004	20	4					467	476		10.1093/bioinformatics/btg431						WOS:000220058800005	14990442	S	Akbani, R; Kwek, S; Japkowicz, N		Boulicaut, JF; Esposito, F; Giannoti, F; Pedreschi, D		Akbani, R; Kwek, S; Japkowicz, N			Applying support vector machines to imbalanced datasets	MACHINE LEARNING: ECML 2004, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE				15th European Conference on Machine Learning/8th European Conference on Principles and Practice of Knowledge Discovery in Databases	SEP 20-24, 2004	Pisa, ITALY	KDNet, Pascal Network, Kluwer & Mach Learning Journal, Springer, Municipal Pisa, Microsoft Res, COOP, Exeura, INSA-Lyon, ISTI-Cnr, Univ Pisa, Univ Bari, Reg Toscana				Support Vector Machines (SVM) have been extensively studied and have shown remarkable success in many applications. However the success of SVM is very limited when it is applied to the problem of learning from imbalanced datasets in which negative instances heavily outnumber the positive instances (e.g. in gene profiling and detecting credit card fraud). This paper discusses the factors behind this failure and explains why the common strategy of undersampling the training data may not be the best choice for SVM. We then propose an algorithm for overcoming these problems which is based on a variant of the SMOTE algorithm by Chawla et al, combined with Veropoulos et al's different error costs algorithm. We compare the performance of our algorithm against these two algorithms, along with undersampling and regular SVM and show that our algorithm outperforms all of them.																	0302-9743		3-540-23105-6				2004	3201						39	50								WOS:000223999500007		J	Bolton, RJ; Hand, DJ				Bolton, RJ; Hand, DJ			Statistical fraud detection: A review	STATISTICAL SCIENCE												Fraud is increasing dramatically with the expansion of modem technology and the global superhighways of communication, resulting in the loss of billions of dollars worldwide each year. Although prevention technologies are the best way to reduce fraud, fraudsters are adaptive and, given time, will usually find ways to circumvent such measures. Methodologies for the detection of fraud are essential if we are to catch fraudsters once fraud prevention has failed. Statistics and machine learning provide effective technologies for fraud detection and have been applied successfully to detect activities such as money laundering, e-commerce credit card fraud, telecommunications fraud and computer intrusion, to name but a few. We describe the tools available for statistical fraud detection and the areas in which fraud detection technologies are most used.																	0883-4237					AUG	2002	17	3					235	249								WOS:000180787700001		J	Hu, QH; Yu, DR; Liu, JF; Wu, CX				Hu, Qinghua; Yu, Daren; Liu, Jinfu; Wu, Congxin			Neighborhood rough set based heterogeneous feature subset selection	INFORMATION SCIENCES												Feature subset selection is viewed as an important preprocessing step for pattern recognition, machine learning and data mining. Most of researches are focused on dealing with homogeneous feature selection, namely, numerical or categorical features. In this paper, we introduce a neighborhood rough set model to deal with the problem of heterogeneous feature subset selection. As the classical rough set model can just be used to evaluate categorical features, we generalize this model with neighborhood relations and introduce a neighborhood rough set model. The proposed model will degrade to the classical one if we specify the size of neighborhood zero. The neighborhood model is used to reduce numerical and categorical features by assigning different thresholds for different kinds of attributes. In this model the sizes of the neighborhood lower and upper approximations of decisions reflect the discriminating capability of feature subsets. The size of lower approximation is computed as the dependency between decision and condition attributes. We use the neighborhood dependency to evaluate the significance of a subset of heterogeneous features and construct forward feature subset selection algorithms. The proposed algorithms are compared with some classical techniques. Experimental results show that the neighborhood model based method is more flexible to deal with heterogeneous data. (C) 2008 Elsevier Inc. All rights reserved.																	0020-0255					SEP 15	2008	178	18					3577	3594		10.1016/j.ins.2008.05.024						WOS:000258349800007		J	Oudeyer, PY; Kaplan, F; Hafner, VV				Oudeyer, Pierre-Yves; Kaplan, Frederic; Hafner, Verena V.			Intrinsic motivation systems for autonomous mental development	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION												Exploratory activities seem to be intrinsically rewarding for children and crucial for their cognitive development. Can a machine be endowed with such an intrinsic motivation system? This is the question we study in this paper, presenting a number of computational systems that try to capture this drive towards novel or curious situations. After discussing related research coming from developmental psychology, neuroscience, developmental robotics, and active learning, this paper presents the mechanism of Intelligent Adaptive Curiosity, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress. This drive makes the robot focus on situations which are neither too predictable nor too unpredictable, thus permitting autonomous mental development. The complexity of the robot's activities autonomously increases and complex developmental sequences self-organize without being constructed in a supervised manner. Two experiments are presented illustrating the stage-like organization emerging with this mechanism. In one of them, a physical robot is placed on a baby play mat with objects that it can learn to manipulate. Experimental results show that the robot first spends time in situations which are easy to learn, then shifts its attention progressively to situations of increasing difficulty, avoiding situations in which nothing can be learned. Finally, these various results are discussed in relation to more complex forms of behavioral organization and data coming from developmental psychology.				Hafner, Verena/D-6186-2014	Hafner, Verena/0000-0002-9125-8466; Kaplan, Frederic/0000-0002-6991-5730												1089-778X					APR	2007	11	2					265	286		10.1109/TEVC.2006.890271						WOS:000245518500009		J	MANGASARIAN, OL; STREET, WN; WOLBERG, WH				MANGASARIAN, OL; STREET, WN; WOLBERG, WH			BREAST-CANCER DIAGNOSIS AND PROGNOSIS VIA LINEAR-PROGRAMMING	OPERATIONS RESEARCH												Two medical applications of linear programming are described in this paper. Specifically, linear programming-based machine learning techniques are used to increase the accuracy and objectivity of breast cancer diagnosis and prognosis. The first application to breast cancer diagnosis utilizes characteristics of individual cells, obtained from a minimally invasive fine needle aspirate, to discriminate benign from malignant breast lumps. This allows an accurate diagnosis without the need for a surgical biopsy. The diagnostic system in current operation at University of Wisconsin Hospitals was trained on samples from 569 patients and has had 100% chronological correctness in diagnosing 131 subsequent patients. The second application, recently put into clinical practice, is a method that constructs a surface that predicts when breast cancer is likely to recur in patients that have had their cancers excised. This gives the physician and the patient better information with which to plan treatment, and may eliminate the need for a prognostic surgical procedure. The novel feature of the predictive approach is the ability to handle cases for which cancer has not recurred (censored data) as well as cases for which cancer has recurred at a specific time. The prognostic system has an expected error of 13.9 to 18.3 months, which is better than prognosis correctness by other available techniques.																	0030-364X					JUL-AUG	1995	43	4					570	577		10.1287/opre.43.4.570						WOS:A1995RU56900003		J	Feng, GR; Huang, GB; Lin, QP; Gay, R				Feng, Guorui; Huang, Guang-Bin; Lin, Qingping; Gay, Robert			Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS												One of the open problems in neural network research is how to automatically determine network architectures for given applications. In this brief, we propose a simple and efficient approach to automatically determine the number of hidden nodes in generalized single-hidden-layer feedforward networks (SLFNs) which need not be neural alike. This approach referred to as error minimized extreme learning machine (EM-ELM) can add random hidden nodes to SLFNs one by one or group by group (with varying group size). During the growth of the networks, the output weights are updated incrementally. The convergence of this approach is proved in this brief as well. Simulation results demonstrate and verify that our new approach is much faster than other sequential/incremental/growing algorithms with good generalization performance.				Huang, Guang-Bin/A-5035-2011; Feng, Guorui/E-6895-2012	Huang, Guang-Bin/0000-0002-2480-4965; 												1045-9227					AUG	2009	20	8					1352	1357		10.1109/TNN.2009.2024147						WOS:000268756800012	19596632	J	Rodriguez-Galiano, VF; Ghimire, B; Rogan, J; Chica-Olmo, M; Rigol-Sanchez, JP				Rodriguez-Galiano, V. F.; Ghimire, B.; Rogan, J.; Chica-Olmo, M.; Rigol-Sanchez, J. P.			An assessment of the effectiveness of a random forest classifier for land-cover classification	ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING												Land cover monitoring using remotely sensed data requires robust classification methods which allow for the accurate mapping of complex land cover and land use categories. Random forest (RF) is a powerful machine learning classifier that is relatively unknown in land remote sensing and has not been evaluated thoroughly by the remote sensing community compared to more conventional pattern recognition techniques. Key advantages of RF include: their non-parametric nature; high classification accuracy; and capability to determine variable importance. However, the split rules for classification are unknown, therefore RF can be considered to be black box type classifier. RF provides an algorithm for estimating missing values; and flexibility to perform several types of data analysis, including regression, classification, survival analysis, and unsupervised learning. In this paper, the performance of the RF classifier for land cover classification of a complex area is explored. Evaluation was based on several criteria: mapping accuracy, sensitivity to data set size and noise. Landsat-5 Thematic Mapper data captured in European spring and summer were used with auxiliary variables derived from a digital terrain model to classify 14 different land categories in the south of Spain. Results show that the RF algorithm yields accurate land cover classifications, with 92% overall accuracy and a Kappa index of 0.92. RF is robust to training data reduction and noise because significant differences in kappa values were only observed for data reduction and noise addition values greater than 50 and 20%, respectively. Additionally, variables that RF identified as most important for classifying land cover coincided with expectations. A McNemar test indicates an overall better performance of the random forest model over a single decision tree at the 0.00001 significance level. (C) 2011 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS) Published by Elsevier B.V. All rights reserved.				Ghimire, Bardan/N-9871-2013; Rigol-Sanchez, Juan P./F-7832-2011	Rigol-Sanchez, Juan P./0000-0002-8003-4184; Rodriguez-Galiano, Victor F./0000-0002-5422-8305												0924-2716					JAN	2012	67						93	104		10.1016/j.isprsjprs.2011.11.002						WOS:000300749900010		J	Zhang, CH; Huang, J				Zhang, Cun-Hui; Huang, Jian			The sparsity and bias of the lasso selection in high-dimensional linear regression	ANNALS OF STATISTICS					Workshop on Qualitative Assumptions and Regularization for High-Dimensional Data	2006	Oberwolfach, GERMANY					Meinshausen and Buhlmann [Ann. Statist. 34 (2006) 1436-1462] showed that, for neighborhood selection in Gaussian graphical models, under a neighborhood stability condition, the LASSO is consistent, even when the number of variables is of greater order than the sample size. Zhao and Yu [(2006) J. Machine Learning Research 7 2541-2567] formalized the neighborhood stability condition in the context of linear regression as a strong irrepresentable condition. That paper showed that under this condition, the LASSO selects exactly the set of nonzero regression coefficients, provided that these coefficients are bounded away from zero at a certain rate. In this paper, the regression coefficients outside an ideal model are assumed to be small, but not necessarily zero. Under a sparse Riesz condition on the correlation of design variables, we prove that the LASSO selects a model of the correct order of dimensionality, controls the bias of the selected model at a level determined by the contributions of small regression coefficients and threshold bias, and selects all coefficients of greater order than the bias of the selected model. Moreover, as a consequence of this rate consistency of the LASSO in model selection, it is proved that the sum of error squares for the mean response and the l(alpha)-loss for the regression coefficients converge at the best possible rates under the given conditions. An interesting aspect of our results is that the logarithm of the number of variables can be of the same order as the sample size for certain random dependent designs.																	0090-5364					AUG	2008	36	4					1567	1594		10.1214/07-AOS520						WOS:000258243000006		J	Capriotti, E; Calabrese, R; Casadio, R				Capriotti, E.; Calabrese, R.; Casadio, R.			Predicting the insurgence of human genetic diseases associated to single point protein mutations with support vector machines and evolutionary information	BIOINFORMATICS												Motivation: Human single nucleotide polymorphisms (SNPs) are the most frequent type of genetic variation in human population. One of the most important goals of SNP projects is to understand which human genotype variations are related to Mendelian and complex diseases. Great interest is focused on non-synonymous coding SNPs (nsSNPs) that are responsible of protein single point mutation. nsSNPs can be neutral or disease associated. It is known that the mutation of only one residue in a protein sequence can be related to a number of pathological conditions of dramatic social impact such as Altzheimer's, Parkinson's and Creutzfeldt-Jakob's diseases. The quality and completeness of presently available SNPs databases allows the application of machine learning techniques to predict the insurgence of human diseases due to single point protein mutation starting from the protein sequence. Results: In this paper, we develop a method based on support vector machines (SVMs) that starting from the protein sequence information can predict whether a new phenotype derived from a nsSNP can be related to a genetic disease in humans. Using a dataset of 21185 single point mutations, 61% of which are disease-related, out of 3587 proteins, we show that our predictor can reach more than 74% accuracy in the specific task of predicting whether a single point mutation can be disease related or not. Our method, although based on less information, outperforms other web-available predictors implementing different approaches. Availability: A beta version of the web tool is available at http://gpcr.biocomp.unibo.it/cgi/predictors/PhD-SNP/PhD-SNP.cgi Contact: casadio@alma.unibo.it.				Capriotti, Emidio/D-9318-2011; Casadio, Rita/K-4814-2015	Capriotti, Emidio/0000-0002-2323-0963; Casadio, Rita/0000-0002-7462-7039												1367-4803					NOV 15	2006	22	22					2729	2734		10.1093/bioinformatics/btl423						WOS:000241958000004	16895930	J	Zhou, ZH; Li, M				Zhou, ZH; Li, M			Tri-training: Exploiting unlabeled data using three classifiers	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												In many practical data mining applications, such as Web page classification, unlabeled training examples are readily available, but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. In this paper, a new co-training style semi-supervised learning algorithm, named tri-training, is proposed. This algorithm generates three classifiers from the original labeled example set. These classifiers are then refined using unlabeled examples in the tri-training process. In detail, in each round of tri-training, an unlabeled example is labeled for a classifier if the other two classifiers agree on the labeling, under certain conditions. Since tri-training neither requires the instance space to be described with sufficient and redundant views nor does it put any constraints on the supervised learning algorithm, its applicability is broader than that of previous co-training style algorithms. Experiments on UCl data sets and application to the Web page classification task indicate that tri-training can effectively exploit unlabeled data to enhance the learning performance.																	1041-4347	1558-2191				NOV	2005	17	11					1529	1541		10.1109/TKDE.2005.186						WOS:000231891300007		J	Bruzzone, L; Chi, MM; Marconcini, M				Bruzzone, Lorenzo; Chi, Mingmin; Marconcini, Mattia			A novel transductive SVM for semisupervised classification of remote-sensing images	IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING												This paper introduces a semisupervised classification method that exploits both labeled and unlabeled samples for addressing ill-posed problems with support vector machines (SVMs). The method is based on recent developments in statistical learning theory concerning transductive inference and in particular transductive SVMs (TSVMs). TSVMs exploit specific iterative algorithms which gradually search a reliable separating hyperplane (in the kernel space) with a transductive process that incorporates both labeled and unlabeled samples in the training phase. Based on an analysis of the properties of the TSVMs presented in the literature, a novel modified TSVM classifier designed for addressing ill-posed remote-sensing problems is proposed. In particular, the proposed technique: 1) is based on a novel transductive procedure that exploits a weighting strategy for unlabeled. patterns, based on a time-dependent criterion; 2) is able to mitigate the effects of suboptimal model selection (which is unavoidable in the presence of small-size training sets); and 3) can address multiclass, cases. Experimental results confirm the effectiveness of the proposed method on a set of ill-posed remote-sensing classification problems representing different operative conditions.				Bruzzone, Lorenzo/A-2076-2012	Bruzzone, Lorenzo/0000-0002-6036-459X												0196-2892	1558-0644				NOV	2006	44	11	2				3363	3373		10.1109/TGRS.2006.877950						WOS:000241933600015		J	Baldi, P; Brunak, S; Frasconi, P; Soda, G; Pollastri, G				Baldi, P; Brunak, S; Frasconi, P; Soda, G; Pollastri, G			Exploiting the past and the future in protein secondary structure prediction	BIOINFORMATICS					2nd Georgia Tech International Conference on Bioinformatics, in Silicon Biology, on Sequence, Structure and Function	NOV 11-14, 1999	ATLANTA, GA					Motivation: Predicting the secondary structure of a protein (alpha-helix, beta-sheet, coil) is an important step towards elucidating its three-dimensional structure, as well as its function. Presently, the best predictors are based on machine learning approaches, in particular neural network architectures with a fixed and relatively short, input window of amino acids, centered at the prediction site. Although a fixed small window avoids overfitting problems, it does not permit capturing variable long-rang information. Results: We introduce a family of novel architectures which can learn to make predictions based on variable ranges of dependencies. These architectures extend recurrent neural networks, introducing non-causal bidirectional dynamics to capture both upstream and downstream information. The prediction algorithm is completed by the use of mixtures of estimators that leverage evolutionary information, expressed in terms of multiple alignments, both at the input and output levels. While our system currently achieves an overall performance close to 76% correct prediction - at least comparable to the best existing systems - the main emphasis here is on the development of new algorithmic ideas. Availability: The executable program for predicting protein secondary structure is available from the authors free of charge. Contact: pfbaldi@ics.uci.edu, gpollast@ics.uci.edu, brunak@cbs.dtu.dk, paolo@dsi.unifi.it.				Frasconi, Paolo/G-2944-2010; Exarchos, Konstantinos/B-5978-2008													1367-4803	1460-2059				NOV	1999	15	11					937	946		10.1093/bioinformatics/15.11.937						WOS:000085533100009	10743560	J	Qian, YH; Liang, JY; Pedrycz, W; Dang, CY				Qian, Yuhua; Liang, Jiye; Pedrycz, Witold; Dang, Chuangyin			Positive approximation: An accelerator for attribute reduction in rough set theory	ARTIFICIAL INTELLIGENCE												Feature selection is a challenging problem in areas such as pattern recognition, machine learning and data mining. Considering a consistency measure introduced in rough set theory, the problem of feature selection, also called attribute reduction, aims to retain the discriminatory power of original features. Many heuristic attribute reduction algorithms have been proposed however, quite often, these methods are computationally time-consuming. To overcome this shortcoming, we introduce a theoretic framework based on rough set theory, called positive approximation, which can be used to accelerate a heuristic process of attribute reduction. Based on the proposed accelerator, a general attribute reduction algorithm is designed. Through the use of the accelerator, several representative heuristic attribute reduction algorithms in rough set theory have been enhanced. Note that each of the modified algorithms can choose the same attribute reduct as its original version, and hence possesses the same classification accuracy. Experiments show that these modified algorithms outperform their original counterparts. It is worth noting that the performance of the modified algorithms becomes more visible when dealing with larger data sets. (C) 2010 Elsevier B.V. All rights reserved.				Liang, Jiye/G-6810-2011; Dang, Chuangyin/F-5964-2012	Dang, Chuangyin/0000-0003-4731-4616												0004-3702					JUN	2010	174	9-10					597	618		10.1016/j.artint.2010.04.018						WOS:000278219200004		J	Saxena, A; Sun, M; Ng, AY				Saxena, Ashutosh; Sun, Min; Ng, Andrew Y.			Make3D: Learning 3D Scene Structure from a Single Still Image	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												We consider the problem of estimating detailed 3D structure from a single still image of an unstructured environment. Our goal is to create 3D models that are both quantitatively accurate as well as visually pleasing. For each small homogeneous patch in the image, we use a Markov Random Field (MRF) to infer a set of "plane parameters" that capture both the 3D location and 3D orientation of the patch. The MRF, trained via supervised learning, models both image depth cues as well as the relationships between different parts of the image. Other than assuming that the environment is made up of a number of small planes, our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 3D structure than does prior art and also give a much richer experience in the 3D flythroughs created using image-based rendering, even for scenes with significant nonvertical structure. Using this approach, we have created qualitatively correct 3D models for 64.9 percent of 588 images downloaded from the Internet. We have also extended our model to produce large-scale 3D models from a few images.																	0162-8828					MAY	2009	31	5					824	840		10.1109/TPAMI.2008.132						WOS:000264144500005	19299858	J	Nychas, GJE; Skandamis, PN; Tassou, CC; Koutsoumanis, KP				Nychas, George-John E.; Skandamis, Panos N.; Tassou, Chrysoula C.; Koutsoumanis, Konstantinos P.			Meat spoilage during distribution	MEAT SCIENCE					MEETING OF THE MIDDLE SECTION OF THE AMERICAN LARYNGOLOGICAL, RHINOLOGICAL AND OTOLOGICAL SOC INC	JAN 25-26, 1986	CHICAGO, IL	LARYNGOL RHINOL & OTOL SOC INC, MIDDLE SECT				Meat spoilage during distribution can be considered as an ecological phenomenon that encompasses the changes of the available substrata (e.g., low molecular compounds), during the prevailing of a particular microbial association, the so-called specific spoilage organisms (SSO). In fact, spoilage of meat depends on an even smaller fraction of SSO, called ephemeral spoilage organisms (ESO). These ESO are the consequence of factors that dynamically persist or imposed during, e.g., processing, transportation and storage in the market. Meanwhile spoilage is a subjective judgment by the consumer, which may be influenced by cultural and economic considerations and background as well as by the sensory acuity of the individual and the intensity of the change. Indeed, when spoilage progresses, most consumers would agree that gross discoloration, strong off-odors, and the-development of slime would constitute the main qualitative criteria for meat rejection. On the other hand, meat industry needs rapid analytical methods or tools for quantification of these indicators to determine the type of processing needed for their raw material and to predict remaining shelf life of their products. The need of an objective evaluation of meat spoilage is of great importance. The use of metabolomics as a potential tool for the evaluation of meat spoilage can be of great importance. The microbial association of meat should be monitored in parallel with the estimation of changes occurring in the production and/or assimilation of certain compounds would allow us to evaluate spoilage found or produced during the storage of meat under different temperatures as well as packaging conditions. (c) 2007 Elsevier Ltd. All rights reserved.				Koutsoumanis, Konstantinos/B-3170-2014; Nychas, George/B-2734-2009; Tassou, Chrysoula/H-2899-2016	Nychas, George/0000-0002-2673-6425; 												0309-1740					JAN-FEB	2008	78	1-2					77	89		10.1016/j.meatsei.2007.06.020						WOS:000251871700010	22062098	J	Lemm, S; Blankertz, B; Curio, G; Muller, KR				Lemm, S; Blankertz, B; Curio, G; Muller, KR			Spatio-spectral filters for improving the classification of single trial EEG	IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING												Data recorded in electroencephalogram (EEG)-based brain-computer interface experiments is generally very noisy, non-stationary, and contaminated with artifacts that can deteriorate discrimination/classification methods. In this paper, we extend the common spatial pattern (CSP) algorithm with the aim to alleviate these adverse effects. In particular, we suggest an extension of CSP to the state space, which utilizes the method of time delay embedding. As we will show, this allows for individually tuned frequency filters at each electrode position and, thus, yields an improved and more robust machine learning procedure. The advantages of the proposed method over the original CSP method are verified in terms of an improved information transfer rate (bits per trial) on a set of EEG-recordings from experiments of imagined limb movements.				Muller, Klaus/C-3196-2013													0018-9294					SEP	2005	52	9					1541	1548		10.1109/TBME.2005.851521						WOS:000231268900006	16189967	J	Wang, HZ; Liu, X; Lv, B; Yang, F; Hong, YZ				Wang, Huazhen; Liu, Xin; Lv, Bing; Yang, Fan; Hong, Yanzhu			Reliable Multi-Label Learning via Conformal Predictor and Random Forest for Syndrome Differentiation of Chronic Fatigue in Traditional Chinese Medicine	PLOS ONE												Objective: Chronic Fatigue (CF) still remains unclear about its etiology, pathophysiology, nomenclature and diagnostic criteria in the medical community. Traditional Chinese medicine (TCM) adopts a unique diagnostic method, namely 'bian zheng lun zhi' or syndrome differentiation, to diagnose the CF with a set of syndrome factors, which can be regarded as the Multi-Label Learning (MLL) problem in the machine learning literature. To obtain an effective and reliable diagnostic tool, we use Conformal Predictor (CP), Random Forest (RF) and Problem Transformation method (PT) for the syndrome differentiation of CF. Methods and Materials: In this work, using PT method, CP-RF is extended to handle MLL problem. CP-RF applies RF to measure the confidence level (p-value) of each label being the true label, and then selects multiple labels whose p-values are larger than the pre-defined significance level as the region prediction. In this paper, we compare the proposed CP-RF with typical CP-NBC(Naive Bayes Classifier), CP-KNN(K-Nearest Neighbors) and ML-KNN on CF dataset, which consists of 736 cases. Specifically, 95 symptoms are used to identify CF, and four syndrome factors are employed in the syndrome differentiation, including 'spleen deficiency', 'heart deficiency', 'liver stagnation' and 'qi deficiency'. The Results: CP-RF demonstrates an outstanding performance beyond CP-NBC, CP-KNN and ML-KNN under the general metrics of subset accuracy, hamming loss, one-error, coverage, ranking loss and average precision. Furthermore, the performance of CP-RF remains steady at the large scale of confidence levels from 80% to 100%, which indicates its robustness to the threshold determination. In addition, the confidence evaluation provided by CP is valid and wellcalibrated. Conclusion: CP-RF not only offers outstanding performance but also provides valid confidence evaluation for the CF syndrome differentiation. It would be well applicable to TCM practitioners and facilitate the utilities of objective, effective and reliable computer-based diagnosis tool.				Ke, Zun-Ji/N-5689-2014; Ke, Zunji/E-4822-2010; Fang, Shengyun/H-3802-2011; Luo, Jia/E-4674-2012	Ke, Zun-Ji/0000-0003-4038-2456; Ke, Zunji/0000-0003-4038-2456; 												1932-6203					JUN 11	2014	9	6							e99565	10.1371/journal.pone.0099565						WOS:000338631000095	24918430	J	Camps-Valls, G; Bandos, TV; Zhou, DY				Camps-Valls, Gustavo; Bandos, Tatyana V.; Zhou, Dengyong			Semi-supervised graph-based hyperspectral image classification	IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING					IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	JUL 31-AUG 04, 2006	Denver, CO	IEEE, IEEE Geosci & Remote Sensing Soc, Canadian Remote Sensing Soc, NASA, NOAA, Off Naval Res, Natl Polar Orbiting Operat Environm Satellite Syst, Japan Aerosp Explorat Agcy, Ball Aerosp & Technologies Corp, Cooperat Inst Res Atmosphere, Colorado State Univ, Univ Colorado, Int Union Radio Sci				This paper presents a semi-supervised graph-based method for the classification of hyperspectral images. The method is designed to handle the special characteristics of hyperspectral images, namely, high-input dimension of pixels, low number of labeled samples, and spatial variability of the spectral signature. To alleviate these problems, the method incorporates three ingredients, respectively. First, being a kernel-based method, it combats the curse of dimensionality efficiently. Second, following a semi-supervised approach, it exploits the wealth of unlabeled samples in the image, and naturally gives relative importance to the labeled ones through a graph-based methodology. Finally, it incorporates contextual information through a full family of composite kernels. Noting that the graph method relies on inverting a huge kernel matrix formed by both labeled and unlabeled samples, we originally introduce the Nystrom method in the formulation to speed up the classification process. The presented semi-supervised-graph-based method is compared to state-of-the-art support vector machines in the classification of hyperspectral data. The proposed method produces better classification maps, which capture the intrinsic structure collectively revealed by labeled and unlabeled points. Good and stable accuracy is produced in ill-posed classification problems (high dimensional spaces and low number of labeled samples). In addition, the introduction of the composite-kernel framework drastically improves results, and the new fast formulation ranks almost linearly in the computational cost, rather than cubic as in the original method, thus allowing the use of this method in remote-sensing applications.				Camps-Valls, Gustavo/A-2532-2011; Bandos, Tatyana/H-7063-2015	Camps-Valls, Gustavo/0000-0003-1683-2138; Bandos, Tatyana/0000-0002-7715-2032												0196-2892	1558-0644				OCT	2007	45	10					3044	3054		10.1109/TGRS.2007.895416						WOS:000249795600007		J	Birbaumer, N				Birbaumer, Niels			Breaking the silence: Brain-computer interfaces (BCI) for communication and motor control	PSYCHOPHYSIOLOGY												Brain-computer interfaces (BCI) allow control of computers or external devices with regulation of brain activity alone. Invasive BCIs, almost exclusively investigated in animal models using implanted electrodes in brain tissue, and noninvasive BCIs using electrophysiological recordings in humans are described. Clinical applications were reserved with few exceptions for the noninvasive approach: communication with the completely paralyzed and locked-in syndrome with slow cortical potentials, sensorimotor rhythm and P300, and restoration of movement and cortical reorganization in high spinal cord lesions and chronic stroke. It was demonstrated that noninvasive EEG-based BCIs allow brain-derived communication in paralyzed and locked-in patients but not in completely locked-in patients. At present no firm conclusion about the clinical utility of BCI for the control of voluntary movement can be made. Invasive multielectrode BCIs in otherwise healthy animals allowed execution of reaching, grasping, and force variations based on spike patterns and extracellular field potentials. The newly developed fMRI-BCIs and NIRS-BCIs, like EEG BCIs, offer promise for the learned regulation of emotional disorders and also disorders of young children.																	0048-5772					NOV	2006	43	6					517	532		10.1111/j.1469-8986.2006.00456.x						WOS:000241626200001	17076808	J	Chen, BJ; Chang, MW; Lin, CJ				Chen, BJ; Chang, MW; Lin, CJ			Load forecasting using support vector machines: A study on EUNITE competition 2001	IEEE TRANSACTIONS ON POWER SYSTEMS												Load forecasting is usually made by constructing models on relative information, such as climate and previous load demand data. In 2001, EUNITE network organized a competition aiming at mid-term load forecasting (predicting daily maximum load of the next 31 days). During the competition we proposed a support vector machine (SVM) model, which was the winning entry, to solve the problem. In this paper, we discuss in detail how SVM, a new learning technique, is successfully applied to load forecasting. In addition, motivated by the competition results and the approaches by other participants, more experiments and deeper analyses are conducted and presented here. Some important conclusions from the results are that temperature (or other types of climate information) might not be useful in such a mid-term load forecasting problem and that the introduction of time-series concept may improve the forecasting.					Lin, Chih-Jen/0000-0003-4684-8747												0885-8950					NOV	2004	19	4					1821	1830		10.1109/TPWRS.2004.835679						WOS:000224849400013		J	Meltzoff, AN; Kuhl, PK; Movellan, J; Sejnowski, TJ				Meltzoff, Andrew N.; Kuhl, Patricia K.; Movellan, Javier; Sejnowski, Terrence J.			Foundations for a New Science of Learning	SCIENCE												Human learning is distinguished by the range and complexity of skills that can be learned and the degree of abstraction that can be achieved compared with those of other species. Homo sapiens is also the only species that has developed formal ways to enhance learning: teachers, schools, and curricula. Human infants have an intense interest in people and their behavior and possess powerful implicit learning mechanisms that are affected by social interaction. Neuroscientists are beginning to understand the brain mechanisms underlying learning and how shared brain systems for perception and action support social learning. Machine learning algorithms are being developed that allow robots and computers to learn autonomously. New insights from many different fields are converging to create a new science of learning that may transform educational practices.																	0036-8075	1095-9203				JUL 17	2009	325	5938					284	288		10.1126/science.1175626						WOS:000268036600035	19608908	J	Smith, AE; Humphreys, MS				Smith, Andrew E.; Humphreys, Michael S.			Evaluation of unsupervised semantic mapping of natural language with Leximancer concept mapping	BEHAVIOR RESEARCH METHODS												The Leximancer system is a relatively new method for transforming lexical co-occurrence information from natural language into semantic patterns in an unsupervised manner. It employs two stages of co-occurrence information extraction-semantic and relational-using a different algorithm for each stage. The algorithms used are statistical, but they employ nonlinear dynamics and machine learning. This article is an attempt to validate the output of Leximancer, using a set of evaluation criteria taken from content analysis that are appropriate for knowledge discovery tasks.				Fitzgerald, Robert/I-7250-2016	Fitzgerald, Robert/0000-0003-1906-4939												1554-351X					MAY	2006	38	2					262	279		10.3758/BF03192778						WOS:000240078900013	16956103	J	Liu, CL; Nakashima, K; Sako, H; Fujisawa, H				Liu, CL; Nakashima, K; Sako, H; Fujisawa, H			Handwritten digit recognition: benchmarking of state-of-the-art techniques	PATTERN RECOGNITION												This paper presents the results of handwritten digit recognition on well-known image databases using state-of-the-art feature extraction and classification techniques. The tested databases are CENPARMI, CEDAR, and MNIST. On the test data set of each database, 80 recognition accuracies are given by combining eight classifiers with ten feature vectors. The features include chaincode feature, gradient feature, profile structure feature, and peripheral direction contributivity. The gradient feature is extracted from either binary image or gray-scale image. The classifiers include the k-nearest neighbor classifier, three neural classifiers, a learning vector quantization classifier, a discriminative learning quadratic discriminant function (DLQDF) classifier, and two support vector classifiers (SVCs). All the classifiers and feature vectors give high recognition accuracies. Relatively, the chaincode feature and the gradient feature show advantage over other features, and the profile structure feature shows efficiency as a complementary feature. The SVC with RBF kernel (SVC-rbf) gives the highest accuracy in most cases but is extremely expensive in storage and computation. Among the non-SV classifiers, the polynomial classifier and DLQDF give the highest accuracies. The results of non-SV classifiers are competitive to the best ones previously reported on the same databases. (C) 2003 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.																	0031-3203					OCT	2003	36	10					2271	2285		10.1016/S0031-3206(03)00085-2						WOS:000184510000006		J	Figueiredo, MAT				Figueiredo, MAT			Adaptive sparseness for supervised learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												The goal of supervised teaming is to infer a functional mapping based on a set of training examples. To achieve good generalization, it is necessary to control the "complexity" of the learned function. In Bayesian approaches, this is done by adapting a prior for the parameters of the function being teamed. We propose a Bayesian approach to supervised teaming, which leads to sparse solutions; that is, in which irrelevant parameters are automatically set exactly to zero. Other ways to obtain sparse classifiers (such as Laplacian priors, support vector machines) involve (hyper)parameters which control the degree of sparseness of the resulting classifiers; these parameters have to be somehow adjusted/estimated from the training data. In contrast, our approach does not involve any (hyper)parameters to be adjusted or estimated. This is achieved by a hierarchical-Bayes interpretation of the Laplacian prior, which is then modified by the adoption of a Jeffreys' noninformative hyperprior. Implementation is carried out by an expectation-maximization (EM) algorithm. Experiments with several benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms SVMs and performs competitively with the best alternative techniques, although it involves no tuning or adjustment of sparseness-controlling hyperparameters.				Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745												0162-8828	1939-3539				SEP	2003	25	9					1150	1159		10.1109/TPAMI.2003.1227989						WOS:000184977300009		J	Girosi, F				Girosi, F			An equivalence between sparse approximation and support vector machines	NEURAL COMPUTATION												This article shows a relationship between two different approximation techniques: the support vector machines (SVM), proposed by V. Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, & Saunders, 1995). SVM is a technique that can be derived from the structural risk minimization principle (Vapnik, 1982) and can be used to estimate the parameters of several different approximation schemes, including radial basis functions, algebraic and trigonometric polynomials, B-splines, and some forms of multilayer perceptrons. Basis pursuit denoising is a sparse approximation technique in which a function is reconstructed by using a small number of basis functions chosen from a large set (the dictionary). We show that if the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem. In the appendix, we present a derivation of the SVM technique in the framework of regularization theory, rather than statistical learning theory, establishing a connection between SVM, sparse approximation, and regularization theory.																	0899-7667	1530-888X				AUG 15	1998	10	6					1455	1480		10.1162/089976698300017269						WOS:000075118400006		J	Hofmann, M; Steinke, F; Scheel, V; Charpiat, G; Farquhar, J; Aschoff, P; Brady, M; Scholkopf, B; Pichler, BJ				Hofmann, Matthias; Steinke, Florian; Scheel, Verena; Charpiat, Guillaume; Farquhar, Jason; Aschoff, Philip; Brady, Michael; Schoelkopf, Bernhard; Pichler, Bernd J.			MRI-Based Attenuation Correction for PET/MRI: A Novel Approach Combining Pattern Recognition and Atlas Registration	JOURNAL OF NUCLEAR MEDICINE												For quantitative PET information, correction of tissue photon attenuation is mandatory. Generally in conventional PET, the attenuation map is obtained from a transmission scan, which uses a rotating radionuclide source, or from the CT scan in a combined PET/CT scanner. In the case of PET/MRI scanners currently under development, insufficient space for the rotating source exists; the attenuation map can be calculated from the MR image instead. This task is challenging because MR intensities correlate with proton densities and tissue-relaxation properties, rather than with attenuation-related mass density. Methods: We used a combination of local pattern recognition and atlas registration, which captures global variation of anatomy, to predict pseudo-CT images from a given MR Image. These pseudo-CT images were then used for attenuation correction, as the process would be performed in a PET/CT scanner. Results: For human brain scans, we show on a database of 17 MR/CT image pairs that our method reliably enables estimation of a pseudo-CT image from the MR image alone. On additional datasets of MRI/PET/ CT triplets of human brain scans, we compare MRI-based attenuation correction with CT-based correction. Our approach enables PET quantification with a mean error of 3.2% for predefined regions of interest, which we found to be clinically not significant. However, our method is not specific to brain imaging, and we show promising Initial results on 1 whole-body animal dataset. Conclusion: This method allows reliable MRI-based attenuation correction for human brain scans. Further work is necessary to validate the method for whole-body imaging.				Farquhar, Jason/D-2285-2010; Pichler, Bernd/B-4483-2012; Scholkopf, Bernhard/A-7570-2013	Farquhar, Jason/0000-0002-8560-0712; 												0161-5505	1535-5667				NOV	2008	49	11					1875	1883		10.2967/jnumed.107.049353						WOS:000260846600028	18927326	J	Hao, G; Derakhshan, B; Shi, L; Campagne, F; Gross, SS				Hao, G; Derakhshan, B; Shi, L; Campagne, F; Gross, SS			SNOSID, a proteomic method for identification of cysteine S-nitrosylation sites in complex protein mixtures	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA												Reversible addition of NO to Cys-sulfur in proteins, a modification termed S-nitrosylation, has emerged as a ubiquitous signaling mechanism for regulating diverse cellular processes. A key first-step toward elucidating the mechanism by which S-nitrosylation modulates a protein's function is specification of the targeted Cys (SNO-Cys) residue. To date, S-nitrosylation site specification has been laboriously tackled on a protein-by-protein basis. Here we describe a high-throughput proteomic approach that enables simultaneous identification of SNO-Cys sites and their cognate proteins in complex biological mixtures. The approach, termed SNOSID (SNO Site Identification), is a modification of the biotin-swap technique [Jaffrey, S. R., Erdjument-Bromage, H., Ferris, C. D., Tempst, P. & Snyder, S. H. (2001) Nat. Cell. Biol. 3, 193-197], comprising biotinylation of protein SNO-Cys residues, trypsinolysis, affinity purification of biotinylated-peptides, and amino acid sequencing by liquid chromatography tandem MS. With this approach, 68 SNO-Cys sites were specified on 56 distinct proteins in S-nitrosoglutathione-treated (2-10 mu M) rat cerebellum lysates. In addition to enumerating these S-nitrosylation sites, the method revealed endogenous SNO-Cys modification sites on cerebellum proteins, including alpha-tubulin, beta-tubulin, GAPDH, and dihydropyrimidinase-related protein-2. Whereas these endogenous SNO proteins were previously recognized, we extend prior knowledge by specifying the SNO-Cys modification sites. Considering all 68 SNO-Cys sites identified, a machine learning approach failed to reveal a linear Cys-flanking motif that predicts stable transnitrosation by S-nitrosoglutathione under test conditions, suggesting that undefined 3D structural features determine S-nitrosylation specificity. SNOSID provides the first effective tool for unbiased elucidation of the SNO proteome, identifying Cys residues that undergo reversible S-nitrosylation.				Campagne, Fabien/F-5158-2010	Campagne, Fabien/0000-0001-6237-3564												0027-8424					JAN 24	2006	103	4					1012	1017		10.1073/pnas.0508412103						WOS:000234938300033	16418269	J	Muttil, N; Chau, KW				Muttil, Nitin; Chau, Kwok-Wing			Neural network and genetic programming for modelling coastal algal blooms	INTERNATIONAL JOURNAL OF ENVIRONMENT AND POLLUTION												In the recent past, machine learning (ML) techniques such as artificial neural networks (ANN) have been increasingly used to model algal bloom dynamics. In the present paper, along with ANN, we select genetic programming (GP) for modelling and prediction of algal blooms in Tolo Harbour, Hong Kong. The study of the weights of the trained ANN and also the GP-evolved equations shows that they correctly identify the ecologically significant variables. Analysis of various ANN and GP scenarios indicates that good predictions of long-term trends in algal biomass can be obtained using only chlorophyll-a as input. The results indicate that the use of biweekly data can simulate long-term trends of algal biomass reasonably well, but it is not ideally suited to give short-term algal bloom predictions.				Chau, Kwok-wing/E-5235-2011; Muttil, Nitin/J-6714-2016	Chau, Kwok-wing/0000-0001-6457-161X; Muttil, Nitin/0000-0001-7758-8365												0957-4352	1741-5101					2006	28	3-4					223	238		10.1504/IJEP.2006.011208						WOS:000243072200002		J	Obradovic, Z; Peng, K; Vucetic, S; Radivojac, P; Brown, CJ; Dunker, AK				Obradovic, Z; Peng, K; Vucetic, S; Radivojac, P; Brown, CJ; Dunker, AK			Predicting intrinsic disorder from amino acid sequence	PROTEINS-STRUCTURE FUNCTION AND GENETICS					5th Meeting on the Critical Assessment of Techniques for Protein Structure Prediction	DEC 01-05, 2002	PACIFIC GROVE, CALIFORNIA					Blind predictions of intrinsic order and disorder were made on 42 proteins subsequently revealed to contain 9,044 ordered residues, 284 disordered residues in 26 segments of length 30 residues or less, and 281 disordered residues in 2 disordered segments of length greater than 30 residues. The accuracies of the six predictors used in this experiment ranged from 77% to 91% for the ordered regions and from 56% to 78% for the disordered segments. The average of the order and disorder predictions ranged from 73% to 77%. The prediction of disorder in the shorter segments was poor, from 25% to 66% correct, while the prediction of disorder in the longer segments was better, from 75% to 95% correct. Four of the predictors were composed of ensembles of neural networks. This enabled them to deal more efficiently with the large asymmetry in the training data through diversified sampling from the significantly larger ordered set and achieve better accuracy on ordered and long disordered regions. The exclusive use of long disordered regions for predictor training likely contributed to the disparity of the predictions on long versus short disordered regions, while averaging the output values over 61-residue windows to eliminate short predictions of order or disorder probably contributed to the even greater disparity for three of the predictors. This experiment supports the predictability of intrinsic disorder from amino acid sequence. (C) 2003 Wiley-Liss, Inc.																	0887-3585						2003	53	6		6			566	572		10.1002/prot.10532						WOS:000186378100027	14579347	J	Lee, YJ; Mangasarian, OL				Lee, YJ; Mangasarian, OL			SSVM: A smooth support vector machine for classification	COMPUTATIONAL OPTIMIZATION AND APPLICATIONS												Smoothing methods, extensively used for solving important mathematical programming problems and applications, are applied here to generate and solve an unconstrained smooth reformulation of the support vector machine for pattern classification using a completely arbitrary kernel. We term such reformulation a smooth support vector machine (SSVM). A fast Newton-Armijo algorithm for solving the SSVM converges globally and quadratically. Numerical results and comparisons are given to demonstrate the effectiveness and speed of the algorithm. On six publicly available datasets, tenfold cross validation correctness of SSVM was the highest compared with four other methods as well as the fastest. On larger problems, SSVM was comparable or faster than SVMlight (T. Joachims, in Advances in Kernel Methods-Support Vector Learning, MIT Press: Cambridge, MA, 1999), SOR (O.L. Mangasarian and David R. Musicant, IEEE Transactions on Neural Networks, vol. 10, pp. 1032-1037, 1999) and SMO (J. Platt, in Advances in Kernel Methods-Support Vector Learning, MIT Press: Cambridge, MA, 1999). SSVM can also generate a highly nonlinear separating surface such as a checkerboard.																	0926-6003					OCT	2001	20	1					5	22		10.1023/A:1011215321374						WOS:000170056400001		J	Jayadeva; Khemchandani, R; Chandra, S				Jayadeva; Khemchandani, R.; Chandra, Suresh			Twin support vector machines for pattern classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												We propose Twin SVM, a binary SVM classifier that determines two nonparallel planes by solving two related SVM-type problems, each of which is smaller than in a conventional SVM. The Twin SVM formulation is in the spirit of proximal SVMs via generalized eigenvalues. On several benchmark data sets, Twin SVM is not only fast, but shows good generalization. Twin SVM is also useful for automatically discovering two-dimensional projections of the data.																	0162-8828					MAY	2007	29	5					905	910		10.1109/TPAMI.2007.1068						WOS:000244855700014	17469239	J	Catchpole, GS; Beckmann, M; Enot, DP; Mondhe, M; Zywicki, B; Taylor, J; Hardy, N; Smith, A; King, RD; Kell, DB; Fiehn, O; Draper, J				Catchpole, GS; Beckmann, M; Enot, DP; Mondhe, M; Zywicki, B; Taylor, J; Hardy, N; Smith, A; King, RD; Kell, DB; Fiehn, O; Draper, J			Hierarchical metabolomics demonstrates substantial compositional similarity between genetically modified and conventional potato crops	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA												There is current debate whether genetically modified (GM) plants might contain unexpected, potentially undesirable changes in overall metabolite composition. However, appropriate analytical technology and acceptable metrics of compositional similarity require development. We describe a comprehensive comparison of total metabolites in field-grown GM and conventional potato tubers using a hierarchical approach initiating with rapid metabolome "fingerprinting" to guide more detailed profiling of metabolites where significant differences are suspected. Central to this strategy are data analysis procedures able to generate validated, reproducible metrics of comparison from complex metabolome data. We show that, apart from targeted changes, these GM potatoes in this study appear substantially equivalent to traditional cultivars.				Kell, Douglas/E-8318-2011	Kell, Douglas/0000-0001-5838-7963; King, Ross/0000-0001-7208-4387												0027-8424					OCT 4	2005	102	40					14458	14462		10.1073/pnas.0503955102						WOS:000232392900060	16186495	J	Lara, OD; Labrador, MA				Lara, Oscar D.; Labrador, Miguel A.			A Survey on Human Activity Recognition using Wearable Sensors	IEEE COMMUNICATIONS SURVEYS AND TUTORIALS												Providing accurate and opportune information on people's activities and behaviors is one of the most important tasks in pervasive computing. Innumerable applications can be visualized, for instance, in medical, security, entertainment, and tactical scenarios. Despite human activity recognition (HAR) being an active field for more than a decade, there are still key aspects that, if addressed, would constitute a significant turn in the way people interact with mobile devices. This paper surveys the state of the art in HAR based on wearable sensors. A general architecture is first presented along with a description of the main components of any HAR system. We also propose a two-level taxonomy in accordance to the learning approach (either supervised or semi-supervised) and the response time (either offline or online). Then, the principal issues and challenges are discussed, as well as the main solutions to each one of them. Twenty eight systems are qualitatively evaluated in terms of recognition performance, energy consumption, obtrusiveness, and flexibility, among others. Finally, we present some open problems and ideas that, due to their high relevance, should be addressed in future research.																	1553-877X						2013	15	3					1192	1209		10.1109/SURV.2012.110112.00192						WOS:000322728300012		J	Misra, C; Fan, Y; Davatzikos, C				Misra, Chandan; Fan, Yong; Davatzikos, Christos			Baseline and longitudinal patterns of brain atrophy in MCI patients, and their use in prediction of short-term conversion to AD: Results from ADNI	NEUROIMAGE												High-dimensional pattern classification was applied to baseline and multiple follow-up MRI scans of the Alzheimer's Disease Neuroimaging Initiative (ADNI) participants with mild cognitive impairment (MCI), in order to investigate the potential of predicting short-term conversion to Alzheimer's Disease (AD) on an individual basis. MCI participants that converted to AD (average follow-up 15 months) displayed significantly lower volumes in a number of grey matter (GM) regions, as well as in the white matter (WM). They also displayed more pronounced periventricular small-vessel pathology, as well as an increased rate of increase of such pathology. Individual person analysis was performed using a pattern classifier previously constructed from AD patients and cognitively normal (CN) individuals to yield an abnormality score that is positive for AD-like brains and negative otherwise. The abnormality scores measured from MCI non-converters (MCI-NC) followed a bimodal distribution, reflecting the heterogeneity of this group, whereas they were positive in almost all MCI converters (MCI-C), indicating extensive patterns of AD-like brain atrophy in almost all MCI-C. Both MCI subgroups had similar MMSE scores at baseline. A more specialized classifier constructed to differentiate converters from non-converters based on their baseline scans provided good classification accuracy reaching 81.5%, evaluated via cross-validation. These pattern classification schemes, which distill spatial patterns of atrophy to a single abnormality score, offer promise as biomarkers of AD and as predictors of subsequent clinical progression, on an individual patient basis. (C) 2008 Elsevier Inc. All rights reserved.				Scharre, Douglas/E-4030-2011	Fan, Yong/0000-0001-9869-4685												1053-8119	1095-9572				FEB 15	2009	44	4					1415	1422		10.1016/j.neuroimage.2008.10.031						WOS:000263861800020	19027862	J	Renouard, F; Nisand, D				Renouard, Franck; Nisand, David			Impact of implant length and diameter on survival rates	CLINICAL ORAL IMPLANTS RESEARCH					1st Consensus Conference of the European-Association-for-Osseointegration	FEB 16-19, 2006	Con, Pfaffikon, SWITZERLAND		Con			Introduction: Despite the high success rates of endosseous oral implants, restrictions have been advocated to their placement with regard to the bone available in height and volume. The use of short or nonstandard-diameter implants could be one way to overcome this limitation. Material and methods: In order to explore the relationship between implant survival rates and their length and diameter, a Medline and a hand search was conducted covering the period 1990-2005. Papers were included which reported: (1) relevant data on implant length and diameter, (2) implant survival rates; either clearly indicated or calculable from data in the paper, (3) clearly defined criteria for implant failure, and in which (4) implants were placed in healed sites and (5) studies were in human subjects. Results: A total of 53 human studies fulfilled the inclusion criteria. Concerning implant length, a relatively high number of published studies (12) indicated an increased failure rate with short implants which was associated with operators' learning curves, a routine surgical preparation (independent of the bone density), the use of machined-surfaced implants, and the placement in sites with poor bone density. Recent publications (22) reporting an adapted surgical preparation and the use of textured-surfaced implants have indicated survival rates of short implants comparable with those obtained with longer ones. Considering implant diameter, a few publications on wide-diameter implants have reported an increased failure rate, which was mainly associated with the operators' learning curves, poor bone density, implant design and site preparation, and the use of a wide implant when primary stability had not been achieved with a standard-diameter implant. More recent publications with an adapted surgical preparation, new implant designs and adequate indications have demonstrated that implant survival rate and diameter have no relationship. Discussion: When surgical preparation is related to bone density, textured-surfaced implants are employed, operators' surgical skills are developed, and indications for implant treatment duly considered, the survival rates for short and for wide-diameter implants has been found to be comparable with those obtained with longer implants and those of a standard diameter. The use of a short or wide implant may be considered in sites thought unfavourable for implant success, such as those associated with bone resorption or previous injury and trauma. While in these situations implant failure rates may be increased, outcomes should be compared with those associated with advanced surgical procedure such as bone grafting, sinus lifting, and the transposition of the alveolar nerve.																	0905-7161					OCT	2006	17			2			35	51		10.1111/j.1600-0501.2006.01349.x						WOS:000240804800005	16968380	J	Kasabov, N				Kasabov, N			Evolving fuzzy neural networks for supervised/unsupervised online knowledge-based learning	IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART B-CYBERNETICS												This paper introduces evolving fuzzy neural networks (EFuNNs) as a means for the implementation of the evolving connectionist systems (ECOS) paradigm that is aimed at building online, adaptive intelligent systems that have both their structure and functionality evolving in time. EFuNNs evolve their structure and parameter values through incremental, hybrid supervised/unsupervised, online learning. They can accommodate new input data, including new features, new classes, etc., through local element tuning. New connections and new neurons are created during the operation of the system. EFuNNs can learn spatial-temporal sequences in an adaptive way through one pass learning and automatically adapt their parameter values as they operate. Fuzzy or crisp rules can be inserted and extracted at any time of the EFuNN operation. The characteristics of EFuNNs are illustrated on several case study data sets for time series prediction and spoken word classification. Their performance is compared with traditional connectionist methods and systems. The applicability of EFuNNs as general purpose online learning machines, what concerns systems that learn from large databases, life-long learning systems, and online adaptive systems in different areas of engineering are discussed.																	1083-4419					DEC	2001	31	6					902	918		10.1109/3477.969494						WOS:000172566600006	18244856	J	Vemuri, P; Gunter, JL; Senjem, ML; Whitwell, JL; Kantarci, K; Knopman, DS; Boeve, BF; Petersen, RC; Jack, CR				Vemuri, Prashanthi; Gunter, Jeffrey L.; Senjem, Matthew L.; Whitwell, Jennifer L.; Kantarci, Kejal; Knopman, David S.; Boeve, Bradley F.; Petersen, Ronald C.; Jack, Clifford R., Jr.			Alzheimer's disease diagnosis in individual subjects using structural MR images: Validation studies	NEUROIMAGE												Objective: To develop and validate a tool for Alzheimer's disease (AD) diagnosis in individual subjects using support vector machine (SVM)-based classification of structural MR (sMR) images. Background. Libraries of sMR scans of clinically well characterized subjects can be harnessed for the purpose of diagnosing new incoming subjects. Methods: One hundred ninety patients with probable AD were age-and gender-matched with 190 cognitively normal (CN) subjects. Three different classification models were implemented: Model I uses tissue densities obtained from sMR scans to give STructural Abnormality iNDex (STAND)-score; and Models II and III use tissue densities as well as covariates (demographics and Apolipoprotein E genotype) to give adjusted-STAND (aSTAND)-score. Data from 140 AD and 140 CN were used for training. The SVM parameter optimization and training were done by four-fold cross validation (CV). The remaining independent sample of 50 AD and 50 CN was used to obtain a minimally biased estimate of the generalization error of the algorithm. Results: The CV accuracy of Model II and Model III aSTAND-scores was 88.5% and 89.3%, respectively, and the developed models generalized well on the independent test data sets. Anatomic patterns best differentiating the groups were consistent with the known distribution of neurofibrillary AD pathology. Conclusions: This paper presents preliminary evidence that application of SVM-based classification of an individual sMR scan relative to a library of scans can provide useful information in individual subjects for diagnosis of AD. Including demographic and genetic information in the classification algorithm slightly improves diagnostic accuracy. (c) 2007 Elsevier Inc. All rights reserved.				Jack, Clifford/F-2508-2010; Vemuri, Prashanthi/K-7030-2012	Jack, Clifford/0000-0001-7916-622X; Senjem, Matthew/0000-0001-9308-9275												1053-8119					FEB 1	2008	39	3					1186	1197		10.1016/j.neuroimage.2007.09.073						WOS:000252691800026	18054253	J	Davatzikos, C; Ruparel, K; Fan, Y; Shen, DG; Acharyya, M; Loughead, JW; Gur, RC; Langleben, DD				Davatzikos, C; Ruparel, K; Fan, Y; Shen, DG; Acharyya, M; Loughead, JW; Gur, RC; Langleben, DD			Classifying spatial patterns of brain activity with machine learning methods: Application to lie detection	NEUROIMAGE												Patterns of brain activity during deception have recently been characterized with fMRI on the multi-subject average group level. The clinical value of fMRI in lie detection will be determined by the ability to detect deception in individual subjects, rather than group averages. High-dimensional non-linear pattern classification methods applied to functional magnetic resonance (fMRI) images were used to discriminate between the spatial patterns of brain activity associated with lie and truth. In 22 participants performing a forced-choice deception task, 99% of the true and false responses were discriminated correctly. Predictive accuracy, assessed by cross-validation in participants not included in training, was 88%. The results demonstrate the potential of non-linear machine learning techniques in lie detection and other possible clinical applications of fMRI in individual subjects, and indicate that accurate clinical tests could be based on measurements of brain function with fMRI. (c) 2005 Elsevier Inc. All rights reserved.				Langleben, Daniel/K-7407-2014	Fan, Yong/0000-0001-9869-4685												1053-8119					NOV 15	2005	28	3					663	668		10.1016/j.neuroimage.2005.08.009						WOS:000233257000013	16169252	J	Qin, SJ				Qin, S. Joe			Survey on data-driven industrial process monitoring and diagnosis	ANNUAL REVIEWS IN CONTROL												This paper provides a state-of-the-art review of the methods and applications of data-driven fault detection and diagnosis that have been developed over the last two decades. The scope of the problem is described with reference to the scale and complexity of industrial process operations, where multi-level hierarchical optimization and control are necessary for efficient operation, but are also prone to hard failure and soft operational faults that lead to economic losses. Commonly used multivariate statistical tools are introduced to characterize normal variations and detect abnormal changes. Further, diagnosis methods are surveyed and analyzed, with fault detectability and fault identifiability for rigorous analysis. Challenges, opportunities, and extensions are summarized with the intent to draw attention from the systems and control community and the process control community. (C) 2012 Elsevier Ltd. All rights reserved.				Qin, S. Joe/A-4234-2010													1367-5788					DEC	2012	36	2					220	234		10.1016/j.arcontrol.2012.09.004						WOS:000311927400004		J	Kim, KI; Kwon, Y				Kim, Kwang In; Kwon, Younghee			Single-Image Super-Resolution Using Sparse Regression and Natural Image Prior	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												This paper proposes a framework for single-image super-resolution. The underlying idea is to learn a map from input low-resolution images to target high-resolution images based on example pairs of input and output images. Kernel ridge regression (KRR) is adopted for this purpose. To reduce the time complexity of training and testing for KRR, a sparse solution is found by combining the ideas of kernel matching pursuit and gradient descent. As a regularized solution, KRR leads to a better generalization than simply storing the examples as has been done in existing example-based algorithms and results in much less noisy images. However, this may introduce blurring and ringing artifacts around major edges as sharp changes are penalized severely. A prior model of a generic image class which takes into account the discontinuity property of images is adopted to resolve this problem. Comparison with existing algorithms shows the effectiveness of the proposed method.																	0162-8828					JUN	2010	32	6					1127	1133		10.1109/TPAMI.2010.25						WOS:000276671900013	20431136	J	El-Naqa, I; Yang, YY; Wernick, MN; Galatsanos, NP; Nishikawa, RM				El-Naqa, I; Yang, YY; Wernick, MN; Galatsanos, NP; Nishikawa, RM			A support vector machine approach for detection of microcalcifications	IEEE TRANSACTIONS ON MEDICAL IMAGING												In this paper, we investigate an approach based on support vector machines (SVMs) for detection of microcalcification (MC) clusters in digital mammograms, and propose a successive enhancement learning scheme for improved performance. SVM is a machine-learning method, based on the principle of structural risk minimization, which performs well when applied to data outside the training set. We formulate MC detection as a supervised-learning problem and apply SVM to develop the detection algorithm. We use the SVM to detect at each location in the image whether an MC is present or not. We tested the proposed method using a database of 76 clinical mammograms containing 1120 MCs. We use free-response receiver operating characteristic curves to evaluate detection performance, and compare the proposed algorithm with several existing methods. In our experiments, the proposed SVM framework outperformed all the other methods tested. In particular, a sensitivity as high as 94% was achieved by the SVM method at an error rate of one false-positive cluster per image. The ability of SVM to outperform several well-known methods developed for the widely studied problem of MC detection suggests that SVM is a promising technique for object detection in a medical imaging application.				Wernick, Miles/J-8394-2013	Wernick, Miles/0000-0001-6736-478X; Nishikawa, Robert/0000-0001-7720-9951												0278-0062	1558-254X				DEC	2002	21	12					1552	1563		10.1109/TMI.2002.806569						WOS:000180871100012	12588039	J	Burgard, W; Cremers, AB; Fox, D; Hahnel, D; Lakemeyer, G; Schulz, D; Steiner, W; Thrun, S				Burgard, W; Cremers, AB; Fox, D; Hahnel, D; Lakemeyer, G; Schulz, D; Steiner, W; Thrun, S			Experiences with an interactive museum tour-guide robot	ARTIFICIAL INTELLIGENCE												This article describes the software architecture of an autonomous, interactive tour-guide robot. It presents a modular and distributed software architecture, which integrates localization, mapping, collision avoidance, planning, and various modules concerned with user interaction and Web-based telepresence. At its heart, the software approach relies on probabilistic computation, on-line learning, and any-time algorithms. It enables robots to operate safely, reliably, and at high speeds in highly dynamic environments, and does not require any modifications of the environment to aid the robot's operation. Special emphasis is placed on the design of interactive capabilities that appeal to people's intuition. The interface provides new means for human-robot interaction with crowds of people in public places, and it also provides people all around the world with the ability to establish a "virtual telepresence" using the Web, To illustrate our approach, results are reported obtained in mid-1997, when our robot "RHINO" was deployed for a period of six days in a densely populated museum. The empirical results demonstrate reliable operation in public environments. The robot successfully raised the museum's attendance by more than 50%. In addition, thousands of people all over the world controlled the robot through the Web. We conjecture that these innovations transcend to a much larger range of application domains for service robots. (C) 1999 Elsevier Science B.V. All rights reserved.																	0004-3702	1872-7921				OCT	1999	114	1-2					3	55		10.1016/S0004-3702(99)00070-3						WOS:000084151700002		J	Hinton, GE				Hinton, Geoffrey E.			Learning multiple a layers of representation	TRENDS IN COGNITIVE SCIENCES												To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time.																	1364-6613					OCT	2007	11	10					428	434		10.1016/j.tics.2007.09.004						WOS:000250790600007	17921042	J	Chou, KC; Wu, ZC; Xiao, XA				Chou, Kuo-Chen; Wu, Zhi-Cheng; Xiao, Xuan			iLoc-Euk: A Multi-Label Classifier for Predicting the Subcellular Localization of Singleplex and Multiplex Eukaryotic Proteins	PLOS ONE												Predicting protein subcellular localization is an important and difficult problem, particularly when query proteins may have the multiplex character, i.e., simultaneously exist at, or move between, two or more different subcellular location sites. Most of the existing protein subcellular location predictor can only be used to deal with the single-location or "singleplex" proteins. Actually, multiple-location or "multiplex" proteins should not be ignored because they usually posses some unique biological functions worthy of our special notice. By introducing the "multi-labeled learning" and "accumulation-layer scale", a new predictor, called iLoc-Euk, has been developed that can be used to deal with the systems containing both singleplex and multiplex proteins. As a demonstration, the jackknife cross-validation was performed with iLoc-Euk on a benchmark dataset of eukaryotic proteins classified into the following 22 location sites: (1) acrosome, (2) cell membrane, (3) cell wall, (4) centriole, (5) chloroplast, (6) cyanelle, (7) cytoplasm, (8) cytoskeleton, (9) endoplasmic reticulum, (10) endosome, (11) extracellular, (12) Golgi apparatus, (13) hydrogenosome, (14) lysosome, (15) melanosome, (16) microsome (17) mitochondrion, (18) nucleus, (19) peroxisome, (20) spindle pole body, (21) synapse, and (22) vacuole, where none of proteins included has >= 25% pairwise sequence identity to any other in a same subset. The overall success rate thus obtained by iLoc-Euk was 79%, which is significantly higher than that by any of the existing predictors that also have the capacity to deal with such a complicated and stringent system. As a user-friendly web-server, iLoc-Euk is freely accessible to the public at the web-site http://icpr.jci.edu.cn/bioinfo/iLoc-Euk. It is anticipated that iLoc-Euk may become a useful bioinformatics tool for Molecular Cell Biology, Proteomics, System Biology, and Drug Development Also, its novel approach will further stimulate the development of predicting other protein attributes.				Chou, Kuo-Chen/A-8340-2009													1932-6203					MAR 30	2011	6	3							e18258	10.1371/journal.pone.0018258						WOS:000289055700042	21483473	S	Cucker, F; Zhou, DX				Cucker, F; Zhou, DX			Learning Theory: An Approximation Theory Viewpoint	LEARNING THEORY: AN APPROXIMATION THEORY VIEWPOINT	Cambridge Monographs on Applied and Computational Mathematics																												1759-2976		978-0-521-86559-3				2007		24					1	224		10.1017/CBO9780511618796						WOS:000297322400013		J	Wang, JM; Fleet, DJ; Hertzmann, A				Wang, Jack M.; Fleet, David J.; Hertzmann, Aaron			Gaussian process dynamical models for human motion	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												We introduce Gaussian process dynamical models (GPDMs) for nonlinear time series analysis, with applications to learning models of human pose and motion from high-dimensional motion capture data. A GPDM is a latent variable model. It comprises a low-dimensional latent space with associated dynamics, as well as a map from the latent space to an observation space. We marginalize out the model parameters in closed form by using Gaussian process priors for both the dynamical and the observation mappings. This results in a nonparametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach and compare four learning algorithms on human motion capture data, in which each pose is 50-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces.																	0162-8828					FEB	2008	30	2					283	298		10.1109/TPAMI.2007.1167						WOS:000251580300007	18084059	J	Chandrasekaran, V; Recht, B; Parrilo, PA; Willsky, AS				Chandrasekaran, Venkat; Recht, Benjamin; Parrilo, Pablo A.; Willsky, Alan S.			The Convex Geometry of Linear Inverse Problems	FOUNDATIONS OF COMPUTATIONAL MATHEMATICS												In applications throughout science and engineering one is often faced with the challenge of solving an ill-posed inverse problem, where the number of available measurements is smaller than the dimension of the model to be estimated. However in many practical situations of interest, models are constrained structurally so that they only have a few degrees of freedom relative to their ambient dimension. This paper provides a general framework to convert notions of simplicity into convex penalty functions, resulting in convex optimization solutions to linear, underdetermined inverse problems. The class of simple models considered includes those formed as the sum of a few atoms from some (possibly infinite) elementary atomic set; examples include well-studied cases from many technical fields such as sparse vectors (signal processing, statistics) and low-rank matrices (control, statistics), as well as several others including sums of a few permutation matrices (ranked elections, multiobject tracking), low-rank tensors (computer vision, neuroscience), orthogonal matrices (machine learning), and atomic measures (system identification). The convex programming formulation is based on minimizing the norm induced by the convex hull of the atomic set; this norm is referred to as the atomic norm. The facial structure of the atomic norm ball carries a number of favorable properties that are useful for recovering simple models, and an analysis of the underlying convex geometry provides sharp estimates of the number of generic measurements required for exact and robust recovery of models from partial information. These estimates are based on computing the Gaussian widths of tangent cones to the atomic norm ball. When the atomic set has algebraic structure the resulting optimization problems can be solved or approximated via semidefinite programming. The quality of these approximations affects the number of measurements required for recovery, and this tradeoff is characterized via some examples. Thus this work extends the catalog of simple models (beyond sparse vectors and low-rank matrices) that can be recovered from limited linear information via tractable convex programming.					Parrilo, Pablo/0000-0003-1132-8477												1615-3375					DEC	2012	12	6					805	849		10.1007/s10208-012-9135-7						WOS:000310338700003		J	Jiang, P; Wu, H; Wang, W; Ma, W; Sun, X; Lu, Z				Jiang, Peng; Wu, Haonan; Wang, Wenkai; Ma, Wei; Sun, Xiao; Lu, Zuhong			MiPred: classification of real and pseudo microRNA precursors using random forest prediction model with combined features	NUCLEIC ACIDS RESEARCH												To distinguish the real pre-miRNAs from other hairpin sequences with similar stem-loops (pseudo pre-miRNAs), a hybrid feature which consists of local contiguous structure-sequence composition, minimum of free energy (MFE) of the secondary structure and P-value of randomization test is used. Besides, a novel machine-learning algorithm, random forest (RF), is introduced. The results suggest that our method predicts at 98.21% specificity and 95.09% sensitivity. When compared with the previous study, Triplet-SVM-classifier, our RF method was nearly 10% greater in total accuracy. Further analysis indicated that the improvement was due to both the combined features and the RF algorithm. The MiPred web server is available at http://www.bioinf.seu.edu.cn/miRNA/. Given a sequence, MiPred decides whether it is a pre-miRNA-like hairpin sequence or not. If the sequence is a pre-miRNA-like hairpin, the RF classifier will predict whether it is a real pre-miRNA or a pseudo one.				Lu, Zuhong/A-5448-2013													0305-1048					JUL	2007	35			S			W339	W344		10.1093/nar/gkm368						WOS:000255311500064	17553836	J	Vedaldi, A; Zisserman, A				Vedaldi, Andrea; Zisserman, Andrew			Efficient Additive Kernels via Explicit Feature Maps	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Large scale nonlinear support vector machines (SVMs) can be approximated by linear ones using a suitable feature map. The linear SVMs are in general much faster to learn and evaluate (test) than the original nonlinear SVMs. This work introduces explicit feature maps for the additive class of kernels, such as the intersection, Hellinger's, and chi(2) kernels, commonly used in computer vision, and enables their use in large scale problems. In particular, we: 1) provide explicit feature maps for all additive homogeneous kernels along with closed form expression for all common kernels; 2) derive corresponding approximate finite-dimensional feature maps based on a spectral analysis; and 3) quantify the error of the approximation, showing that the error is independent of the data dimension and decays exponentially fast with the approximation order for selected kernels such as chi(2). We demonstrate that the approximations have indistinguishable performance from the full kernels yet greatly reduce the train/test times of SVMs. We also compare with two other approximation methods: Nystrom's approximation of Perronnin et al. [1], which is data dependent, and the explicit map of Maji and Berg [2] for the intersection kernel, which, as in the case of our approximations, is data independent. The approximations are evaluated on a number of standard data sets, including Caltech-101 [3], Daimler-Chrysler pedestrians [4], and INRIA pedestrians [5].				Vedaldi, Andrea/B-9071-2015	Vedaldi, Andrea/0000-0003-1374-2858												0162-8828					MAR	2012	34	3					480	492		10.1109/TPAMI.2011.153						WOS:000299381600005	21808094	J	Patton, JL; Stoykov, ME; Kovic, M; Mussa-Ivaldi, FA				Patton, JL; Stoykov, ME; Kovic, M; Mussa-Ivaldi, FA			Evaluation of robotic training forces that either enhance or reduce error in chronic hemiparetic stroke survivors	EXPERIMENTAL BRAIN RESEARCH												This investigation is one in a series of studies that address the possibility of stroke rehabilitation using robotic devices to facilitate "adaptive training." Healthy subjects, after training In the presence of systematically applied forces, typically exhibit a predictable "after-effect." A critical question is whether this adaptive characteristic is preserved following stroke so that it might be exploited for restoring function. Another important question is whether subjects benefit more from training forces that enhance their errors than from forces that reduce their errors. We exposed hemiparetic stroke survivors and healthy age-matched controls to a pattern of disturbing forces that have been Found by previous studies to induce a dramatic adaptation in healthy individuals. Eighteen stroke survivors made 834 movements in the presence of a robot-generated force field that Pushed their hands proportional to its speed and perpendicular to its direction of motion - either clockwise or counterclockwise. We found that subjects could adapt, as evidenced by significant after-effects. After-effects were not correlated with the clinical scores that we used for measuring motor impairment. Further examination revealed that significant improvements occurred only when the training forces magnified the original errors, and not when the training forces reduced the errors or were zero. Within this constrained experimental task we found that error-enhancing therapy (as opposed to guiding the limb closer to the correct path) to be more effective than therapy that assisted the subject.				Kovic, Mark/F-9713-2015	Kovic, Mark/0000-0002-5255-6911; Stoykov, Mary Ellen/0000-0002-9045-9231												0014-4819	1432-1106				JAN	2006	168	3					368	383		10.1007/s00221-005-0097-8						WOS:000235652800006	16249912	J	Kaper, M; Meinicke, P; Grossekathoefer, U; Lingner, T; Ritter, H				Kaper, M; Meinicke, P; Grossekathoefer, U; Lingner, T; Ritter, H			BCI competition 2003 - Data set IIb: Support vector machines for the P300 speller paradigm	IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING												We propose an approach to analyze data from the P300 speller paradigm using the machine-learning technique support vector machines. In a conservative classification scheme, we found the correct solution after five repetitions. While the classification within the competition is designed for offline analysis, our approach is also well-suited for a real-world online solution: It is fast, requires only 10 electrode positions and demands only a small amount of preprocessing.				Lingner, Thomas/A-2345-2008													0018-9294					JUN	2004	51	6					1073	1076		10.1109/TBME.2004.826698						WOS:000221578000029	15188881	J	Mosheiov, G				Mosheiov, G			Scheduling problems with a learning effect	EUROPEAN JOURNAL OF OPERATIONAL RESEARCH												In many realistic settings, the production facility (a machine, a worker) improves continuously as a result of repeating the same or similar activities; hence, the later a given product is scheduled in the sequence, the shorter its production time. This "learning effect" is investigated in the context of various scheduling problems. It is shown in several examples that although the optimal schedule may be very different from that of the classical version of the problem, and the computational effort becomes significantly greater, polynomial-time solutions still exist. In particular, we introduce polynomial solutions for the single-machine makespan minimization problem, and two for multi-criteria single-machine problems and the minimum flow-time problem on parallel identical machines. (C) 2001 Elsevier Science B.V. All rights reserved.																	0377-2217					AUG 1	2001	132	3					687	693		10.1016/S0377-2217(00)00175-2						WOS:000169231600017		J	Kosinski, M; Stillwell, D; Graepel, T				Kosinski, Michal; Stillwell, David; Graepel, Thore			Private traits and attributes are predictable from digital records of human behavior	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA												We show that easily accessible digital records of behavior, Facebook Likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. The analysis presented is based on a dataset of over 58,000 volunteers who provided their Facebook Likes, detailed demographic profiles, and the results of several psychometric tests. The proposed model uses dimensionality reduction for preprocessing the Likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from Likes. The model correctly discriminates between homosexual and heterosexual men in 88% of cases, African Americans and Caucasian Americans in 95% of cases, and between Democrat and Republican in 85% of cases. For the personality trait "Openness," prediction accuracy is close to the test retest accuracy of a standard personality test. We give examples of associations between attributes and Likes and discuss implications for online personalization and privacy.																	0027-8424					APR 9	2013	110	15					5802	5805		10.1073/pnas.1218772110						WOS:000317537900020	23479631	J	Lu, Z; Szafron, D; Greiner, R; Lu, P; Wishart, DS; Poulin, B; Anvik, J; Macdonell, C; Eisner, R				Lu, Z; Szafron, D; Greiner, R; Lu, P; Wishart, DS; Poulin, B; Anvik, J; Macdonell, C; Eisner, R			Predicting subcellular localization of proteins using machine-learned classifiers	BIOINFORMATICS												Motivation: Identifying the destination or localization of proteins is key to understanding their function and facilitating their purification. A number of existing computational prediction methods are based on sequence analysis. However, these methods are limited in scope, accuracy and most particularly breadth of coverage. Rather than using sequence information alone, we have explored the use of database text annotations from homologs and machine learning to substantially improve the prediction of subcellular location. Results: We have constructed five machine-learning classifiers for predicting subcellular localization of proteins from animals, plants, fungi, Gram-negative bacteria and Gram-positive bacteria, which are 81% accurate for fungi and 92-94% accurate for the other four categories. These are the most accurate subcellular predictors across the widest set of organisms ever published. Our predictors are part of the Proteome Analyst web-service.					Wishart, David S/0000-0002-3207-2434												1367-4803					MAR 1	2004	20	4					547	556		10.1093/bioinformatics/btg447						WOS:000220058800014	14990451	J	Kononenko, I				Kononenko, I			Machine learning for medical diagnosis: history, state of the art and perspective	ARTIFICIAL INTELLIGENCE IN MEDICINE												The paper provides an overview of the development of intelligent data analysis in medicine from a machine learning perspective: a historical view, a state-of-the-art view, and a view on some future trends in this subfield of applied artificial intelligence. The paper is not intended to provide a comprehensive overview but rather describes some subareas and directions which from my personal point of view seem to be important for applying machine learning in medical diagnosis. In the historical overview, I emphasize the naive Bayesian classifier, neural networks and decision trees. I present a comparison of some state-of-the-art systems, representatives from each branch of machine learning, when applied to several medical diagnostic tasks. The future trends are illustrated by two case studies. The first describes a recently developed method for dealing with reliability of decisions of classifiers, which seems to be promising for intelligent data analysis in medicine. The second describes an approach to using machine learning in order to verify some unexplained phenomena from complementary medicine, which is not (yet) approved by the orthodox medical community but could in the future play an important role in overall medical diagnosis and treatment. (C) 2001 Elsevier Science B.V. All rights reserved.																	0933-3657					AUG	2001	23	1					89	109		10.1016/S0933-3657(01)00077-X						WOS:000170512400007	11470218	B	Hofmann, T		Laskey, KB; Prade, H		Hofmann, T			Probabilistic latent semantic analysis	UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS					15th Conference on Uncertainty in Artificial Intelligence	JUL 30-AUG 01, 1999	ROYAL INST TECHNOL, STOCKHOLM, SWEDEN	AT&T Labs, Fair, Isaac & Co Inc, Hewlett Packard Co, Hugin Expert A/S, Informat Extract & Transport Inc, Knowledge Ind, Microsoft Res, Norsys Software Corp, Rockwell Int	ROYAL INST TECHNOL			Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.																			1-55860-614-9				1999							289	296								WOS:000089548100033		J	Zhang, JP; Wang, FY; Wang, KF; Lin, WH; Xu, X; Chen, C				Zhang, Junping; Wang, Fei-Yue; Wang, Kunfeng; Lin, Wei-Hua; Xu, Xin; Chen, Cheng			Data-Driven Intelligent Transportation Systems: A Survey	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS												For the last two decades, intelligent transportation systems (ITS) have emerged as an efficient way of improving the performance of transportation systems, enhancing travel security, and providing more choices to travelers. A significant change in ITS in recent years is that much more data are collected from a variety of sources and can be processed into various forms for different stakeholders. The availability of a large amount of data can potentially lead to a revolution in ITS development, changing an ITS from a conventional technology-driven system into a more powerful multifunctional data-driven intelligent transportation system ((DITS)-I-2): a system that is vision, multisource, and learning algorithm driven to optimize its performance. Furthermore, (DITS)-I-2 is trending to become a privacy-aware people-centric more intelligent system. In this paper, we provide a survey on the development of (DITS)-I-2, discussing the functionality of its key components and some deployment issues associated with (DITS)-I-2. Future research directions for the development of (DITS)-I-2 is also presented.																	1524-9050					DEC	2011	12	4					1624	1639		10.1109/TITS.2011.2158001						WOS:000297588500061		J	Huang, W; Nakamori, Y; Wang, SY				Huang, W; Nakamori, Y; Wang, SY			Forecasting stock market movement direction with support vector machine	COMPUTERS & OPERATIONS RESEARCH												Support vector machine (SVM) is a very specific type of learning algorithms characterized by the capacity control of the decision function, the use of the kernel functions and the sparsity of the solution. In this paper, we investigate the predictability of financial movement direction with SVM by forecasting the weekly movement direction of NIKKEI 225 index. To evaluate the forecasting ability of SVM, we compare its performance with those of Linear Discriminant Analysis, Quadratic Discriminant Analysis and Elman Backpropagation Neural Networks. The experiment results show that SVM outperforms the other classification methods. Further, we propose a combining model by integrating SVM with the other classification methods. The combining model performs best among all the forecasting methods. (c) 2004 Elsevier Ltd. All rights reserved.																	0305-0548					OCT	2005	32	10					2513	2522		10.1016/j.cor.2004.03.016						WOS:000228207700003		J	Spear, SF; Balkenhol, N; Fortin, MJ; McRae, BH; Scribner, K				Spear, Stephen F.; Balkenhol, Niko; Fortin, Marie-Josee; McRae, Brad H.; Scribner, Kim			Use of resistance surfaces for landscape genetic studies: considerations for parameterization and analysis	MOLECULAR ECOLOGY												Measures of genetic structure among individuals or populations collected at different spatial locations across a landscape are commonly used as surrogate measures of functional (i.e. demographic or genetic) connectivity. In order to understand how landscape characteristics influence functional connectivity, resistance surfaces are typically created in a raster GIS environment. These resistance surfaces represent hypothesized relationships between landscape features and gene flow, and are based on underlying biological functions such as relative abundance or movement probabilities in different land cover types. The biggest challenge for calculating resistance surfaces is assignment of resistance values to different landscape features. Here, we first identify study objectives that are consistent with the use of resistance surfaces and critically review the various approaches that have been used to parameterize resistance surfaces and select optimal models in landscape genetics. We then discuss the biological assumptions and considerations that influence analyses using resistance surfaces, such as the relationship between gene flow and dispersal, how habitat suitability may influence animal movement, and how resistance surfaces can be translated into estimates of functional landscape connectivity. Finally, we outline novel approaches for creating optimal resistance surfaces using either simulation or computational methods, as well as alternatives to resistance surfaces (e.g. network and buffered paths). These approaches have the potential to improve landscape genetic analyses, but they also create new challenges. We conclude that no single way of using resistance surfaces is appropriate for every situation. We suggest that researchers carefully consider objectives, important biological assumptions and available parameterization and validation techniques when planning landscape genetic studies.				Balkenhol, Niko/G-2725-2015													0962-1083					SEP	2010	19	17			SI		3576	3591		10.1111/j.1365-294X.2010.04657.x						WOS:000281285200007	20723064	J	Bifet, A; Holmes, G; Kirkby, R; Pfahringer, B				Bifet, Albert; Holmes, Geoff; Kirkby, Richard; Pfahringer, Bernhard			MOA: Massive Online Analysis	JOURNAL OF MACHINE LEARNING RESEARCH												Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Naive Bayes classifiers at the leaves. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis, and is released under the GNU GPL license.				Bifet, Albert/E-4984-2017	Bifet, Albert/0000-0002-8339-7773; Holmes, Geoffrey/0000-0003-0433-8925												1532-4435					MAY	2010	11						1601	1604								WOS:000282522000001		J	LaConte, S; Strother, S; Cherkassky, V; Anderson, J; Hu, XP				LaConte, S; Strother, S; Cherkassky, V; Anderson, J; Hu, XP			Support vector machines for temporal classification of block design fMRI data	NEUROIMAGE												This paper treats support vector machine (SVM) classification applied to block design fMRI, extending our previous work with linear discriminant analysis [LaConte, S., Anderson, J., Muley, S., Ashe, J., Frutiger, S., Rehm, K., Hansen, L.K., Yacoub, E., Hu, X., Rottenberg, D., Strother, S., 2003a. The evaluation of preprocessing choices in single-subject BOLD fMRI using NPAIRS performance metrics. NeuroImage 18, 1027; Strother, S.C., Anderson, J., Hansen, L.K., Kjems, U., Kustra, R., Siditis, J., Frutiger, S., Muley, S., LaConte, S., Rottenberg, D., 2002. The quantitative evaluation of functional neuroimaging experiments: the NPAIRS data analysis framework. NeuroImage 15, 747-771]. We compare SVM to canonical variates analysis (CVA) by examining the relative sensitivity of each method to ten combinations of preprocessing choices consisting of spatial smoothing, temporal detrending, and motion correction. Important to the discussion are the issues of classification performance, model interpretation, and validation in the context of fMRI. As the SVM has many unique properties, we examine the interpretation of support vector models with respect to neuroimaging data. We propose four methods for extracting activation maps from SVM models, and we examine one of these in detail. For both CVA and SVM, we have classified individual time samples of whole brain data, with TRs of roughly 4 s, thirty slices, and nearly 30,000 brain voxels, with no averaging of scans or prior feature selection. (c) 2005 Elsevier Inc. All rights reserved.					Strother, Stephen/0000-0002-3198-217X												1053-8119					JUN	2005	26	2					317	329		10.1016/j.neuroimage.2005.01.048						WOS:000229472100001	15907293	J	van Gestel, T; Suykens, JAK; Baesens, B; Viaene, S; Vanthienen, J; Dedene, G; de Moor, B; Vandewalle, J				van Gestel, T; Suykens, JAK; Baesens, B; Viaene, S; Vanthienen, J; Dedene, G; de Moor, B; Vandewalle, J			Benchmarking least squares support vector machine classifiers	MACHINE LEARNING												In Support Vector Machines (SVMs), the solution of the classification problem is characterized by a ( convex) quadratic programming (QP) problem. In a modified version of SVMs, called Least Squares SVM classifiers (LS-SVMs), a least squares cost function is proposed so as to obtain a linear set of equations in the dual space. While the SVM classifier has a large margin interpretation, the LS-SVM formulation is related in this paper to a ridge regression approach for classification with binary targets and to Fisher's linear discriminant analysis in the feature space. Multiclass categorization problems are represented by a set of binary classifiers using different output coding schemes. While regularization is used to control the effective number of parameters of the LS-SVM classifier, the sparseness property of SVMs is lost due to the choice of the 2-norm. Sparseness can be imposed in a second stage by gradually pruning the support value spectrum and optimizing the hyperparameters during the sparse approximation procedure. In this paper, twenty public domain benchmark datasets are used to evaluate the test set performance of LS-SVM classifiers with linear, polynomial and radial basis function (RBF) kernels. Both the SVM and LS-SVM classifier with RBF kernel in combination with standard cross-validation procedures for hyperparameter selection achieve comparable test set performances. These SVM and LS-SVM performances are consistently very good when compared to a variety of methods described in the literature including decision tree based algorithms, statistical algorithms and instance based learning methods. We show on ten UCI datasets that the LS-SVM sparse approximation procedure can be successfully applied.				Viaene, Stijn/C-2981-2009; Suykens, Johan/C-9781-2014													0885-6125					JAN	2004	54	1					5	32		10.1023/B:MACH.0000008082.80494.e0						WOS:000187280200001		J	Chang, E; Goh, K; Sychay, G; Wu, G				Chang, E; Goh, K; Sychay, G; Wu, G			CBSA: Content-based soft annotation for multimodal image retrieval using Bayes point machines	IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY												We propose a content-based soft annotation (CBSA) procedure for providing images with semantical labels. The annotation procedure starts with labeling a small set of training images, each with one single semantical label (e.g., forest, animal, or sky). An ensemble of binary classifiers is then trained for predicting label membership for images. The trained ensemble is applied to each individual image to give the image multiple soft labels, and each label is associated with a label membership factor. To select a base binary-classifier for CBSA, we experiment with two learning methods, support vector machines (SVMs) and Bayes point machines (BPMs), and compare their class-prediction accuracy. Our empirical study on a 116-category 25K-image set shows that the BPM-based ensemble provides better annotation quality than the SVM-based ensemble for supporting multimodal image retrievals.																	1051-8215	1558-2205				JAN	2003	13	1					26	38		10.1109/TCSVT.2002.808079						WOS:000181131800004		J	Wellendorff, J; Lundgaard, KT; Mogelhoj, A; Petzold, V; Landis, DD; Norskov, JK; Bligaard, T; Jacobsen, KW				Wellendorff, Jess; Lundgaard, Keld T.; Mogelhoj, Andreas; Petzold, Vivien; Landis, David D.; Norskov, Jens K.; Bligaard, Thomas; Jacobsen, Karsten W.			Density functionals for surface science: Exchange-correlation model development with Bayesian error estimation	PHYSICAL REVIEW B												A methodology for semiempirical density functional optimization, using regularization and cross-validation methods from machine learning, is developed. We demonstrate that such methods enable well-behaved exchange-correlation approximations in very flexible model spaces, thus avoiding the overfitting found when standard least-squares methods are applied to high-order polynomial expansions. A general-purpose density functional for surface science and catalysis studies should accurately describe bond breaking and formation in chemistry, solid state physics, and surface chemistry, and should preferably also include van der Waals dispersion interactions. Such a functional necessarily compromises between describing fundamentally different types of interactions, making transferability of the density functional approximation a key issue. We investigate this trade-off between describing the energetics of intramolecular and intermolecular, bulk solid, and surface chemical bonding, and the developed optimization method explicitly handles making the compromise based on the directions in model space favored by different materials properties. The approach is applied to designing the Bayesian error estimation functional with van der Waals correlation (BEEF-vdW), a semilocal approximation with an additional nonlocal correlation term. Furthermore, an ensemble of functionals around BEEF-vdW comes out naturally, offering an estimate of the computational error. An extensive assessment on a range of data sets validates the applicability of BEEF-vdW to studies in chemistry and condensed matter physics. Applications of the approximation and its Bayesian ensemble error estimate to two intricate surface science problems support this.				Jacobsen, Karsten/B-3602-2009; Bligaard, Thomas/A-6161-2011; Norskov, Jens/D-2539-2017	Jacobsen, Karsten/0000-0002-1121-2979; Bligaard, Thomas/0000-0001-9834-9179; Norskov, Jens/0000-0002-4427-7728												2469-9950	2469-9969				JUN 27	2012	85	23							235149	10.1103/PhysRevB.85.235149						WOS:000305739800001		J	Ernst, MD; Perkins, JH; Guo, PJ; McCarnant, S; Pacheco, C; Tschantz, MS; Xiao, C				Ernst, Michael D.; Perkins, Jeff H.; Guo, Philip J.; McCarnant, Stephen; Pacheco, Carlos; Tschantz, Matthew S.; Xiao, Chen			The Daikon system for dynamic detection of likely invariants	SCIENCE OF COMPUTER PROGRAMMING												Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often used in assert statements, documentation, and formal specifications. Examples include being constant (x = a), non-zero (x not equal 0), being in a range (a <= x <= b), linear relationships (y = ax + b), ordering (x <= y), functions from a library (x = fn(y)), containment (x epsilon y), sortedness (x is sorted), and many more. Users can extend Daikon to check for additional invariants. Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data. Daikon can detect invariants in C, C + +, Java, and Perl programs, and in record-structured data sources; it is easy to extend Daikon to other applications. Invariants can be useful in program understanding and a host of other applications. Daikon's output has been used for generating test cases, predicting incompatibilities in component integration, automating theorem proving, repairing inconsistent data structures, and checking the validity of data streams, among other tasks. Daikon is freely available in source and binary form, along with extensive documentation, at http://pag.csaii.mit.edu/daikon/. (c) 2007 Elsevier B.V. All rights reserved.																	0167-6423					DEC 1	2007	69	1-3					35	45		10.1016/j.scico.2007.01.015						WOS:000252496700005		J	Verstraeten, D; Schrauwen, B; D'Haene, M; Stroobandt, D				Verstraeten, D.; Schrauwen, B.; D'Haene, M.; Stroobandt, D.			An experimental unification of reservoir computing methods	NEURAL NETWORKS												Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks. (c) 2007 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2007	20	3					391	403		10.1016/j.neunet.2007.04.003						WOS:000247684600010	17517492	J	Grochow, K; Martin, SL; Hertzmann, A; Popovic, Z				Grochow, K; Martin, SL; Hertzmann, A; Popovic, Z			Style-based inverse kinematics	ACM TRANSACTIONS ON GRAPHICS					Annual Symposium of the ACM SIGGRAPH	AUG 27-29, 2004	Grenoble, FRANCE	ACM SIGGRAPH				This paper presents an inverse kinematics system based on a learned model of human poses. Given a set of constraints, our system can produce the most likely pose satisfying those constraints, in real-time. Training the model on different input data leads to different styles of IK. The model is represented as a probability distribution over the space of all possible poses. This means that our IK system can generate any pose, but prefers poses that are most similar to the space of poses in the training data. We represent the probability with a novel model called a Scaled Gaussian Process Latent Variable Model. The parameters of the model are all learned automatically; no manual tuning is required for the learning component of the system. We additionally describe a novel procedure for interpolating between styles. Our style-based IK can replace conventional IK, wherever it is used in computer animation and computer vision. We demonstrate our system in the context of a number of applications: interactive character posing, trajectory keyframing, real-time motion capture with missing markers, and posing from a 2D image.																	0730-0301	1557-7368				AUG	2004	23	3					522	531		10.1145/1015706.1015755						WOS:000222972600043		J	Helmstaedter, M; Briggman, KL; Turaga, SC; Jain, V; Seung, HS; Denk, W				Helmstaedter, Moritz; Briggman, Kevin L.; Turaga, Srinivas C.; Jain, Viren; Seung, H. Sebastian; Denk, Winfried			Connectomic reconstruction of the inner plexiform layer in the mouse retina	NATURE												Comprehensive high-resolution structural maps are central to functional exploration and understanding in biology. For the nervous system, in which high resolution and large spatial extent are both needed, such maps are scarce as they challenge data acquisition and analysis capabilities. Here we present for the mouse inner plexiform layer-the main computational neuropil region in the mammalian retina-the dense reconstruction of 950 neurons and their mutual contacts. This was achieved by applying a combination of crowd-sourced manual annotation and machine-learning-based volume segmentation to serial block-face electron microscopy data. We characterize a new type of retinal bipolar interneuron and show that we can subdivide a known type based on connectivity. Circuit motifs that emerge from our data indicate a functional mechanism for a known cellular response in a ganglion cell that detects localized motion, and predict that another ganglion cell is motion sensitive.					Turaga, Srinivas/0000-0003-3247-6487												0028-0836					AUG 8	2013	500	7461					168	+		10.1038/nature12346						WOS:000322825500027	23925239	J	Saha, B; Goebel, K; Poll, S; Christophersen, J				Saha, Bhaskar; Goebel, Kai; Poll, Scott; Christophersen, Jon			Prognostics Methods for Battery Health Monitoring Using a Bayesian Framework	IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT					42nd Annual AUTOTESTCON Conference	SEP 17-20, 2007	Baltimore, MD	IEEE Aerosp & Elect Syst Soc, IEEE Instrumentat & Measurement Soc				This paper explores how the remaining useful life, (RUL) can be assessed for complex systems whose internal state variables are either inaccessible to sensors or hard to measure tinder operational conditions. Consequently, inference and estimation techniques need to he applied on indirect measurements, anticipated operational conditions, and historical data for which a Bayesian statistical approach is suitable. Models or electrochemical processes in the form of equivalent electric circuit parameters were combined with statistical models of state transitions, aging processes, and measurement fidelity in a formal framework. Relevance vector machines (RVMs) and several different particle filters (PFs) are examined for remaining life prediction and for providing uncertainty hounds. Results are shown on battery data.(1)																	0018-9456					FEB	2009	58	2					291	296		10.1109/TIM.2008.2005965						WOS:000262428800008		J	Hu, QH; Yu, DR; Xie, ZX				Hu, QH; Yu, DR; Xie, ZX			Information-preserving hybrid data reduction based on fuzzy-rough techniques	PATTERN RECOGNITION LETTERS												Data reduction plays an important role in machine learning and pattern recognition with a high-dimensional data. In real-world applications data usually exists with hybrid formats, and it unified data reducing technique for hybrid data is desirable. In this paper, an information measure is proposed to computing discernibility power of a crisp equivalence relation or a fuzzy one,. which is the key concept in classical rough set model and fuzzy-rough set model. Based on the information measure, a general definition of significance of nominal, numeric and fuzzy attributes is presented. We redefine the independence of hybrid attribute subset, reduct, and relative reduct. Then two greedy reduction algorithms for unsupervised and supervised data dimensionality reduction based on the proposed information measure are constructed. Experiments show the reducts found by the proposed algorithms get a better performance compared with classical rough set approaches. (c) 2005 Elsevier B.V. All rights reserved.				Hu, Qinghua/B-8857-2008													0167-8655	1872-7344				APR 1	2006	27	5					414	423		10.1016/patrec.2005.09.004						WOS:000235862300011		J	Kushmerick, N				Kushmerick, N			Wrapper induction: Efficiency and expressiveness	ARTIFICIAL INTELLIGENCE												The Internet presents numerous sources of useful information-telephone directories, product catalogs, stock quotes, event listings, etc. Recently, many systems have been built that automatically gather and manipulate such information on a user's behalf. However, these resources are usually formatted for use by people (e.g., the relevant content is embedded in HTML pages), so extracting their content is difficult. Most systems use customized wrapper procedures to perform this extraction task. Unfortunately, writing wrappers is tedious and error-prone. As an alternative, we advocate wrapper induction, a technique for automatically constructing wrappers. In this article, we describe six wrapper classes, and use a combination of empirical and analytical techniques to evaluate the computational tradeoffs among them. We first consider expressiveness: how well the classes can handle actual Internet resources, and the extent to which wrappers in one class can mimic those in another. We then turn to efficiency: we measure the number of examples and time required to learn wrappers in each class, and we compare these results to PAC models of our task and asymptotic complexity analyses of our algorithms. Summarizing our results, we find that most of our wrapper classes are reasonably useful (70% of surveyed sites can be handled in total), yet can rapidly learned (learning usually requires just a handful of examples and a fraction of a CPU second per example). (C) 2000 Elsevier Science B.V. All rights reserved.																	0004-3702	1872-7921				APR	2000	118	1-2					15	68		10.1016/S0004-3702(99)00100-9						WOS:000086736700002		J	Guo, GD; Fu, Y; Dyer, CR; Huang, TS				Guo, Guodong; Fu, Yun; Dyer, Charles R.; Huang, Thomas S.			Image-based human age estimation by manifold learning and locally adjusted robust regression	IEEE TRANSACTIONS ON IMAGE PROCESSING												Estimating human age automatically via facial image analysis has lots of potential real-world applications, such as human computer interaction and multimedia communication. However, it is still a challenging problem for the existing computer vision systems to automatically and effectively estimate human ages. The aging process is determined by not only the person's gene, but also many external factors, such as health, living style, living location, and weather conditions. Males and females may also age differently. The current age estimation performance is still not good enough for practical use and more effort has to be put into this research direction. In this paper, we introduce the age manifold learning scheme for extracting face aging features and design a locally adjusted robust regressor for learning and prediction of human ages. The novel approach improves the age estimation accuracy significantly over all previous methods. The merit of the proposed approaches for image-based age estimation is shown by extensive experiments on a large internal age database and the public available FG-NET database.					Guo, Guodong/0000-0001-9583-0055												1057-7149					JUL	2008	17	7					1178	1188		10.1109/TIP.2008.924280						WOS:000256885700013	18586625	J	Bordes, A; Ertekin, S; Weston, J; Bottou, L				Bordes, A; Ertekin, S; Weston, J; Bottou, L			Fast kernel classifiers with online and active learning	JOURNAL OF MACHINE LEARNING RESEARCH												Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efficient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We first present an online SVM algorithm based on this premise. LASVM yields competitive misclassification rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.				Ertekin, Seyda/N-9066-2013													1532-4435					SEP	2005	6						1579	1619								WOS:000236330100012		J	Morin, RD; Aksay, G; Dolgosheina, E; Ebhardt, HA; Magrini, V; Mardis, ER; Sahinalp, SC; Unrau, PJ				Morin, Ryan D.; Aksay, Gozde; Dolgosheina, Elena; Ebhardt, H. Alexander; Magrini, Vincent; Mardis, Elaine R.; Sahinalp, S. Cenk; Unrau, Peter J.			Comparative analysis of the small RNA transcriptomes of Pinus contorta and Oryza sativa	GENOME RESEARCH												The diversity of microRNAs and small-interfering RNAs has been extensively explored within angiosperms by focusing on a few key organisms such as Oryza sativa and Arabidopsis thaliana. A deeper division of the plants is defined by the radiation of the angiosperms and gymnosperms, with the latter comprising the commercially important conifers. The conifers are expected to provide important information regarding the evolution of highly conserved small regulatory RNAs. Deep sequencing provides the means to characterize and quantitatively profile small RNAs in understudied organisms such as these. Pyrosequencing of small RNAs from O. sativa revealed, as expected, similar to 21- and similar to 24-nt RNAs. The former contained known microRNAs, and the latter largely comprised intergenic-derived sequences likely representing heterochromatin siRNAs. In contrast, sequences from Pinus contorta were dominated by 21-nt small RNAs. Using a novel sequence-based clustering algorithm, we identified sequences belonging to 18 highly conserved microRNA families in P. contorta as well as numerous clusters of conserved small RNAs of unknown function. Using multiple methods, including expressed sequence folding and machine learning algorithms, we found a further 53 candidate novel microRNA families, 51 appearing specific to the P. contorta library. In addition, alignment of small RNA sequences to the O. sativa genome revealed six perfectly conserved classes of small RNA that included chloroplast transcripts and specific types of genomic repeats. The conservation of microRNAs and other small RNAs between the conifers and the angiosperms indicates that important RNA silencing processes were highly developed in the earliest spermatophytes. Genomic mapping of all sequences to the O. sativa genome can be viewed at http://microrna.bcgsc.ca/cgi-bin/gbrowse/rice_build_3/.					Ebhardt, H. Alexander/0000-0001-9019-8388												1088-9051					APR	2008	18	4					571	584		10.1101/gr.6897308						WOS:000254562400007	18323537	J	Soon, WM; Ng, HT; Lim, DCY				Soon, WM; Ng, HT; Lim, DCY			A machine learning approach to coreference resolution of noun phrases	COMPUTATIONAL LINGUISTICS												In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e. g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of "organization," "person," or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.																	0891-2017	1530-9312				DEC	2001	27	4					521	544		10.1162/089120101753342653						WOS:000173575800004		S	Cauwenberghs, G; Poggio, T		Leen, TK; Dietterich, TG; Tresp, V		Cauwenberghs, G; Poggio, T			Incremental and decremental support vector machine learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS				14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					An on-line recursive algorithm for training support vector machines, one vector at a time, is Presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental "unlearning" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data.																	1049-5258		0-262-12241-3				2001	13						409	415								WOS:000171891800058		J	De Martino, F; Valente, G; Staeren, N; Ashburner, J; Goebel, R; Formisano, E				De Martino, Federico; Valente, Giancarlo; Staeren, Noel; Ashburner, John; Goebel, Rainer; Formisano, Elia			Combining multivariate voxel selection and support vector machines for mapping and classification of fMRI spatial patterns	NEUROIMAGE												In functional brain mapping, pattern recognition methods allow detecting multivoxel patterns of brain activation which are informative with respect to a subject's perceptual or cognitive state. The sensitivity of these methods, however, is greatly reduced when the proportion of voxels that convey the discriminative information is small compared to the total number of measured voxels. To reduce this dimensionality problem, previous studies employed univariate voxel selection or region-of-interest-based strategies as a preceding step to the application of machine learning algorithms. Here we employ a strategy for classifying functional imaging data based on a multivariate feature selection algorithm, Recursive Feature Elimination (RFE) that uses the training algorithm (support vector machine) recursively to eliminate irrelevant voxels and estimate informative spatial patterns. Generalization performances on test data increases while features/voxels are pruned based on their discrimination ability. In this article we evaluate RFE in terms of sensitivity of discriminative maps (Receiver Operative Characteristic analysis) and generalization performances and compare it to previously used univariate voxel selection strategies based on activation and discrimination measures. Using simulated fMRI data, we show that the recursive approach is suitable for mapping discriminative patterns and that the combination of an initial univariate activation-based (F-test) reduction of voxels and multivariate recursive feature elimination produces the best results, especially when differences between conditions have a low contrast-to-noise ratio. Furthermore, we apply our method to high resolution (2 x 2 x 2mm(3)) data from an auditory fMRI experiment in which subjects were stimulated with sounds from four different categories. With these real data, our recursive algorithm proves able to detect and accurately classify multivoxel spatial patterns, highlighting the role of the superior temporal gyrus in encoding the information of sound categories. In line with the simulation results, our method outperforms univariate statistical analysis and statistical learning without feature selection. (C) 2008 Elsevier Inc. All rights reserved.				De Martino, Federico/I-1817-2012; Ashburner, John/I-3757-2013	Ashburner, John/0000-0001-7605-2518; De Martino, Federico/0000-0002-0352-0648												1053-8119					OCT 15	2008	43	1					44	58		10.1016/j.neuroimage.2008.06.037						WOS:000259927400006	18672070	J	Ishibuchi, H; Nojima, Y				Ishibuchi, Hisao; Nojima, Yusuke			Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING					1st International Workshop on Genetic Fuzzy Systems	MAR 17-19, 2005	Granada, SPAIN					This paper examines the interpretability-accuracy tradeoff in fuzzy rule-based classifiers using a multiobjective fuzzy genetics-based machine learning (GBML) algorithm. Our GBML algorithm is a hybrid version of Michigan and Pittsburgh approaches, which is implemented in the framework of evolutionary multiobjective optimization (EMO). Each fuzzy rule is represented by its antecedent fuzzy sets as an integer string of fixed length. Each fuzzy rule-based classifier, which is a set of fuzzy rules, is represented as a concatenated integer string of variable length. Our GBML algorithm simultaneously maximizes the accuracy of rule sets and minimizes their complexity. The accuracy is measured by the number of correctly classified training patterns while the complexity is measured by the number of fuzzy rules and/or the total number of antecedent conditions of fuzzy rules. We examine the in terpretability-accuracy tradeoff for training patterns through computational experiments on some benchmark data sets. A clear tradeoff structure is visualized for each data set. We also examine the interpretabitity-accuracy tradeoff for test patterns. Due to the overfitting to training patterns, a clear tradeoff structure is not always obtained in computational experiments for test patterns. (C) 2006 Elsevier Inc. All rights reserved.				Ishibuchi, Hisao/B-3599-2009; Nojima, Yusuke/F-4832-2010	Ishibuchi, Hisao/0000-0001-9186-6472; 												0888-613X	1873-4731				JAN	2007	44	1					4	31		10.1016/j.ijar.2006.01.004						WOS:000243807500002		J	Garcia, C; Delakis, M				Garcia, C; Delakis, M			Convolutional face finder: A neural architecture for fast and robust face detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												In this paper, we present a novel face detection approach based on a convolutional neural architecture, designed to robustly detect highly variable face patterns, rotated up to +/-20 degrees in image plane and turned up to 9+/-60 degrees, in complex real world images. The proposed system automatically synthesizes simple problem-specific feature extractors from a training set of face and nonface patterns, without making any assumptions or using any hand-made design concerning the features to extract or the areas of the face pattern to analyze. The face detection procedure acts like a pipeline of simple convolution and subsampling modules that treat the raw input image as a whole. We therefore show that an efficient face detection system does not require any costly local preprocessing before classification of image areas. The proposed scheme provides very high detection rate with a particularly low level of false positives, demonstrated on difficult test sets, without requiring the use of multiple networks for handling difficult cases. We present extensive experimental results illustrating the efficiency of the proposed approach on difficult test sets and including an indepth sensitivity analysis with respect to the degrees of variability of the face patterns.					Garcia, christophe/0000-0001-7997-9837												0162-8828					NOV	2004	26	11					1408	1423		10.1109/TPAMI.2004.97						WOS:000223737000002	15521490	J	Cheng, TCE; Wang, GQ				Cheng, TCE; Wang, GQ			Single machine scheduling with learning effect considerations	ANNALS OF OPERATIONS RESEARCH					4th International Conference on Optimisation Techniques and Applications	JUL 01-03, 1998	CURTIN UNIV TECHNOL, PERTH, AUSTRALIA		CURTIN UNIV TECHNOL			In this paper we study a single machine scheduling problem in which the job processing times will decrease as a result of learning. A volume-dependent piecewise linear processing time function is used to model the learning effects. The objective is to minimize the maximum lateness. We first show that the problem is NP-hard in the strong sense and then identify two special cases which are polynomially solvable. We also propose two heuristics and analyse their worst-case performance.					Cheng, Edwin/0000-0001-5127-6419												0254-5330						2000	98						273	290		10.1023/A:1019216726076						WOS:000169227200015		J	Han, B; Liu, YF; Ginzinger, SW; Wishart, DS				Han, Beomsoo; Liu, Yifeng; Ginzinger, Simon W.; Wishart, David S.			SHIFTX2: significantly improved protein chemical shift prediction	JOURNAL OF BIOMOLECULAR NMR												A new computer program, called SHIFTX2, is described which is capable of rapidly and accurately calculating diamagnetic H-1, C-13 and N-15 chemical shifts from protein coordinate data. Compared to its predecessor (SHIFTX) and to other existing protein chemical shift prediction programs, SHIFTX2 is substantially more accurate (up to 26% better by correlation coefficient with an RMS error that is up to 3.3x smaller) than the next best performing program. It also provides significantly more coverage (up to 10% more), is significantly faster (up to 8.5x) and capable of calculating a wider variety of backbone and side chain chemical shifts (up to 6x) than many other shift predictors. In particular, SHIFTX2 is able to attain correlation coefficients between experimentally observed and predicted backbone chemical shifts of 0.9800 (N-15), 0.9959 (C-13 alpha), 0.9992 (C-13 beta), 0.9676 (C-13'), 0.9714 ((HN)-H-1), 0.9744 (H-1 alpha) and RMS errors of 1.1169, 0.4412, 0.5163, 0.5330, 0.1711, and 0.1231 ppm, respectively. The correlation between SHIFTX2's predicted and observed side chain chemical shifts is 0.9787 (C-13) and 0.9482 (H-1) with RMS errors of 0.9754 and 0.1723 ppm, respectively. SHIFTX2 is able to achieve such a high level of accuracy by using a large, high quality database of training proteins (> 190), by utilizing advanced machine learning techniques, by incorporating many more features (chi(2) and chi(3) angles, solvent accessibility, H-bond geometry, pH, temperature), and by combining sequence-based with structure-based chemical shift prediction techniques. With this substantial improvement in accuracy we believe that SHIFTX2 will open the door to many long-anticipated applications of chemical shift prediction to protein structure determination, refinement and validation. SHIFTX2 is available both as a standalone program and as a web server (http://www.shiftx2.ca).					Wishart, David S/0000-0002-3207-2434												0925-2738					MAY	2011	50	1					43	57		10.1007/s10858-011-9478-4						WOS:000290044400005	21448735	J	Fernandez-Delgado, M; Cernadas, E; Barro, S; Amorim, D				Fernandez-Delgado, Manuel; Cernadas, Eva; Barro, Senen; Amorim, Dinani			Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?	JOURNAL OF MACHINE LEARNING RESEARCH												We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearest-neighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Mat lab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1% of the maximum accuracy overcoming 90% in the 84.3% of the data sets. However, the difference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classifiers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).																	1532-4435					OCT	2014	15						3133	3181								WOS:000344638800010		J	Domingos, P				Domingos, Pedro			A Few Useful Things to Know About Machine Learning	COMMUNICATIONS OF THE ACM																													0001-0782	1557-7317				OCT	2012	55	10					78	87		10.1145/2347736.2347755						WOS:000309215800024		J	Zhang, Y				Zhang, Yang			I-TASSER: Fully automated protein structure prediction in CASP8	PROTEINS-STRUCTURE FUNCTION AND BIOINFORMATICS												The I-TASSER algorithm for 3D protein structure prediction was tested in CASP8, with the procedure fully automated in both the Server and Human sections. The quality of the server models is close to that of human ones but the human predictions incorporate more diverse templates from other servers which improve the human predictions in some of the distant homology targets. For the first time, the sequence-based contact predictions from machine learning techniques are found helpful for both template-based modeling (TBM) and template-free modeling (FM). In TBM, although the accuracy of the sequence based contact predictions is on average lower than that from template-based ones, the novel contacts in the sequence-based predictions, which are complementary to the threading templates in the weakly or unaligned regions, are important to improve the global and local packing in these regions. Moreover, the newly developed atomic structural refinement algorithm was tested in CASP8 and found to improve the hydrogen-bonding networks and the overall TM-score, which is mainly due to its ability of removing steric clashes so that the models can be generated from cluster centroids. Nevertheless, one of the major issues of the I-TASSER pipeline is the model selection where the best models could not be appropriately recognized when the correct templates are detected only by the minority of the threading algorithms. There are also problems related with domain-splitting and mirror image recognition which mainly influences the performance of I-TASSER modeling in the FM-based structure predictions.				YZ, YZ/B-3243-2011													0887-3585						2009	77						100	113		10.1002/prot.22588						WOS:000272244700010	19768687	J	Ji, SH; Dunson, D; Carin, L				Ji, Shihao; Dunson, David; Carin, Lawrence			Multitask Compressive Sensing	IEEE TRANSACTIONS ON SIGNAL PROCESSING												Compressive sensing (CS) is a framework whereby one performs N nonadaptive measurements to constitute a vector upsilon is an element of R(N), with upsilon used to recover an approximation (u) over cap is an element of R(M) to a desired signal u is an element of R(M), with N << M; this is performed under the assumption that a is sparse in the basis represented by the matrix Psi is an element of R(MxM). It has been demonstrated that with appropriate design of the compressive measurements used to define upsilon, the decompressive mapping upsilon -> (u) over cap may be performed with error parallel to u - (u) over cap parallel to(2)(2) having asymptotic properties analogous to those of the best adaptive transform-coding algorithm applied in the basis T. The mapping upsilon -> (u) over cap constitutes an inverse problem, often solved using l(1) regularization or related techniques. In most previous research, if L > 1 sets of compressive measurements {upsilon(i)}(i)=1, L are performed, each of the associated {(u) over cap (i)}(i)=1, L are recovered one at a time, independently. In many applications the L "tasks" defined by the mappings upsilon(i) -> (u) over cap (i) are not statistically independent, and it may be possible to improve the performance of the inversion if statistical interrelationships are exploited. In this paper, we address this problem within a multitask learning setting, wherein the mapping v; - u; for each task corresponds to inferring the parameters (here, wavelet coefficients) associated with the desired signal u;, and a shared prior is placed across all of the L tasks. Under this hierarchical Bayesian modeling, data from all L tasks contribute toward inferring a posterior on the hyperparameters, and once the shared prior is thereby inferred, the data from each of the L individual tasks is then employed to estimate the task-dependent wavelet coefficients. An empirical Bayesian procedure for the estimation of hyperparameters is considered; two fast inference algorithms extending the relevance vector machine (RVM) are developed. Example results on several data sets demonstrate the effectiveness and robustness of the proposed algorithms.																	1053-587X					JAN	2009	57	1					92	106		10.1109/TSP.2008.2005866						WOS:000262557500009		J	Brunato, M; Battiti, R				Brunato, M; Battiti, R			Statistical learning theory for location fingerprinting in wireless LANs	COMPUTER NETWORKS												In this paper, techniques and algorithms developed in the framework of Statistical Learning Theory are applied to the problem of determining the location of a wireless device by measuring the signal strength values from a set of access points (location fingerprinting). Statistical Learning Theory provides a rich theoretical basis for the development of models starting from a set of examples. Signal strength measurement is part of the normal operating mode of wireless equipment, in particular Wi-Fi, so that no special-purpose hardware is required. The proposed techniques, based on the Support Vector Machine paradigm, have been implemented and compared, on the same data set, with other approaches considered in scientific literature. Tests performed in a real-world environment show that results are comparable, with the advantage of a low algorithmic complexity in the normal operating phase. Moreover, the algorithm is particularly suitable for classification, where it outperforms the other techniques. (c) 2004 Elsevier B.V. All rights reserved.				Battiti, Roberto/C-2108-2009													1389-1286	1872-7069				APR 22	2005	47	6					825	845		10.1016/j.comnet.2004.09.004						WOS:000228021300003		J	Mosheiov, G; Sidney, JB				Mosheiov, G; Sidney, JB			Scheduling with general job-dependent learning curves	EUROPEAN JOURNAL OF OPERATIONAL RESEARCH												Several recent papers focused on the effect of learning on the optimal solution of scheduling problems. We extend the setting studied so far to the case of job-dependent learning curves, that is, we allow the learning in the production process of some jobs to be faster than that of others. Our learning curve approach, which assumes learning takes place as a function of repetition of the production process, is otherwise completely general, and is not based upon any particular model of learning acquisition. We show that in the new, possibly more realistic setting, the problems of makespan and total flow-time minimization on a single machine, a due-date assignment problem and total flow-time minimization on unrelated parallel machines remain polynomially solvable. (C) 2002 Elsevier Science B.V. All rights reserved.																	0377-2217					JUN 16	2003	147	3					665	670		10.1016/S0377-2217(02)00358-2						WOS:000181578300016		J	Ishibuchi, H; Nakashima, T; Murata, T				Ishibuchi, H; Nakashima, T; Murata, T			Three-objective genetics-based machine learning for linguistic rule extraction	INFORMATION SCIENCES					8th International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU 2000)	JUL 03-07, 2000	MADRID, SPAIN	Univ Politecn				This paper shows how a small number of linguistically interpretable fuzzy rules can be extracted from numerical data for high-dimensional pattern classification problems. One difficulty in the handling of high-dimensional problems by fuzzy rule-based systems is the exponential increase in the number of fuzzy rules with the number of input variables. Another difficulty is the deterioration in the comprehensibility of fuzzy rules when they involve many antecedent conditions. Our task is to design comprehensible fuzzy rule-based systems with high classification ability. This task is formulated as a combinatorial optimization problem with three objectives: to maximize the number of correctly classified training patterns, to minimize the number of fuzzy rules, and to minimize the total number of antecedent conditions. We show two genetic-algorithm-based approaches. One is rule selection where a small number of linguistically interpretable fuzzy rules are selected from a large number of prespecified candidate rules. The other is fuzzy genetics-based machine learning where rule sets are evolved by genetic operations. These two approaches search for non-dominated rule sets with respect to the three objectives. (C) 2001 Elsevier Science Inc. All rights reserved.				Ishibuchi, Hisao/B-3599-2009	Ishibuchi, Hisao/0000-0001-9186-6472												0020-0255	1872-6291				AUG	2001	136	1-4			SI		109	133		10.1016/S0020-0255(01)00144-X						WOS:000169958400007		J	Jensen, R; Shen, Q				Jensen, Richard; Shen, Qiang			Fuzzy-rough sets assisted attribute selection	IEEE TRANSACTIONS ON FUZZY SYSTEMS												Attribute selection (AS) refers to the problem of selecting those input attributes or features that are most predictive of a given outcome; a problem encountered in many areas such as machine learning, pattern recognition and signal processing. Unlike other dimensionality reduction methods, attribute selectors preserve the original meaning of the attributes after reduction. This has found application in tasks that involve datasets containing huge numbers of attributes (in the order of tens of thousands) which, for some learning algorithms, might be impossible to process further. Recent examples include text processing and web content classification. AS techniques have also been applied to small and medium-sized datasets in order to locate the most informative attributes for later use. One of the many successful applications of rough set theory has been to this area. The rough set ideology of using only the supplied data and no other information has many benefits in AS, where most other methods require supplementary knowledge. However, the main limitation of rough set-based attribute selection in the literature is the restrictive requirement that all data is discrete. In classical rough set theory, it is not possible to consider real-valued or noisy data. This paper investigates a novel approach based on fuzzy-rough sets, fuzzy rough feature selection (FRFS), that addresses these problems and retains dataset semantics. FRFS is applied to two challenging domains where a feature reducing step is important; namely, web content classification and complex systems monitoring. The utility of this approach is demonstrated and is compared empirically with several dimensionality reducers. In the experimental studies, FRFS is shown to equal or improve classification accuracy when compared to the results from unreduced data. Classifiers that use a lower dimensional set of attributes which are retained by fuzzy-rough reduction outperform those that employ more attributes returned by the existing crisp rough reduction method. In addition, it is shown that FRFS is more powerful than the other AS techniques in the comparative study.																	1063-6706	1941-0034				FEB	2007	15	1					73	89		10.1109/TFUZZ.2006.889761						WOS:000244803400007		S	Duygulu, P; Barnard, K; de Freitas, JFG; Forsyth, DA		Heyden, A; Sparr, G; Nielsen, M; Johansen, P		Duygulu, P; Barnard, K; de Freitas, JFG; Forsyth, DA			Object recognition as machine translation: Learning a lexicon for a fixed imago vocabulary	COMPUTER VISION - ECCV 2002, PT IV	LECTURE NOTES IN COMPUTER SCIENCE				7th European Conference on Computer Vision	MAY 28-31, 2002	COPENHAGEN, DENMARK	IT Univ Copenhagen, Univ Copenhagen, Lund Univ				We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well-for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.				Duygulu, Pinar/N-2707-2013	Duygulu, Pinar/0000-0002-6420-2838												0302-9743		3-540-43748-7				2002	2353						97	112								WOS:000180067300007		J	Babacan, SD; Molina, R; Katsaggelos, AK				Babacan, S. Derin; Molina, Rafael; Katsaggelos, Aggelos K.			Bayesian Compressive Sensing Using Laplace Priors	IEEE TRANSACTIONS ON IMAGE PROCESSING												In this paper, we model the components of the compressive sensing ( CS) problem, i.e., the signal acquisition process, the unknown signal coefficients and the model parameters for the signal and noise using the Bayesian framework. We utilize a hierarchical form of the Laplace prior to model the sparsity of the unknown signal. We describe the relationship among a number of sparsity priors proposed in the literature, and show the advantages of the proposed model including its high degree of sparsity. Moreover, we show that some of the existing models are special cases of the proposed model. Using our model, we develop a constructive ( greedy) algorithm designed for fast reconstruction useful in practical settings. Unlike most existing CS reconstruction methods, the proposed algorithm is fully automated, i.e., the unknown signal coefficients and all necessary parameters are estimated solely from the observation, and, therefore, no user-intervention is needed. Additionally, the proposed algorithm provides estimates of the uncertainty of the reconstructions. We provide experimental results with synthetic 1-D signals and images, and compare with the state-of-the-art CS reconstruction algorithms demonstrating the superior performance of the proposed approach.				Katsaggelos, Aggelos/B-7233-2009; Molina Soriano, Rafael/B-1849-2012; Katsaggelos, Aggelos/I-8002-2012	Molina Soriano, Rafael/0000-0003-4694-8588; 												1057-7149					JAN	2010	19	1					53	63		10.1109/TIP.2009.2032894						WOS:000272844000005	19775966	J	Roth, S; Black, MJ				Roth, Stefan; Black, Michael J.			Fields of Experts	INTERNATIONAL JOURNAL OF COMPUTER VISION												We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach provides a practical method for learning high-order Markov random field (MRF) models with potential functions that extend over large pixel neighborhoods. These clique potentials are modeled using the Product-of-Experts framework that uses non-linear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field-of-Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with specialized techniques.					Roth, Stefan/0000-0001-9002-9832												0920-5691					APR	2009	82	2					205	229		10.1007/s11263-008-0197-6						WOS:000263421200005		J	Guo, GD; Li, SZ				Guo, GD; Li, SZ			Content-based audio classification and retrieval by support vector machines	IEEE TRANSACTIONS ON NEURAL NETWORKS												Support vector machines (SVMs) have been recently proposed as a new learning algorithm for pattern recognition. In this paper, the SVMs with a binary tree recognition strategy are used to tackle the audio classification problem. We illustrate the potential of SVMs on a common audio database, which consists of 409 sounds of 16 classes. We compare the SVMs based classification with other popular approaches. For audio retrieval, we propose a new metric, called distance-from-boundary (DFB). When a query audio is given, the system first finds a boundary inside which the query pattern is located. Then, all the audio patterns in the database are sorted by their distances to this boundary. All boundaries are learned by the SVMs and stored together with the audio database. Experimental comparisons for audio retrieval are presented to show the superiority of this hovel metric to other similarity measures.					Guo, Guodong/0000-0001-9583-0055												1045-9227					JAN	2003	14	1					209	215		10.1109/TNN.2002.806626						WOS:000180862400019	18238003	J	Zhu, WH; Lomsadze, A; Borodovsky, M				Zhu, Wenhan; Lomsadze, Alexandre; Borodovsky, Mark			Ab initio gene identification in metagenomic sequences	NUCLEIC ACIDS RESEARCH												We describe an algorithm for gene identification in DNA sequences derived from shotgun sequencing of microbial communities. Accurate ab initio gene prediction in a short nucleotide sequence of anonymous origin is hampered by uncertainty in model parameters. While several machine learning approaches could be proposed to bypass this difficulty, one effective method is to estimate parameters from dependencies, formed in evolution, between frequencies of oligonucleotides in protein-coding regions and genome nucleotide composition. Original version of the method was proposed in 1999 and has been used since for (i) reconstructing codon frequency vector needed for gene finding in viral genomes and (ii) initializing parameters of self-training gene finding algorithms. With advent of new prokaryotic genomes en masse it became possible to enhance the original approach by using direct polynomial and logistic approximations of oligonucleotide frequencies, as well as by separating models for bacteria and archaea. These advances have increased the accuracy of model reconstruction and, subsequently, gene prediction. We describe the refined method and assess its accuracy on known prokaryotic genomes split into short sequences. Also, we show that as a result of application of the new method, several thousands of new genes could be added to existing annotations of several human and mouse gut metagenomes.																	0305-1048					JUL	2010	38	12							e132	10.1093/nar/gkq275						WOS:000288745700005	20403810	J	Schneider, A; Friedl, MA; Potere, D				Schneider, A.; Friedl, M. A.; Potere, D.			A new map of global urban extent from MODIS satellite data	ENVIRONMENTAL RESEARCH LETTERS												Although only a small percentage of global land cover, urban areas significantly alter climate, biogeochemistry, and hydrology at local, regional, and global scales. To understand the impact of urban areas on these processes, high quality, regularly updated information on the urban environment-including maps that monitor location and extent-is essential. Here we present results from efforts to map the global distribution of urban land use at 500 m spatial resolution using remotely sensed data from the Moderate Resolution Imaging Spectroradiometer (MODIS). Our approach uses a supervised decision tree classification algorithm that we process using region-specific parameters. An accuracy assessment based on sites from a stratified random sample of 140 cities shows that the new map has an overall accuracy of 93% (k = 0.65) at the pixel level and a high level of agreement at the city scale (R(2) = 0.90). Our results (available at http://sage.wisc.edu/urbanenvironment.html) also reveal that the land footprint of cities occupies less than 0.5% of the Earth's total land area.																	1748-9326					OCT-DEC	2009	4	4							044003	10.1088/1748-9326/4/4/044003						WOS:000272900500005		J	Rogan, J; Chen, DM				Rogan, J; Chen, DM			Remote sensing technology for mapping and monitoring land-cover and land-use change	PROGRESS IN PLANNING																													0305-9006						2004	61		4				301	325		10.1016/S0305-9006(03)00066-7						WOS:000220838100003		J	Koutsouleris, N; Meisenzahl, EM; Davatzikos, C; Bottlender, R; Frodl, T; Scheuerecker, J; Schmitt, G; Zetzsche, T; Decker, P; Reiser, M; Moller, HJ; Gaser, C				Koutsouleris, Nikolaos; Meisenzahl, Eva M.; Davatzikos, Christos; Bottlender, Ronald; Frodl, Thomas; Scheuerecker, Johanna; Schmitt, Gisela; Zetzsche, Thomas; Decker, Petra; Reiser, Maximilian; Moeller, Hans-Juergen; Gaser, Christian			Use of Neuroanatomical Pattern Classification to Identify Subjects in At-Risk Mental States of Psychosis and Predict Disease Transition	ARCHIVES OF GENERAL PSYCHIATRY												Context: Identification of individuals at high risk of developing psychosis has relied on prodromal symptomatology. Recently, machine learning algorithms have been successfully used for magnetic resonance imaging based diagnostic classification of neuropsychiatric patient populations. Objective: To determine whether multivariate neuroanatomical pattern classification facilitates identification of individuals in different at-risk mental states (ARMS) of psychosis and enables the prediction of disease transition at the individual level. Design: Multivariate neuroanatomical pattern classification was performed on the structural magnetic resonance imaging data of individuals in early or late ARMS vs healthy controls (HCs). The predictive power of the method was then evaluated by categorizing the baseline imaging data of individuals with transition to psychosis vs those without transition vs HCs after 4 years of clinical follow-up. Classification generalizability was estimated by cross-validation and by categorizing an independent cohort of 45 new HCs. Setting: Departments of Psychiatry and Psychotherapy, Ludwig-Maximilians-University, Munich, Germany. Participants: The first classification analysis included 20 early and 25 late at-risk individuals and 25 matched HCs. The second analysis consisted of 15 individuals with transition, 18 without transition, and 17 matched HCs. Main Outcome Measures: Specificity, sensitivity, and accuracy of classification. Results: The 3-group, cross-validated classification accuracies of the first analysis were 86% (HCs vs the rest), 91% (early at-risk individuals vs the rest), and 86% (late at-risk individuals vs the rest). The accuracies in the second analysis were 90% ( HCs vs the rest), 88% (individuals with transition vs the rest), and 86% (individuals without transition vs the rest). Independent HCs were correctly classified in 96% (first analysis) and 93% (second analysis) of cases. Conclusions: Different ARMSs and their clinical outcomes may be reliably identified on an individual basis by assessing patterns of whole-brain neuroanatomical abnormalities. These patterns may serve as valuable biomarkers for the clinician to guide early detection in the prodromal phase of psychosis.				Gaser, Christian/F-5103-2010; Frodl, Thomas/D-8118-2012	Gaser, Christian/0000-0002-9940-099X; Frodl, Thomas/0000-0002-8113-6959												0003-990X					JUL	2009	66	7					700	712								WOS:000267720200003	19581561	J	Tang, YC; Zhang, YQ; Chawla, NV; Krasser, S				Tang, Yuchun; Zhang, Yan-Qing; Chawla, Nitesh V.; Krasser, Sven			SVMs Modeling for Highly Imbalanced Classification	IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART B-CYBERNETICS												Traditional classification algorithms can be limited in their performance on highly unbalanced data sets. A popular stream of work for countering the problem of class imbalance has been the application of a sundry of sampling strategies. In this correspondence, we focus on designing modifications to support vector machines (SVMs) to appropriately tackle the problem of class imbalance. We incorporate different "rebalance" heuristics in SVM modeling, including cost-sensitive learning, and over- and undersampling. These SVM-based strategies are compared with various state-of-the-art approaches on a variety of data sets by using various metrics, including G-mean, area under the receiver operating characteristic curve, F-measure, and area under the precision/recall curve. We show that we am able to surpass or match the previously known best algorithms on each data set. In particular, of the four SVM variations considered in this correspondence, the novel granular SVMs-repetitive undersampling algorithm (GSVM-RU) is the best in terms of both effectiveness and efficiency. GSVM-RU is effective, as it can minimize the negative effect of information loss while maximizing the positive effect of data cleaning in the undersampling process. GSVM-RU is efficient by extracting much less support vectors and, hence, greatly speeding up SVM prediction.																	1083-4419					FEB	2009	39	1					281	288		10.1109/TSMCB.2008.2002909						WOS:000262562700024	19068445	J	Hu, QH; Xie, ZX; Yu, DR				Hu, Qinghua; Xie, Zongxia; Yu, Daren			Hybrid attribute reduction based on a novel fuzzy-rough model and information granulation	PATTERN RECOGNITION												Feature subset selection has become an important challenge in areas of pattern recognition, machine learning and data mining. As different semantics are hidden in numerical and categorical features, there are two strategies for selecting hybrid attributes: discretizing numerical variables or numericalize categorical features. In this paper, we introduce a simple and efficient hybrid attribute reduction algorithm based on a generalized fuzzy-rough model. A theoretic framework of fuzzy-rough model based on fuzzy relations is presented, which underlies a foundation for algorithm construction. We derive several attribute significance measures based on the proposed fuzzy-rough model and construct a forward greedy algorithm for hybrid attribute reduction. The experiments show that the technique of variable precision fuzzy inclusion in computing decision positive region can get the optimal classification performance. Number of the selected features is the least but accuracy is the best. (c) 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.				Hu, Qinghua/B-8857-2008													0031-3203					DEC	2007	40	12					3509	3521		10.1016/j.patcog.2007.03.017						WOS:000249678400015		J	Elias, JE; Gibbons, FD; King, OD; Roth, FP; Gygi, SP				Elias, JE; Gibbons, FD; King, OD; Roth, FP; Gygi, SP			Intensity-based protein identification by machine learning from a library of tandem mass spectra	NATURE BIOTECHNOLOGY												Tandem mass spectrometry (MS/MS) has emerged as a cornerstone of proteomics owing in part to robust spectral interpretation algorithms(1-6). Widely used algorithms do not fully exploit the intensity patterns present in mass spectra. Here, we demonstrate that intensity pattern modeling improves peptide and protein identification from MS/MS spectra. We modeled fragment ion intensities using a machine-learning approach that estimates the likelihood of observed intensities given peptide and fragment attributes. From 1,000,000 spectra, we chose 27,000 with high-quality, nonredundant matches as training data. Using the same 27,000 spectra, intensity was similarly modeled with mismatched peptides. We used these two probabilistic models to compute the relative likelihood of an observed spectrum given that a candidate peptide is matched or mismatched. We used a 'decoy' proteome approach to estimate incorrect match frequency(7), and demonstrated that an intensity-based method reduces peptide identification error by 50-96% without any loss in sensitivity.				Roth, Frederick/H-6308-2011	Roth, Frederick/0000-0002-6628-649X												1087-0156					FEB	2004	22	2					214	219		10.1038/nbt930						WOS:000188730500022	14730315	J	Krebs, WG; Alexandrov, V; Wilson, CA; Echols, N; Yu, HY; Gerstein, M				Krebs, WG; Alexandrov, V; Wilson, CA; Echols, N; Yu, HY; Gerstein, M			Normal mode analysis of macromolecular motions in a database framework: Developing mode concentration as a useful classifying statistic	PROTEINS-STRUCTURE FUNCTION AND GENETICS												We investigated protein motions using normal modes within a database framework, determining on a large sample the degree to which normal modes anticipate the direction of the observed motion and were useful for motions classification. As a starting point for our analysis, we identified a large number of examples of protein flexibility from a comprehensive set of structural alignments of the proteins in the PDB. Each example consisted of a pair of proteins that were considerably different in structure given their sequence similarity. On each pair, we performed geometric comparisons and adiabatic-mapping interpolations in a high-throughput pipeline, arriving at a final list of 3,814 putative motions and standardized statistics for each. We then computed the normal modes of each motion in this list, determining the linear combination of modes that best approximated the direction of the observed motion. We integrated our new motions and normal mode calculations in the Macromolecular Motions Database, through a new ranking interface at http://molmovdb.org. Based on the normal mode calculations and the interpolations, we identified a new statistic, mode concentration, related to the mathematical concept of information content, which describes the degree to which the direction of the observed motion can be summarized by a few modes. Using this statistic, we were able to determine the fraction of the 3,814 motions where one could anticipate the direction of the actual motion from only a few modes. We also investigated mode concentration in comparison to related statistics on combinations of normal modes and correlated it,with quantities characterizing protein flexibility (e.g., maximum backbone displacement or number of mobile atoms). Finally, we evaluated the ability of mode concentration to automatically classify motions into a variety of simple categories (e.g., whether or not they are "fragment-like"), in comparison to motion statistics. This involved the application of decision trees and feature selection (particular machine-learning techniques) to training and testing sets derived from merging the "list" of motions with manually classified ones. Proteins 2002;48:682-695. (C) 2002Wiley-Liss,Inc.				Krebs, Werner/N-3116-2015	Krebs, Werner/0000-0003-3155-1422												0887-3585					SEP 1	2002	48	4					682	695		10.1002/prot.10168						WOS:000177711000011	12211036	J	Shawe-Taylor, J; Bartlett, PL; Williamson, RC; Anthony, M				Shawe-Taylor, J; Bartlett, PL; Williamson, RC; Anthony, M			Structural risk minimization over data-dependent hierarchies	IEEE TRANSACTIONS ON INFORMATION THEORY												The paper introduces some generalizations of Vapnik's method of structural risk minimization (SRM). As well as making explicit some of the details on SRM, it provides a result that allows one to trade off errors on the training sample against improved generalization performance. It then considers the more general case when the hierarchy of classes is chosen in response to the data. A result is presented on the generalization performance of classifiers with a "large margin." This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines). The paper concludes with a more general result in terms of "luckiness" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets. Four examples are given of such functions, including the Vapnik-Chervonenkis (VC) dimension measured on the sample.					Bartlett, Peter/0000-0002-8760-3140												0018-9448	1557-9654				SEP	1998	44	5					1926	1940		10.1109/18.705570						WOS:000075317000016		J	SAMUEL, AL				SAMUEL, AL			SOME STUDIES IN MACHINE LEARNING USING GAME OFCHECKERS .2 - RECENT PROGRESS	IBM JOURNAL OF RESEARCH AND DEVELOPMENT																													0018-8646						1967	11	6					601	&								WOS:A1967A378400002		J	Gu, B; Sheng, VS; Tay, KY; Romano, W; Li, S				Gu, Bin; Sheng, Victor S.; Tay, Keng Yeow; Romano, Walter; Li, Shuo			Incremental Support Vector Learning for Ordinal Regression	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS												Support vector ordinal regression (SVOR) is a popular method to tackle ordinal regression problems. However, until now there were no effective algorithms proposed to address incremental SVOR learning due to the complicated formulations of SVOR. Recently, an interesting accurate on-line algorithm was proposed for training nu-support vector classification (nu-SVC), which can handle a quadratic formulation with a pair of equality constraints. In this paper, we first present a modified SVOR formulation based on a sum-of-margins strategy. The formulation has multiple constraints, and each constraint includes a mixture of an equality and an inequality. Then, we extend the accurate on-line nu-SVC algorithm to the modified formulation, and propose an effective incremental SVOR algorithm. The algorithm can handle a quadratic formulation with multiple constraints, where each constraint is constituted of an equality and an inequality. More importantly, it tackles the conflicts between the equality and inequality constraints. We also provide the finite convergence analysis for the algorithm. Numerical experiments on the several benchmark and real-world data sets show that the incremental algorithm can converge to the optimal solution in a finite number of steps, and is faster than the existing batch and incremental SVOR algorithms. Meanwhile, the modified formulation has better accuracy than the existing incremental SVOR algorithm, and is as accurate as the sum-of-margins based formulation of Shashua and Levin.																	2162-237X	2162-2388				JUL	2015	26	7					1403	1416		10.1109/TNNLS.2014.2342533						WOS:000356506700005	25134094	J	Zelenko, D; Aone, C; Richardella, A				Zelenko, D; Aone, C; Richardella, A			Kernel methods for relation extraction	JOURNAL OF MACHINE LEARNING RESEARCH					Workshop on Machine Learning Methods for Text and Images	2001	VANCOUVER, CANADA					We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.																	1532-4435					AUG 15	2003	3	6					1083	1106		10.1162/153244303322533205						WOS:000186002400004		J	Larranaga, P; Calvo, B; Santana, R; Bielza, C; Galdiano, J; Inza, I; Lozano, JA; Armananzas, R; Santafe, G; Perez, A; Robles, V				Larranaga, Pedro; Calvo, Borja; Santana, Roberto; Bielza, Concha; Galdiano, Josu; Inza, Inaki; Lozano, Jose A.; Armananzas, Ruben; Santafe, Guzman; Perez, Aritz; Robles, Victor			Machine learning in bioinformatics	BRIEFINGS IN BIOINFORMATICS												This article reviews machine learning methods for bioinformatics. It presents modelling methods, such as supervised classification, clustering and probabilistic graphical models for knowledge discovery, as well as deterministic and stochastic heuristics for optimization. Applications in genomics, proteomics, systems biology, evolution and text mining are also shown.				Lozano, Jose/F-5120-2010; Santana, Roberto/B-2799-2009; Calvo, Borja/D-8814-2012; Armananzas, Ruben/C-2735-2013; Robles, Victor/L-4220-2014; Bielza, Concha/F-9277-2013; Larranaga, Pedro/F-9293-2013	Lozano, Jose/0000-0002-4683-8111; Santana, Roberto/0000-0002-1005-8535; Armananzas, Ruben/0000-0003-4049-0000; Robles, Victor/0000-0003-3937-2269; Bielza, Concha/0000-0001-7109-2668; Larranaga, Pedro/0000-0003-0652-9872; Calvo, Borja/0000-0001-9969-9664; INZA CANO, INAKI/0000-0003-4674-1755												1467-5463	1477-4054				MAR	2006	7	1					86	112		10.1093/bib/bbk007						WOS:000240930700007	16761367	J	Muegge, I				Muegge, I			Selection criteria for drug-like compounds	MEDICINAL RESEARCH REVIEWS												The fast identification of quality lead compounds in the pharmaceutical industry through a combination of high throughput synthesis and screening has become more challenging in recent years. Although the number of available compounds for high throughput screening (HTS) has dramatically increased, large-scale random combinatorial libraries have contributed proportionally less to identify novel leads for drug discovery projects. Therefore, the concept of 'drug-likeness' of compound selections has become a focus in recent years. In parallel, the low success rate of converting lead compounds into drugs often due to unfavorable pharmacokinetic parameters has sparked a renewed interest in understanding more clearly what makes a compound drug-like. Various approaches have been devised to address the drug-likeness of molecules employing retrospective analyses of known drug collections as well as attempting to capture 'chemical wisdom' in algorithms. For example, simple property counting schemes, machine learning methods, regression models, and clustering methods have been employed to distinguish between drugs and non-drugs. Here we review computational techniques to address the drug-likeness of compound selections and offer an outlook for the further development of the field. (C) 2003 Wiley Periodicals, Inc. Med Res Rev, 23 No. 3, 302-321, 2003.																	0198-6325					MAY	2003	23	3					302	321		10.1002/med.10041						WOS:000182185300003	12647312	J	Pan, SJ; Tsang, IW; Kwok, JT; Yang, QA				Pan, Sinno Jialin; Tsang, Ivor W.; Kwok, James T.; Yang, Qiang			Domain Adaptation via Transfer Component Analysis	IEEE TRANSACTIONS ON NEURAL NETWORKS												Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.				Tsang, Ivor/E-8653-2011; PAN, Sinno Jialin/P-6696-2014; Hajra, Suvadeep/L-8460-2015	Tsang, Ivor/0000-0003-2211-8176; PAN, Sinno Jialin/0000-0001-6565-3836; Tsang, Ivor/0000-0001-8095-4637; Yang, Qiang/0000-0001-5059-8360												1045-9227					FEB	2011	22	2					199	210		10.1109/TNN.2010.2091281						WOS:000287097500003	21095864	J	Tan, PN; Kumar, V; Srivastava, J				Tan, PN; Kumar, V; Srivastava, J			Selecting the right objective measure for association analysis	INFORMATION SYSTEMS					8th International Conference on Knowledge Discovery and Data Mining (KDD 2002)	JUL 23-26, 2002	EDMONTON, CANADA	ACM, SIGKDD				Objective measures such as support, confidence, interest factor, correlation, and entropy are often used to evaluate the interestingness of association patterns. However, in many situations, these measures may provide conflicting information about the interestingness of a pattern. Data mining practitioners also tend to apply an objective measure without realizing that there may be better alternatives available for their application. In this paper, we describe several key properties one should examine in order to select the right measure for a given application. A comparative study of these properties is made using twenty-one measures that were originally developed in diverse fields such as statistics, social science, machine learning, and data mining. We show that depending on its properties, each measure is useful for some application, but not for others. We also demonstrate two scenarios in which many existing measures become consistent with each other, namely, when support-based pruning and a technique known as table standardization are applied. Finally, we present an algorithm for selecting a small set of patterns such that domain experts can find a measure that best fits their requirements by ranking this small set of patterns. (C) 2003 Elsevier Ltd. All rights reserved.					Srivastava, Jaideep/0000-0001-9385-7545												0306-4379	1873-6076				JUN	2004	29	4					293	313		10.1016/S0306-4379(03)00072-3						WOS:000220162000003		J	Dibike, YB; Velickov, S; Solomatine, D; Abbott, MB				Dibike, YB; Velickov, S; Solomatine, D; Abbott, MB			Model induction with support vector machines: Introduction and applications	JOURNAL OF COMPUTING IN CIVIL ENGINEERING												The rapid advance in information processing systems in recent decades had directed engineering research towards the development of intelligent systems that can evolve models of natural phenomena automatically-"by themselves," so to speak. In this respect, a wide range of machine learning techniques like decision trees, artificial neural networks (ANNs), Bayesian methods, fuzzy-rule based systems, and evolutionary algorithms have been successfully applied to model different civil engineering systems. In this study, the possibility of using yet another machine learning paradigm that is firmly based on the theory of statistical learning, namely that of the support vector machine (SVM), is investigated. An interesting property of this approach is that it is an approximate implementation of a structural risk minimization (SRM) induction principle that aims at minimizing a bound on the generalization error of a model, rather than minimizing only the mean square error over the data set. In this paper, the basic ideas underlying statistical learning theory and SVM are reviewed, and the potential of the SVM for feature classification and multiple regression (modeling) problems is demonstrated by applying the method to two different cases of model induction from empirical data. The relative performance of the SVM is then analyzed by comparing its results with that of ANNs on the same data sets.					Dibike, Yonas/0000-0003-2138-9708												0887-3801	1943-5487				JUL	2001	15	3					208	216		10.1061/(ASCE)0887-3801(2001)15:3(208)						WOS:000169391800006		J	Huang, G; Huang, GB; Song, SJ; You, KY				Huang, Gao; Huang, Guang-Bin; Song, Shiji; You, Keyou			Trends in extreme learning machines: A review	NEURAL NETWORKS												Extreme learning machine (ELM) has gained increasing interest from various research fields recently. In this review, we aim to report the current state of the theoretical research and practical advances on this subject. We first give an overview of ELM from the theoretical perspective, including the interpolation theory, universal approximation capability, and generalization ability. Then we focus on the various improvements made to ELM which further improve its stability, sparsity and accuracy under general or specific conditions. Apart from classification and regression, ELM has recently been extended for clustering, feature selection, representational learning and many other learning tasks. These newly emerging algorithms greatly expand the applications of ELM. From implementation aspect, hardware implementation and parallel computation techniques have substantially sped up the training of ELM, making it feasible for big data processing and real-time reasoning. Due to its remarkable efficiency, simplicity, and impressive generalization performance, ELM have been applied in a variety of domains, such as biomedical engineering, computer vision, system identification, and control and robotics. In this review, we try to provide a comprehensive view of these advances in ELM together with its future perspectives. (C) 2014 Elsevier Ltd. All rights reserved.				Huang, Guang-Bin/A-5035-2011	Huang, Guang-Bin/0000-0002-2480-4965; You, Keyou /0000-0003-4355-5340												0893-6080	1879-2782				JAN	2015	61						32	48		10.1016/j.neunet.2014.10.001						WOS:000347595400005	25462632	S	Gama, J; Medas, P; Castillo, G; Rodrigues, P		Bazzan, ALC; Labidi, S		Gama, J; Medas, P; Castillo, G; Rodrigues, P			Learning with drift detection	ADVANCES IN ARTIFICIAL INTELLIGENCE - SBIA 2004	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE				17th Brazilian Symposium on Artificial Intelligence (SBIA 2004)	SEP 29-OCT 01, 2004	Sao Luis, BRAZIL	CNPq, CAPES, FAPEMA, FINEP				Most of the work in machine learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generate the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to control the online error-rate of the algorithm. The training examples are presented in sequence. When a new training example is available, it is classified using the actual model. Statistical theory guarantees that while the distribution is stationary, the error will decrease. When the distribution changes, the error will increase. The method controls the trace of the online error of the algorithm. For the actual context we define a warning level, and a drift level. A new context is declared, if in a sequence of examples, the error increases reaching the warning level at example k(w), and the drift level at example k(d). This is an indication of a change in the distribution of the examples. The algorithm learns a new model using only the examples since k(w). The method was tested with a set of eight artificial datasets and a real world dataset. We used three learning algorithms: a perceptron, a neural network and a decision tree. The experimental results show a good performance detecting drift and with learning the new concept. We also observe that the method is independent of the learning algorithm.				Gama, Joao/A-2070-2008; Rodrigues, Pedro Pereira/H-5789-2012	Gama, Joao/0000-0003-3357-1195; Rodrigues, Pedro Pereira/0000-0001-7867-6682												0302-9743		3-540-23237-0				2004	3171						286	295								WOS:000224323800029		J	Roy, DK; Pentland, AP				Roy, DK; Pentland, AP			Learning words from sights and sounds: a computational model	COGNITIVE SCIENCE												This paper presents an implemented computational model of word acquisition which learns directly from raw multimodal sensory input. Set in an information theoretic framework, the model acquires a lexicon by finding and statistically modeling consistent cross-modal structure. The model has been implemented in a system using novel speech processing, computer vision, and machine learning algorithms. In evaluations the model successfully performed speech segmentation, word discovery and visual categorization from spontaneous infant-directed speech paired with video images of single objects. These results demonstrate the possibility of using state-of-the-art techniques from sensory pattern recognition and machine learning to implement cognitive models which can process raw sensor data without the need for human transcription or labeling. (C) 2002 Cognitive Science Society, Inc. All rights reserved.																	0364-0213					JAN-FEB	2002	26	1					113	146		10.1207/s15516709cog2601_4						WOS:000173730000004		S	Burges, CJC; Scholkopf, B		Mozer, MC; Jordan, MI; Petsche, T		Burges, CJC; Scholkopf, B			Improving the accuracy and speed of support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS				10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Support Vector Learning Machines (SVM) are finding application in pattern recognition, regression estimation, and operator inversion for ill-posed problems. Against this very general backdrop, any methods for improving the generalization performance, or for improving the speed in test phase, of SVMs are of increasing interest. In this paper we combine tyro such techniques on a pattern recognition problem. The method for improving generalization performance (the ''virtual support vector'' method) does so by incorporating known invariances of the problem. This method achieves a drop in the error rate on 10,000 NIST test digit images of 1.4% to 1.0%. The method for improving the speed (the ''reduced set'' method) does so by approximating the support vector decision surface. We apply this method to achieve a factor of fifty speedup in test phase over the virtual support vector machine. The combined approach yields a machine which is both 22 times faster than the original machine, and which has better generalization performance, achieving 1.1% error. The virtual support vector method is applicable to any SVM problem with known invariances. The reduced set method is applicable to any support vector machine.				Scholkopf, Bernhard/A-7570-2013													1049-5258		0-262-10065-7				1997	9						375	381								WOS:A1997BH93C00053		J	Schneider, A; Friedl, MA; Potere, D				Schneider, Annemarie; Friedl, Mark A.; Potere, David			Mapping global urban areas using MODIS 500-m data: New methods and datasets based on 'urban ecoregions'	REMOTE SENSING OF ENVIRONMENT												Although cities, towns and settlements cover only a tiny fraction (<1%) of the world's surface, urban areas are the nexus of human activity with more than 50% of the population and 70-90% of economic activity. As such, material and energy consumption, air pollution, and expanding impervious surface are all concentrated in urban areas, with important environmental implications at local, regional and potentially global scales. New ways to measure and monitor the built environment over large areas are thus critical to answering a wide range of environmental research questions related to the role of urbanization in climate, biogeochemistry and hydrological cycles. This paper presents a new dataset depicting global urban land at 500-m spatial resolution based on MODIS data (available at http://sage.wisc.edu/urbanenvironment.html). The methodological approach exploits temporal and spectral information in one year of MODIS observations, classified using a global training database and an ensemble decision-tree classification algorithm. To overcome confusion between urban and built-up lands and other land cover types, a stratification based on climate, vegetation, and urban topology was developed that allowed region-specific processing. Using reference data from a sample of 140 cities stratified by region, population size, and level of economic development, results show a mean overall accuracy of 93% (k = 0.65) at the pixel level and a high level of agreement at the city scale (R-2 = 0.90). (C) 2010 Elsevier Inc. All rights reserved.																	0034-4257	1879-0704				AUG 16	2010	114	8					1733	1746		10.1016/j.rse.2010.03.003						WOS:000278943900010		J	Fan, Y; Shen, DG; Gur, RC; Gur, RE; Davatzikos, C				Fan, Yong; Shen, Dinggang; Gur, Ruben C.; Gur, Raquel E.; Davatzikos, Christos			COMPARE: Classification of morphological patterns using adaptive regional elements	IEEE TRANSACTIONS ON MEDICAL IMAGING												This paper presents a method for classification of structural brain magnetic resonance (MR) images, by using a combination of deformation- based morphometry and machine learning methods. A morphological representation of the anatomy of interest is first obtained using a high-dimensional mass-preserving template warping method, which results in tissue density maps that constitute local tissue volumetric measurements. Regions that display strong correlations between tissue volume and classification (clinical) variables are extracted using a watershed segmentation algorithm, taking into account the regional smoothness of the correlation map which is estimated by a cross-validation strategy to achieve robustness to outliers. A volume increment algorithm is then applied to these regions to extract regional volumetric features, from which a feature selection technique using support vector machine (SVM)-based criteria is used to select the most discriminative features, according to their effect on the upper bound of the leave-one-out generalization error. Finally, SVM-based classification is applied using the best set of features, and it is tested using a leave-one-out cross-validation strategy. The results on MR brain images of healthy controls and schizophrenia patients demonstrate not only high classification accuracy (91.8% for female subjects and 90.8% for male subjects), but also good stability with respect to the number of features selected and the size of SVM kernel used.					Fan, Yong/0000-0001-9869-4685												0278-0062					JAN	2007	26	1					93	105		10.1109/TMI.2006.886812						WOS:000243286800009	17243588	J	Lao, ZQ; Shen, DG; Xue, Z; Karacali, B; Resnick, SM; Davatzikos, C				Lao, ZQ; Shen, DG; Xue, Z; Karacali, B; Resnick, SM; Davatzikos, C			Morphological classification of brains via high-dimensional shape transformations and machine learning methods	NEUROIMAGE												A high-dimensional shape transformation posed in a mass-preserving framework is used as a morphological signature of a brain image. Population differences with complex spatial patterns are then determined by applying a nonlinear support vector machine (SVM) pattern classification method to the morphological signatures. Significant reduction of the dimensionality of the morphological signatures is achieved via wavelet decomposition and feature reduction methods. Applying the method to MR images with simulated atrophy shows that the method can correctly detect subtle and spatially complex atrophy, even when the simulated atrophy represents only a 5% variation from the original image. Applying this method to actual MR images shows that brains can be correctly determined to be male or female with a successful classification rate of 97%, using the leave-one-out method. This proposed method also shows a high classification rate for old adults' age classification, even under difficult test scenarios. The main characteristic of the proposed methodology is that, by applying multivariate pattern classification methods, it can detect subtle and spatially complex patterns of morphological group differences which are often not detectable by voxel-based morphometric methods, because these methods analyze morphological measurements voxel-by-voxel and do not consider the entirety of the data simultaneously. (C) 2003 Elsevier Inc. All rights reserved.				Xue, Zhong/I-3414-2012													1053-8119					JAN	2004	21	1					46	57		10.1016/j.neuroimage.2003.09.027						WOS:000188597500005	14741641	J	Cai, D; He, XF; Han, JW				Cai, Deng; He, Xiaofei; Han, Jiawei			SRDA: An efficient algorithm for large-scale discriminant analysis	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												Linear Discriminant Analysis (LDA) has been a popular method for extracting features that preserves class separability. The projection functions of LDA are commonly obtained by maximizing the between-class covariance and simultaneously minimizing the within-class covariance. It has been widely used in many fields of information processing, such as machine learning, data mining, information retrieval, and pattern recognition. However, the computation of LDA involves dense matrices eigendecomposition, which can be computationally expensive in both time and memory. Specifically, LDA has O(mnt + t(3) )time complexity and requires O(mn + mt + nt) memory, where m is the number of samples, n is the number of features, and t = min(m,n). When both m and n are large, it is infeasible to apply LDA. In this paper, we propose a novel algorithm for discriminant analysis, called Spectral Regression Discriminant Analysis (SRDA). By using spectral graph analysis, SRDA casts discriminant analysis into a regression framework that facilitates both efficient computation and the use of regularization techniques. Specifically, SRDA only needs to solve a set of regularized least squares problems, and there is no eigenvector computation involved, which is a huge save of both time and memory. Our theoretical analysis shows that SRDA can be computed with O(ms) time and O(ms) memory, where s(<= n) n is the average number of nonzero features in each sample. Extensive experimental results on four real-world data sets demonstrate the effectiveness and efficiency of our algorithm.																	1041-4347					JAN	2008	20	1					1	12		10.1109/TKDE.2007.190669						WOS:000251003300001		J	Settles, B				Settles, B			ABNER: an open source tool for automatically tagging genes, proteins and other entity names in text	BIOINFORMATICS												ABNER ( A Biomedical Named Entity Recognizer) is an open source software tool for molecular biology text mining. At its core is a machine learning system using conditional random fields with a variety of orthographic and contextual features. The latest version is 1.5, which has an intuitive graphical interface and includes two modules for tagging entities ( e. g. protein and cell line) trained on standard corpora, for which performance is roughly state of the art. It also includes a Java application programming interface allowing users to incorporate ABNER into their own systems and train models on new corpora.																	1367-4803					JUL 15	2005	21	14					3191	3192		10.1093/bioinformatics/bti475						WOS:000230204400026	15860559	J	Walters, WP; Murcko, MA				Walters, WP; Murcko, MA			Prediction of 'drug-likeness'	ADVANCED DRUG DELIVERY REVIEWS												Recent developments in combinatorial chemistry and high-throughput screening have dramatically increased the scale on which drug discovery programs are carried out. Along with these advances has come a need for automated methods of determining which compounds from a library should be synthesized and screened. These methods range from simple counting schemes to sophisticated machine learning techniques such as neural networks. While many of these methods have performed well in validation studies, the field is still in its formative stage. This paper reviews a number of computational techniques for identifying drug-like molecules and examines challenges facing the field. (C) 2002 Published by Elsevier Science B.V.																	0169-409X					MAR 31	2002	54	3					255	271	PII S0169-409X(02)00003-0	10.1016/S0169-409X(02)00003-0						WOS:000175221500002	11922947	J	Xia, JG; Broadhurst, DI; Wilson, M; Wishart, DS				Xia, Jianguo; Broadhurst, David I.; Wilson, Michael; Wishart, David S.			Translational biomarker discovery in clinical metabolomics: an introductory tutorial	METABOLOMICS												Metabolomics is increasingly being applied towards the identification of biomarkers for disease diagnosis, prognosis and risk prediction. Unfortunately among the many published metabolomic studies focusing on biomarker discovery, there is very little consistency and relatively little rigor in how researchers select, assess or report their candidate biomarkers. In particular, few studies report any measure of sensitivity, specificity, or provide receiver operator characteristic (ROC) curves with associated confidence intervals. Even fewer studies explicitly describe or release the biomarker model used to generate their ROC curves. This is surprising given that for biomarker studies in most other biomedical fields, ROC curve analysis is generally considered the standard method for performance assessment. Because the ultimate goal of biomarker discovery is the translation of those biomarkers to clinical practice, it is clear that the metabolomics community needs to start "speaking the same language" in terms of biomarker analysis and reporting-especially if it wants to see metabolite markers being routinely used in the clinic. In this tutorial, we will first introduce the concept of ROC curves and describe their use in single biomarker analysis for clinical chemistry. This includes the construction of ROC curves, understanding the meaning of area under ROC curves (AUC) and partial AUC, as well as the calculation of confidence intervals. The second part of the tutorial focuses on biomarker analyses within the context of metabolomics. This section describes different statistical and machine learning strategies that can be used to create multi-metabolite biomarker models and explains how these models can be assessed using ROC curves. In the third part of the tutorial we discuss common issues and potential pitfalls associated with different analysis methods and provide readers with a list of nine recommendations for biomarker analysis and reporting. To help readers test, visualize and explore the concepts presented in this tutorial, we also introduce a web-based tool called ROCCET (ROC Curve Explorer & Tester, http://www.roccet.ca). ROCCET was originally developed as a teaching aid but it can also serve as a training and testing resource to assist metabolomics researchers build biomarker models and conduct a range of common ROC curve analyses for biomarker studies.					Wishart, David S/0000-0002-3207-2434												1573-3882					APR	2013	9	2					280	299		10.1007/s11306-012-0482-9						WOS:000316913200003		B	Izenman, AJ	Izenman, AJ			Izenman, Alan Julian	Izenman, AJ		Modern Multivariate Statistical Techniques Regression, Classification, and Manifold Learning Introduction and Preview	MODERN MULTIVARIATE STATISTICAL TECHNIQUES: REGRESSION, CLASSIFICATION, AND MANIFOLD LEARNING	Springer Texts in Statistics																														978-0-387-78188-4				2008							1	+		10.1007/978-0-387-78189-1_1	10.1007/978-0-387-78189-1					WOS:000266979300001		J	Tan, AC; Naiman, DQ; Xu, L; Winslow, RL; Geman, D				Tan, AC; Naiman, DQ; Xu, L; Winslow, RL; Geman, D			Simple decision rules for classifying human cancers from gene expression profiles	BIOINFORMATICS												Motivation: Various studies have shown that cancer tissue samples can be successfully detected and classified by their gene expression patterns using machine learning approaches. One of the challenges in applying these techniques for classifying gene expression data is to extract accurate, readily interpretable rules providing biological insight as to how classification is performed. Current methods generate classifiers that are accurate but difficult to interpret. This is the trade-off between credibility and comprehensibility of the classifiers. Here, we introduce a new classifier in order to address these problems. It is referred to as k-TSP (k-Top Scoring Pairs) and is based on the concept of 'relative expression reversals'. This method generates simple and accurate decision rules that only involve a small number of gene-to-gene expression comparisons, thereby facilitating follow-up studies. Results: In this study, we have compared our approach to other machine learning techniques for class prediction in 19 binary and multi-class gene expression datasets involving human cancers. The k-TSP classifier performs as efficiently as Prediction Analysis of Microarray and support vector machine, and outperforms other learning methods (decision trees, k-nearest neighbour and naive Bayes). Our approach is easy to interpret as the classifier involves only a small number of informative genes. For these reasons, we consider the k-TSP method to be a useful tool for cancer classification from microarray gene expression data.				Naiman, Daniel/A-3304-2010; Geman, Donald/A-3325-2010; Tan, Aik Choon/A-3135-2011	Naiman, Daniel/0000-0001-6504-9081; Tan, Aik Choon/0000-0003-2955-8369												1367-4803					OCT 15	2005	21	20					3896	3904		10.1093/bioinformatics/bti631						WOS:000232596300013	16105897	J	Dai, HY; van't Veer, L; Lamb, J; He, YD; Mao, M; Fine, BM; Bernards, R; de Vijver, MV; Deutsch, P; Sachs, A; Stoughton, R; Friend, S				Dai, HY; van't Veer, L; Lamb, J; He, YD; Mao, M; Fine, BM; Bernards, R; de Vijver, MV; Deutsch, P; Sachs, A; Stoughton, R; Friend, S			A cell proliferation signature is a marker of extremely poor outcome in a subpopulation of breast cancer patients	CANCER RESEARCH												Breast cancer comprises a group of distinct subtypes that despite having similar histologic appearances, have very different metastatic potentials. Being able to identify the biological driving force, even for a subset of patients, is crucially important given the large population of women diagnosed with breast cancer. Here, we show that within a subset of patients characterized by relatively high estrogen receptor expression for their age, the occurrence of metastases is strongly predicted by a homogeneous gene expression pattern almost entirely consisting of cell cycle genes (5-year odds ratio of metastasis, 24.0; 95% confidence interval, 6.0-95.5). Overexpression of this set of genes is clearly associated with an extremely poor outcome, with the 10-year metastasis-free probability being only 24% for the poor group, compared with 85% for the good group. In contrast, this gene expression pattern is much less correlated with the outcome in other patient subpopulations. The methods described here also illustrate the value of combining clinical variables, biological insight, and machine-learning to dissect biological complexity. Our work presented here may contribute a crucial step towards rational design of personalized treatment.					Bernards, Rene/0000-0001-8677-3423												0008-5472					MAY 15	2005	65	10					4059	4066		10.1158/0008-5472.CAN-04-3953						WOS:000229062000012	15899795	J	Cai, YD; Liu, XJ; Xu, XB; Chou, KC				Cai, YD; Liu, XJ; Xu, XB; Chou, KC			Prediction of protein structural classes by support vector machines	COMPUTERS & CHEMISTRY												In this paper, we apply a new machine learning method which is called support vector machine to approach the prediction of protein structural class. The support vector machine method is performed based on the database derived from SCOP which is based upon domains of known structure and the evolutionary relationships and the principles that govern their 3D structure. As a result, high rates of both self-consistency and jackknife test are obtained. This indicates that the structural class of a protein inconsiderably correlated with its amino acid composition, and the support vector machine can be referred as a powerful computational tool for predicting the structural classes of proteins. (C) 2002 Elsevier Science Ltd. All rights reserved.				Chou, Kuo-Chen/A-8340-2009	cai, yudong/0000-0001-5664-7979												0097-8485					FEB	2002	26	3					293	296	PII S0097-8485(01)00113-9	10.1016/S0097-8485(01)00113-9						WOS:000173969900008	11868916	J	Lemm, S; Blankertz, B; Dickhaus, T; Muller, KR				Lemm, Steven; Blankertz, Benjamin; Dickhaus, Thorsten; Mueller, Klaus-Robert			Introduction to machine learning for brain imaging	NEUROIMAGE												Machine learning and pattern recognition algorithms have in the past years developed to become a working horse in brain imaging and the computational neurosciences, as they are instrumental for mining vast amounts of neural data of ever increasing measurement precision and detecting minuscule signals from an overwhelming noise floor. They provide the means to decode and characterize task relevant brain states and to distinguish them from non-informative brain signals. While undoubtedly this machinery has helped to gain novel biological insights, it also holds the danger of potential unintentional abuse. Ideally machine learning techniques should be usable for any non-expert, however, unfortunately they are typically not. Overfitting and other pitfalls may occur and lead to spurious and nonsensical interpretation. The goal of this review is therefore to provide an accessible and clear introduction to the strengths and also the inherent dangers of machine learning usage in the neurosciences. (C) 2010 Elsevier Inc. All rights reserved.				Dickhaus, Thorsten/I-3571-2012; Muller, Klaus/C-3196-2013													1053-8119	1095-9572				MAY 15	2011	56	2			SI		387	399		10.1016/j.neuroimage.2010.11.004						WOS:000290081900002	21172442	J	Liu, B; Wang, L; Jin, YH				Liu, Bo; Wang, Ling; Jin, Yi-Hui			An effective PSO-based memetic algorithm for flow shop scheduling	IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART B-CYBERNETICS												This paper proposes an effective particle swarm optimization (PSO)-based memetic algorithm (MA) for the permutation flow shop scheduling problem (PFSSP) with the objective to minimize the maximum completion time, which is a typical non-deterministic polynomial-time (NP) hard combinatorial optimization problem. In the proposed PSO-based MA (PSOMA), both PSO-based searching operators and some special local searching operators are designed to balance the exploration and exploitation abilities. In particular, the PSOMA applies the evolutionary searching mechanism of PSO, which is characterized by individual improvement, population cooperation, and competition to effectively perform exploration. On the other hand, the PSOMA utilizes several adaptive local searches to perform exploitation. First, to make PSO suitable for solving PFSSP, a ranked-order value rule based on random key representation is presented to convert the continuous position values of particles to job permutations. Second, to generate an initial swarm with certain quality and diversity, the famous Nawaz-Enscore-Ham (NEH) heuristic is incorporated into the initialization of population. Third, to balance the exploration and exploitation abilities, after the standard PSO-based searching operation, a new local search technique named NEH_1 insertion is probabilistically applied to some good particles selected by using a roulette wheel mechanism with a specified probability. Fourth, to enrich the searching behaviors and to avoid premature convergence, a simulated annealing (SA)-based local search with multiple different neighborhoods is designed and incorporated into the PSOMA. Meanwhile, an effective adaptive meta-Lamarckian learning strategy is employed to decide which neighborhood to be used in SA-based local search. Finally, to further enhance the exploitation ability, a pairwise-based local search is applied after the SA-based search. Simulation results based on benchmarks demonstrate the effectiveness of the PSOMA. Additionally, the effects of some parameters on optimization performances are also discussed.				Wang, Ling/B-1195-2009; Liu, Bo/A-9995-2008													1083-4419					FEB	2007	37	1					18	27		10.1109/TSMCB.2006.883272						WOS:000244131800003	17278555	J	Zhu, XQ; Wu, XD				Zhu, XQ; Wu, XD			Class noise vs. attribute noise: A quantitative study of their impacts	ARTIFICIAL INTELLIGENCE REVIEW												Real-world data is never perfect and can often suffer from corruptions (noise) that may impact interpretations of the data, models created front the data and decisions made based on the data. Noise can reduce system performance in terms of classification accuracy, time in building a classifier and the size of the classifier. Accordingly, most existing learning algorithms have integrated various approaches to enhance their learning abilities from noisy environments, but the existence of noise can still introduce serious negative impacts. A more reasonable solution might be to employ sonic preprocessing mechanisms to handle noisy instances before a learner is formed. Unfortunately, rare research has been conducted to systematically explore the impact of noise, especially from the noise handling point of view. This has made various noise processing techniques less significant, specifically when dealing with noise that is introduced in attributes. In this paper, we present a systematic evaluation on the effect of noise in machine learning. Instead of taking any unified theory of noise to evaluate the noise impacts, we differentiate noise into two categories: class noise and attribute noise, and analyze their impacts on the system performance separately. Because class noise has been widely addressed in existing research efforts, we concentrate on attribute noise. We investigate the relationship between attribute noise and classification accuracy, the impact of noise at different attributes, and possible solutions in handling attribute noise. Our conclusions can be used to guide interested readers to enhance data quality by designing various noise handling mechanisms.																	0269-2821					NOV	2004	22	3					177	210		10.1007/s10462-004-0751-8						WOS:000225767100001		J	Timmis, J; Neal, M; Hunt, J				Timmis, J; Neal, M; Hunt, J			An artificial immune system for data analysis	BIOSYSTEMS					3rd International Workshop on Information Processing in Cells and Tissues	AUG 23-24, 1999	INDIANAPOLIS, INDIANA					We present a simplified view of those parts of the human immune system which can be used to provide the basis for a data analysis tool. The motivation for and reasoning behind such a model is given and the desire for a 'transparent' model and meaningful visualization and interpretation techniques is noted. A minimalist formulation of an artificial immune system and some of its behaviour is described. A simple implementation and a suitable visualization technique are demonstrated using some trivial data and the famous 'iris' data set. (C) 2000 Elsevier Science Ireland Ltd. All rights reserved.																	0303-2647					FEB	2000	55	1-3					143	150		10.1016/S0303-2647(99)00092-1						WOS:000086628800017	10745118	J	MICHIE, D				MICHIE, D			MEMO FUNCTIONS AND MACHINE LEARNING	NATURE																													0028-0836	1476-4687					1968	218	5136					19	+		10.1038/218019a0						WOS:A1968A895300003		J	Preece, SJ; Goulermas, JY; Kenney, LPJ; Howard, D; Meijer, K; Crompton, R				Preece, Stephen J.; Goulermas, John Y.; Kenney, Laurence P. J.; Howard, Dave; Meijer, Kenneth; Crompton, Robin			Activity identification using body-mounted sensors-a review of classification techniques	PHYSIOLOGICAL MEASUREMENT												With the advent of miniaturized sensing technology, which can be body-worn, it is now possible to collect and store data on different aspects of human movement under the conditions of free living. This technology has the potential to be used in automated activity profiling systems which produce a continuous record of activity patterns over extended periods of time. Such activity profiling systems are dependent on classification algorithms which can effectively interpret body worn sensor data and identify different activities. This article reviews the different techniques which have been used to classify normal activities and/or identify falls from body-worn sensor data. The review is structured according to the different analytical techniques and illustrates the variety of approaches which have previously been applied in this field. Although significant progress has been made in this important area, there is still significant scope for further work, particularly in the application of advanced classification techniques to problems involving many different activities.				Meijer, Kenneth/A-8573-2014	Meijer, Kenneth/0000-0001-8236-8754; Kenney, Laurence/0000-0003-2164-3892												0967-3334	1361-6579				APR	2009	30	4					R1	R33		10.1088/0967-3334/30/4/R01						WOS:000264813400001	19342767	J	Riesen, K; Bunke, H				Riesen, Kaspar; Bunke, Horst			Approximate graph edit distance computation by means of bipartite graph matching	IMAGE AND VISION COMPUTING					6th International Workshop on Graph-Based Representations in Pattern Recognition	JUN 11-13, 2007	Alicante, SPAIN	IAPR TC15, Univ Alicante, Robot Vis Grp, Pattern Anal, Stat Modelling & Computat Learning, European Excellence Network, Generalitat Valenciana, Dept Co, Univ & Sci, UA, Off Extracurricular Activities, UA, Inst Informat Res, Alicante Convent Bur, Patronato Municipal Turismo, UA, Dept Comp Sci & Artificial Intelligence, UA, Off Res, Dept & Innovat				In recent years, the use of graph based object representation has gained popularity. Simultaneously, graph edit distance emerged as a powerful and flexible graph matching paradigm that can be used to address different tasks in pattern recognition, machine learning, and data mining. The key advantages of graph edit distance are its high degree of flexibility, which makes it applicable to any type of graph, and the fact that one can integrate domain specific knowledge about object similarity by means of specific edit cost functions. Its computational complexity, however, is exponential in the number of nodes of the involved graphs. Consequently, exact graph edit distance is feasible for graphs of rather small size only. In the present paper we introduce a novel algorithm which allows us to approximately, or suboptimally, compute edit distance in a substantially faster way. The proposed algorithm considers only local, rather than global, edge structure during the optimization process. In experiments on different datasets we demonstrate a substantial speed-up of our proposed method over two reference systems. Moreover, it is emprically verified that the accuracy of the suboptimal distance remains sufficiently accurate for various pattern recognition applications. (C) 2008 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JUN 4	2009	27	7			SI		950	959		10.1016/j.imavis.2008.04.004						WOS:000266393300012		J	Ron, D; Singer, Y; Tishby, N				Ron, D; Singer, Y; Tishby, N			The power of amnesia: Learning probabilistic automata with variable memory length	MACHINE LEARNING					7th Annual ACM Conference on Computational Learning Theory (COLT 94)	1994	NEW BRUNSWICK, NJ	ACM				We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata (PSA). Though hardness results are known for learning distributions generated by general probabilistic automata, we prove that the algorithm we present can efficiently learn distributions generated by PSAs. In particular, we show that for any target PSA, the KL-divergence between the distribution generated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity. The learning algorithm is motivated by applications in human-machine interaction. Here we present two applications of the algorithm. In the first one we apply the algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In the second application we construct a simple stochastic model for E. coli DNA.																	0885-6125					NOV-DEC	1996	25	2-3					117	149		10.1023/A:1026490906255						WOS:A1996WK31000002		J	Kodovsky, J; Fridrich, J; Holub, V				Kodovsky, Jan; Fridrich, Jessica; Holub, Vojtech			Ensemble Classifiers for Steganalysis of Digital Media	IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY												Today, the most accurate steganalysis methods for digital media are built as supervised classifiers on feature vectors extracted from the media. The tool of choice for the machine learning seems to be the support vector machine (SVM). In this paper, we propose an alternative and well-known machine learning tool-ensemble classifiers implemented as random forests-and argue that they are ideally suited for steganalysis. Ensemble classifiers scale much more favorably w.r.t. the number of training examples and the feature dimensionality with performance comparable to the much more complex SVMs. The significantly lower training complexity opens up the possibility for the steganalyst to work with rich (high-dimensional) cover models and train on larger training sets-two key elements that appear necessary to reliably detect modern steganographic algorithms. Ensemble classification is portrayed here as a powerful developer tool that allows fast construction of steganography detectors with markedly improved detection accuracy across a wide range of embedding methods. The power of the proposed framework is demonstrated on three steganographic methods that hide messages in JPEG images.																	1556-6013	1556-6021				APR	2012	7	2					432	444		10.1109/TIFS.2011.2175919						WOS:000301506500008		J	Tetko, IV; Sushko, I; Pandey, AK; Zhu, H; Tropsha, A; Papa, E; Oberg, T; Todeschini, R; Fourches, D; Varnek, A				Tetko, Igor V.; Sushko, Iurii; Pandey, Anil Kumar; Zhu, Hao; Tropsha, Alexander; Papa, Ester; Oberg, Tomas; Todeschini, Roberto; Fourches, Denis; Varnek, Alexandre			Critical assessment of QSAR models of environmental toxicity against Tetrahymena pyriformis: Focusing on applicability domain and overfitting by variable selection	JOURNAL OF CHEMICAL INFORMATION AND MODELING												The estimation of the accuracy of predictions is a critical problem in QSAR modeling. The "distance to model" can be defined as a metric that defines the similarity between the training set molecules and the test set compound for the given property in the context of a specific model. It could be expressed in many different ways, e.g., using Tanimoto coefficient, leverage, correlation in space of models, etc. In this paper we have used mixtures of Gaussian distributions as well as statistical tests to evaluate six types of distances to models with respect to their ability to discriminate compounds with small and large prediction errors. The analysis was performed for twelve QSAR models of aqueous toxicity against T. pyriformis obtained with different machine-learning methods and various types of descriptors. The distances to model based oil standard deviation of predicted toxicity calculated from the ensemble of models afforded the best results. This distance also successfully discriminated molecules with low and large prediction errors for a mechanism-based model developed using log P and the Maximum Acceptor Superdelocalizability descriptors. Thus, the distance to model metric could also be used to augment mechanistic QSAR models by estimating their prediction errors. Moreover, the accuracy of prediction is mainly determined by the training set data distribution in the chemistry and activity spaces but not by QSAR approaches used to develop the models. We have shown that incorrect validation of a model may result in the wrong estimation of its performance and suggested how this problem could be circumvented. The toxicity of 3182 and 48774 molecules from the EPA High Production Volume (HPV) Challenge Program and EINECS (European chemical Substances Information System), respectively, was predicted, and the accuracy of prediction was estimated. The developed models are available online at http://www.qspr.org site.				Oberg, Tomas/H-4543-2011; Todeschini, Roberto/B-5868-2014; Tropsha, Alexander/G-6245-2014; Tetko, Igor/B-1540-2010; Varnek, Alexandre/E-7076-2017	Oberg, Tomas/0000-0001-9382-9296; Todeschini, Roberto/0000-0002-6454-4192; Tetko, Igor/0000-0002-6855-0012; Varnek, Alexandre/0000-0003-1886-925X; PAPA, ESTER/0000-0002-0041-556X; Fourches, Denis/0000-0001-5642-8303												1549-9596					SEP	2008	48	9					1733	1746		10.1021/ci800151m						WOS:000259398500001	18729318	J	Davatzikos, C; Fan, Y; Wu, X; Shen, D; Resnick, SM				Davatzikos, Christos; Fan, Yong; Wu, Xiaoying; Shen, Dinggang; Resnick, Susan M.			Detection of prodromal Alzheimer's disease via pattern classification of magnetic resonance imaging	NEUROBIOLOGY OF AGING												We report evidence that computer-based high-dimensional pattern classification of magnetic resonance imaging (MRI) detects patterns of brain structure characterizing mild cognitive impairment (MCI), often a prodromal phase of Alzheimer's disease (AD). Ninety percent diagnostic accuracy was achieved, using cross-validation, for 30 participants in the Baltimore Longitudinal Study of Aging. Retrospective evaluation of serial scans obtained during prior years revealed gradual increases in structural abnormality for the MCI group, often before clinical symptoms, but slower increase for individuals remaining cognitively normal. Detecting complex patterns of brain abnormality in very early stages of cognitive impairment has pivotal importance for the detection and management of AD. (C) 2006 Elsevier Inc. All rights reserved.					Fan, Yong/0000-0001-9869-4685												0197-4580					APR	2008	29	4					514	523		10.1016/j.neurobiolaging.2006.11.010						WOS:000254077200003	17174012	J	Huang, HJ; Shiffman, ML; Friedman, S; Venkatesh, R; Bzowej, N; Abar, OT; Rowland, CM; Catanese, JJ; Leong, DU; Sninsky, JJ; Layden, TJ; Wright, TL; White, T; Cheung, RC				Huang, Honjin; Shiffman, Mitchefl L.; Friedman, Scott; Venkatesh, Ramasubbu; Bzowej, Natatie; Abar, Ohvia T.; Rowland, Charles M.; Catanese, Joseph J.; Leong, Diane U.; Sninsky, John J.; Layden, Thomas J.; Wright, Teresa L.; White, Thomas; Cheung, Ramsey C.			A 7 gene signature identifies the risk of developing cirrhosis in patients with chronic hepatitis C	HEPATOLOGY												Clinical factors such as age, gender, alcohol use, and age-at-infection influence the progression to cirrhosis but cannot accurately predict the risk of developing cirrhosis in patients with chronic hepatitis C (CHC). The aim of this study was to develop a predictive signature for cirrhosis in Caucasian patients. All patients had well-characterized liver histology and clinical factors; DNA was extracted from whole blood for genotyping. We validated all significant markers from a genome scan in the training cohort, and selected 361 markers for the signature building. Using a "machine learning" approach, a signature consisting of markers most predictive for cirrhosis risk in Caucasian patients was developed in the training set (N = 420). The Cirrhosis Risk Score (CRS) was calculated to estimate the risk of developing cirrhosis for each patient. The CRS performance was then tested in an independently enrolled validation cohort of 154 Caucasian patients. A CRS signature consisting of 7 markers was developed for Caucasian patients. The area-under-the-ROC curves (AUC) of the CRS was 0.75 in the training cohort. In the validation cohort, AUC was only 0.53 for clinical factors, increased to 0.73 for CRS, and 0.76 when CRS and clinical factors were combined. A low CRS cutoff of < 0.50 to identify low-risk patients would misclassify only 10.3% of high-risk patients, while a high cutoff of > 0.70 to identify high-risk patients would misclassify 22.3% of low-risk patients. Conclusion: CRS is a better predictor than clinical factors in differentiating high-risk versus low-risk for cirrhosis in Caucasian CHC patients. Prospective studies should be conducted to further validate these findings.																	0270-9139					AUG	2007	46	2					297	306		10.1002/hep.21695						WOS:000248501600005	17461418	J	Ficetola, GF; Thuiller, W; Miaud, C				Ficetola, Gentile Francesco; Thuiller, Wilfried; Miaud, Claude			Prediction and validation of the potential global distribution of a problematic alien invasive species - the American bullfrog	DIVERSITY AND DISTRIBUTIONS												Predicting the probability of successful establishment and invasion of alien species at global scale, by matching climatic and land use data, is a priority for the risk assessment. Both large- and local-scale factors contribute to the outcome of invasions, and should be integrated to improve the predictions. At global scale, we used climatic and land use layers to evaluate the habitat suitability for the American bullfrog Rana catesbeiana, a major invasive species that is among the causes of amphibian decline. Environmental models were built by using Maxent, a machine learning method. Then, we integrated global data with information on richness of native communities and hunting pressure collected at the local scale. Global-scale data allowed us to delineate the areas with the highest suitability for this species. Predicted suitability was significantly related to the invasiveness observed for bullfrog populations historically introduced in Europe, but did not explain a large portion of variability in invasion success. The integration of data at the global and local scales greatly improved the performance of models, and explained > 57% of the variance in introduction success: bullfrogs were more invasive in areas with high suitability and low hunting pressure over frogs. Our study identified the climatic factors entailing the risk of invasion by bullfrogs, and stresses the importance of the integration of biotic and abiotic data collected at different spatial scales, to evaluate the areas where monitoring and management efforts need to be focused.				Ficetola, Gentile Francesco/A-2813-2008; THUILLER, Wilfried/G-3283-2010	Ficetola, Gentile Francesco/0000-0003-3414-5155; THUILLER, Wilfried/0000-0002-5388-5274												1366-9516	1472-4642				JUL	2007	13	4					476	485		10.1111/j.1472-4642.2007.00377.x						WOS:000247263100013		J	Olden, JD; Lawler, JJ; Poff, NL				Olden, Julian D.; Lawler, Joshua J.; Poff, N. Leroy			Machine learning methods without tears: A primer for ecologists	QUARTERLY REVIEW OF BIOLOGY												Machine learning methods, a family of statistical techniques with origins in the field of artificial intelligence, are recognized as holding great promise far the advancement of understanding and prediction about ecological phenomena. These modeling techniques are flexible enough to handle complex problems with multiple interacting elements and typically outcompete traditional approaches (e.g., generalized linear models), making them ideal for modi ling ecological systems. Despite their inherent advantages, a review of the literature reveals only a modest use of these approaches in ecology as compared to other disciplines. One potential explanation for this lack of interest is that machine learning techniques do not fall neatly into the class of statistical, modeling approaches with which most ecologists are familiar In this paper, we provide an introduction three machine learning approaches that can be broadly used by ecologists: classification and regression trees, artificial neural networks, and evolutionary computation. For each approach, we provide a brief background to the methodology, give examples of its application in ecology, describe model development and implementation, discuss strengths and weaknesses, explore the availability of statistical of software, and provide an illustrative				Poff, Nathan/C-1239-2009; Olden, Julian/A-8535-2010	Olden, Julian/0000-0003-2143-1187												0033-5770					JUN	2008	83	2					171	193		10.1086/587826						WOS:000256697400002	18605534	J	Carpousis, AJ; Vanzo, NF; Raynal, LC				Carpousis, AJ; Vanzo, NF; Raynal, LC			mRNA degradation - a tale of poly(A) and multiprotein machines	TRENDS IN GENETICS												The Escherichia coli RNA degradosome is a multiprotein complex containing an endoribonuclease, polynucleotide phosphorylase and a DEAD-box RNA helicase. A related complex has been described in the spinach chloroplast. The exosome and the mtEXO complex have recently been described in yeast and it is likely that related complexes also exist in animal cells. This research suggests the widespread existence of sophisticated machines for the efficient degradation of messenger RNA. The DEAD-box helicase in the degradosome can unwind regions of RNA structure that interfere with 3'-5' degradation. The polyadenylation of RNA 3' ends is also known to promote degradation by creating a 'toehold' for the degradation machinery. Much remains to be learned about the regulation of mRNA stability. The complexity of the degradation process, both in the eubacteria and in the eukaryotes, suggests that many steps are possible points of control.																	0168-9525					JAN	1999	15	1					24	28		10.1016/S0168-9525(98)01627-8						WOS:000079419200009	10087930	J	Esposito, F; Malerba, D; Semeraro, G				Esposito, F; Malerba, D; Semeraro, G			A comparative analysis of methods for pruning decision trees	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												In this paper, we address the problem of retrospectively pruning decision trees induced from data, according to a top-down approach. This problem has received considerable attention in the areas of pattern recognition and machine learning, and many distinct methods have been proposed in literature. We make a comparative study of six well-known pruning methods with the aim of understanding their theoretical foundations, their computational complexity, and the strengths and weaknesses of their formulation. Comments on the characteristics of each method are empirically supported. In particular, a wide experimentation performed on several data sets leads us to opposite conclusions on the predictive accuracy of simplified trees from some drawn in the literature. We attribute this divergence to differences in experimental designs. Finally, we prove and make use of a property of the reduced error pruning method to obtain an objective evaluation of the tendency to overprune/underprune observed in each method.				Malerba, Donato/H-3850-2012	Malerba, Donato/0000-0001-8432-4608; Esposito, Floriana/0000-0002-1075-3239; Semeraro, Giovanni/0000-0001-6883-1853												0162-8828					MAY	1997	19	5					476	491		10.1109/34.589207						WOS:A1997XB16300006		J	Rottig, M; Medema, MH; Blin, K; Weber, T; Rausch, C; Kohlbacher, O				Roettig, Marc; Medema, Marnix H.; Blin, Kai; Weber, Tilmann; Rausch, Christian; Kohlbacher, Oliver			NRPSpredictor2-a web server for predicting NRPS adenylation domain specificity	NUCLEIC ACIDS RESEARCH												The products of many bacterial non-ribosomal peptide synthetases (NRPS) are highly important secondary metabolites, including vancomycin and other antibiotics. The ability to predict substrate specificity of newly detected NRPS Adenylation (A-) domains by genome sequencing efforts is of great importance to identify and annotate new gene clusters that produce secondary metabolites. Prediction of A-domain specificity based on the sequence alone can be achieved through sequence signatures or, more accurately, through machine learning methods. We present an improved predictor, based on previous work (NRPSpredictor), that predicts A-domain specificity using Support Vector Machines on four hierarchical levels, ranging from gross physicochemical properties of an A-domain's substrates down to single amino acid substrates. The three more general levels are predicted with an F-measure better than 0.89 and the most detailed level with an average F-measure of 0.80. We also modeled the applicability domain of our predictor to estimate for new A-domains whether they lie in the applicability domain. Finally, since there are also NRPS that play an important role in natural products chemistry of fungi, such as peptaibols and cephalosporins, we added a predictor for fungal A-domains, which predicts gross physicochemical properties with an F-measure of 0.84. The service is available at http://nrps.informatik.uni-tuebingen.de/.				Weber, Tilmann/C-7159-2009; Kohlbacher, Oliver/B-7310-2008	Weber, Tilmann/0000-0002-8260-5120; Kohlbacher, Oliver/0000-0003-1739-4598; Blin, Kai/0000-0003-3764-6051; Medema, Marnix/0000-0002-2191-2821												0305-1048	1362-4962				JUL	2011	39			2			W362	W367		10.1093/nar/gkr323						WOS:000292325300059	21558170	J	Prado-Prado, FJ; Gonzalez-Diaz, H; de la Vega, OM; Ubeira, FM; Chou, KC				Prado-Prado, Francisco J.; Gonzalez-Diaz, Humberto; Martinez de la Vega, Octavio; Ubeira, Florencio M.; Chou, Kuo-Chen			Unified QSAR approach to antimicrobials. Part 3: First multi-tasking QSAR model for Input-Coded prediction, structural back-projection, and complex networks clustering of antiprotozoal compounds	BIOORGANIC & MEDICINAL CHEMISTRY												Several pathogen parasite species show different susceptibilities to different antiparasite drugs. Unfortunately, almost all structure-based methods are one-task or one-target Quantitative Structure-Activity Relationships (ot-QSAR) that predict the biological activity of drugs against only one parasite species. Consequently, multi-tasking learning to predict drugs activity against different species by a single model (mt-QSAR) is vitally important. In the two previous works of the present series we reported two single mt-QSAR models in order to predict the antimicrobial activity against different fungal (Bioorg. Med. Chem. 2006, 14, 5973-5980) or bacterial species (Bioorg. Med. Chem. 2007, 15, 897-902). These mt-QSARs offer a good opportunity (unpractical with ot-QSAR) to construct drug-drug similarity Complex Networks and to map the contribution of sub-structures to function for multiple species. These possibilities were unattended in our previous works. In the present work, we continue this series toward other important direction of chemotherapy (antiparasite drugs) with the development of an mt-QSAR for more than 500 drugs tested in the literature against different parasites. The data were processed by Linear Discriminant Analysis (LDA) classifying drugs as active or non-active against the different tested parasite species. The model correctly classifies 212 out of 244 (87.0%) cases in training series and 207 out of 243 compounds (85.4%) in external validation series. In order to illustrate the performance of the QSAR for the selection of active drugs we carried out an additional virtual screening of antiparasite compounds not used in training or predicting series; the model recognized 97 out of 114 (85.1%) of them. We also give the procedures to construct back-projection maps and to calculate sub-structures contribution to the biological activity. Finally, we used the outputs of the QSAR to construct, by the first time, a multi-species Complex Networks of antiparasite drugs. The network predicted has 380 nodes (compounds), 634 edges (pairs of compounds with similar activity). This network allows us to cluster different compounds and identify on average three known compounds similar to a new query compound according to their pro. le of biological activity. This is the first attempt to calculate probabilities of antiparasitic action of drugs against different parasites. (C) 2008 Elsevier Ltd. All rights reserved.				Prado-Prado, Francisco/B-8335-2012; Chou, Kuo-Chen/A-8340-2009; Martinez, Octavio/F-3951-2014; Gonzalez-Diaz, Humbert/A-6785-2012	Martinez, Octavio/0000-0002-7590-0041; Prado-Prado, Francisco Javier/0000-0002-2411-7727; Gonzalez-Diaz, Humbert/0000-0002-9392-2797; Ubeira, Florencio/0000-0001-7312-2752												0968-0896	1464-3391				JUN 1	2008	16	11					5871	5880		10.1016/j.bmc.2008.04.068						WOS:000256378200004	18485714	J	Stanley, KO; Miikkulainen, R				Stanley, KO; Miikkulainen, R			Competitive coevolution through evolutionary complexification	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												Two major goals in machine learning are the discovery and improvement of solutions to complex problems. In this paper, we argue that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals. We demonstrate the power of complexification through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures. NEAT is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head. Because the robot duel domain supports a wide range of strategies, and because coevolution benefits from an escalating arms race, it serves as a suitable testbed for studying complexification. When compared to the evolution of networks with fixed structure, complexifying evolution discovers significantly more sophisticated strategies. The results suggest that in order to discover and improve complex solutions, evolution, and search in general, should be allowed to complexify as well as optimize.																	1076-9757						2004	21						63	100								WOS:000188953000001		J	Kasturi, R; Goldgof, D; Soundararajan, P; Manohar, V; Garofolo, J; Bowers, R; Boonstra, M; Korzhova, V; Zhang, J				Kasturi, Rangachar; Goldgof, Dmitry; Soundararajan, Padmanabhan; Manohar, Vasant; Garofolo, John; Bowers, Rachel; Boonstra, Matthew; Korzhova, Valentina; Zhang, Jing			Framework for Performance Evaluation of Face, Text, and Vehicle Detection and Tracking in Video: Data, Metrics, and Protocol	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Common benchmark data sets, standardized performance metrics, and baseline algorithms have demonstrated considerable impact on research and development in a variety of application domains. These resources provide both consumers and developers of technology with a common framework to objectively compare the performance of different algorithms and algorithmic improvements. In this paper, we present such a framework for evaluating object detection and tracking in video: specifically for face, text, and vehicle objects. This framework includes the source video data, ground-truth annotations (along with guidelines for annotation), performance metrics, evaluation protocols, and tools including scoring software and baseline algorithms. For each detection and tracking task and supported domain, we developed a 50-clip training set and a 50-clip test set. Each data clip is approximately 2.5 minutes long and has been completely spatially/temporally annotated at the I-frame level. Each task/domain, therefore, has an associated annotated corpus of approximately 450,000 frames. The scope of such annotation is unprecedented and was designed to begin to support the necessary quantities of data for robust machine learning approaches, as well as a statistically significant comparison of the performance of algorithms. The goal of this work was to systematically address the challenges of object detection and tracking through a common evaluation framework that permits a meaningful objective comparison of techniques, provides the research community with sufficient data for the exploration of automatic modeling techniques, encourages the incorporation of objective evaluation into the development process, and contributes useful lasting resources of a scale and magnitude that will prove to be extremely useful to the computer vision research community for years to come.																	0162-8828					FEB	2009	31	2					319	336		10.1109/TPAMI.2008.57						WOS:000261846800009	19110496	J	Jung, M; Reichstein, M; Bondeau, A				Jung, M.; Reichstein, M.; Bondeau, A.			Towards global empirical upscaling of FLUXNET eddy covariance observations: validation of a model tree ensemble approach using a biosphere model	BIOGEOSCIENCES												Global, spatially and temporally explicit estimates of carbon and water fluxes derived from empirical up-scaling eddy covariance measurements would constitute a new and possibly powerful data stream to study the variability of the global terrestrial carbon and water cycle. This paper introduces and validates a machine learning approach dedicated to the upscaling of observations from the current global network of eddy covariance towers (FLUXNET). We present a new model TRee Induction ALgorithm (TRIAL) that performs hierarchical stratification of the data set into units where particular multiple regressions for a target variable hold. We propose an ensemble approach (Evolving tRees with RandOm gRowth, ERROR) where the base learning algorithm is perturbed in order to gain a diverse sequence of different model trees which evolves over time. We evaluate the efficiency of the model tree ensemble (MTE) approach using an artificial data set derived from the Lund-Potsdam-Jena managed Land (LPJmL) biosphere model. We aim at reproducing global monthly gross primary production as simulated by LPJmL from 1998-2005 using only locations and months where high quality FLUXNET data exist for the training of the model trees. The model trees are trained with the LPJmL land cover and meteorological input data, climate data, and the fraction of absorbed photosynthetic active radiation simulated by LPJmL. Given that we know the 'true result' in the form of global LPJmL simulations we can effectively study the performance of the MTE upscaling and associated problems of extrapolation capacity. We show that MTE is able to explain 92% of the variability of the global LPJmL GPP simulations. The mean spatial pattern and the seasonal variability of GPP that constitute the largest sources of variance are very well reproduced (96% and 94% of variance explained respectively) while the monthly interannual anomalies which occupy much less variance are less well matched (41% of variance explained). We demonstrate the substantially improved accuracy of MTE over individual model trees in particular for the monthly anomalies and for situations of extrapolation. We estimate that roughly one fifth of the domain is subject to extrapolation while MTE is still able to reproduce 73% of the LPJmL GPP variability here. This paper presents for the first time a benchmark for a global FLUXNET upscaling approach that will be employed in future studies. Although the real world FLUXNET upscaling is more complicated than for a noise free and reduced complexity biosphere model as presented here, our results show that an empirical upscaling from the current FLUXNET network with MTE is feasible and able to extract global patterns of carbon flux variability.				Bondeau, Alberte/E-9909-2012; Reichstein, Markus/A-7494-2011	Reichstein, Markus/0000-0001-5736-1112												1726-4170						2009	6	10					2001	2013								WOS:000271354900005		J	Wang, WJ; Xu, ZB; Lu, WZ; Zhang, XY				Wang, WJ; Xu, ZB; Lu, WZ; Zhang, XY			Determination of the spread parameter in the Gaussian kernel for classification and regression	NEUROCOMPUTING												Based on statistical learning theory, Support Vector Machine (SVM) is a novel type of learning machine, and it contains polynomial, neural network and radial basis function (RBF) as special cases. In the RBF case, the Gaussian kernel is commonly used, while the spread parameter sigma in the Gaussian kernel is essential to generalization performance of SVMs. In this paper, determination of sigma is studied based on discussions of the influence of sigma on generalization performance. For classification problems, the optimal sigma can be computed on the basis of Fisher discrimination. And for regression problems, based on scale space theory, we demonstrate the existence of a certain range of sigma, within which the generalization performance is stable. An appropriate sigma within the range can be achieved via dynamic evaluation. In addition, the lower bound of iterating step size of sigma is given. Simulation results show the effectiveness of the presented method. (C) 2002 Published by Elsevier B.V.				Lu, Wei-Zhen Jane/I-6301-2012	Lu, Wei-Zhen Jane/0000-0001-7421-347X												0925-2312					OCT	2003	55	3-4					643	663	PII S0925-2312(02)00632-X	10.1016/S0925-2312(02)00632-X						WOS:000186953400012		J	Kumar, MA; Gopal, M				Kumar, M. Arun; Gopal, M.			Least squares twin support vector machines for pattern classification	EXPERT SYSTEMS WITH APPLICATIONS												In this paper we formulate a least squares version of the recently proposed twin support vector machine (TSVM) for binary classification. This formulation leads to extremely simple and fast algorithm for generating binary classifiers based on two non-parallel hyperplanes. Here we attempt to solve two modified primal problems of TSVM. instead of two dual problems usually solved. We show that the solution of the two modified primal problems reduces to solving just two systems of linear equations as opposed to solving two quadratic programming problems along with two systems of linear equations in TSVM. Classification using nonlinear kernel also leads to systems of linear equations. our experiments on publicly available datasets indicate that the proposed least squares TSVM has comparable classification accuracy to that of TSVM but with considerably lesser computational time. Since linear least squares TSVM can easily handle large clatasets, we further went on to investigate its efficiency for text categorization applications. Computational results demonstrate the effectiveness of the proposed method over linear proximal SVM on all the text corpuses considered. (C) 2008 Elsevier Ltd. All rights reserved.																	0957-4174					MAY	2009	36	4					7535	7543		10.1016/j.eswa.2008.09.066						WOS:000264528600026		J	Li, J; Wang, JZ				Li, Jia; Wang, James Z.			Real-time computerized annotation of pictures	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Developing effective methods for automated annotation of digital pictures continues to challenge computer scientists. The capability of annotating pictures by computers can lead to breakthroughs in a wide range of applications, including Web image search, online picture-sharing communities, and scientific experiments. In this work, the authors developed new optimization and estimation techniques to address two fundamental problems in machine learning. These new techniques serve as the basis for the Automatic Linguistic Indexing of Pictures-Real Time (ALIPR) system of fully automatic and high-speed annotation for online pictures. In particular, the D2-clustering method, in the same spirit as K-Means for vectors, is developed to group objects represented by bags of weighted vectors. Moreover, a generalized mixture modeling technique (kernel smoothing as a special case) for nonvector data is developed using the novel concept of Hypothetical Local Mapping (HLM). ALIPR has been tested by thousands of pictures from an Internet photo-sharing site, unrelated to the source of those pictures used in the training process. Its performance has also been studied at an online demonstration site, where arbitrary users provide pictures of their choices and indicate the correctness of each annotation word. The experimental results show that a single computer processor can suggest annotation terms in real time and with good accuracy.																	0162-8828					JUN	2008	30	6					985	1002		10.1109/TPAMI.2007.70847						WOS:000254872500005	18421105	J	Zheng, R; Li, JX; Chen, HC; Huang, Z				Zheng, R; Li, JX; Chen, HC; Huang, Z			A framework for authorship identification of Online messages: Writing-style features and classification techniques	JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY												With the rapid proliferation of Internet technologies and applications, misuse of online messages for inappropriate or illegal purposes has become a major concern for society. The anonymous nature of online-message distribution makes identity tracing a critical problem. We developed a framework for authorship identification of online messages to address the identity-tracing problem. In this framework, four types of writing-style features (lexical, syntactic, structural, and content-specific features) are extracted and inductive learning algorithms are used to build feature-based classification models to identify authorship of online messages. To examine this framework, we conducted experiments on English and Chinese online-newsgroup messages. We compared the discriminating power of the four types of features and of three classification techniques: decision trees, back-propagation neural networks, and support vector machines. The experimental results showed that the proposed approach was able to identify authors of online messages with satisfactory accuracy of 70 to 95%. All four types of message features contributed to discriminating authors of online messages. Support vector machines outperformed the other two classification techniques in our experiments. The high performance we achieved for both the English and Chinese datasets showed the potential of this approach in a multiple-language context.																	1532-2882					FEB 1	2006	57	3					378	393		10.1002/asi.20316						WOS:000234932600009		J	Brand, M; Kettnaker, V				Brand, M; Kettnaker, V			Discovery and segmentation of activities in video	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												Hidden Markov models (HMMs) have become the workhorses of the monitoring and event recognition literature because they bring to time-series analysts the utility of density estimation and the convenience of dynamic time warping. Once trained. the internals of these models are considered opaque; there is no effort to interpret the hidden states. We show that by minimizing the entropy of the joint distribution, an HMM's internal state machine can be made to organize observed activity into meaningful states. This has uses in video monitoring and annotation, low bit-rate coding of scene activity. and detection of anomalous behavior. We demonstrate with models of office activity and outdoor traffic, showing how the framework learns principal modes of activity and patterns of activity change. We then show how this framework can be adapted to infer hidden state from extremely ambiguous images. In particular, inferring 3D body orientation and pose from sequences of low-resolution silhouettes.																	0162-8828					AUG	2000	22	8					844	851		10.1109/34.868685						WOS:000089321500010		J	Larochelle, H; Bengio, Y; Louradour, J; Lamblin, P				Larochelle, Hugo; Bengio, Yoshua; Louradour, Jerome; Lamblin, Pascal			Exploring Strategies for Training Deep Neural Networks	JOURNAL OF MACHINE LEARNING RESEARCH												Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.																	1532-4435					JAN	2009	10						1	40								WOS:000270824100001		J	Adamczak, R; Porollo, A; Meller, J				Adamczak, R; Porollo, A; Meller, J			Combining prediction of secondary structure and solvent accessibility in proteins	PROTEINS-STRUCTURE FUNCTION AND BIOINFORMATICS												Owing to the use of evolutionary information and advanced machine learning protocols, secondary structures of amino acid residues in proteins can be predicted from the primary sequence with more than 75% per-residue accuracy for the 3-state (i.e., helix, beta-strand, and coil) classification problem. In this work we investigate whether further progress may be achieved by incorporating the relative solvent accessibility (RSA) of an amino acid residue as a fingerprint of the overall topology of the protein. Toward that goal, we developed a novel method for secondary structure prediction that uses predicted RSA in addition to attributes derived from evolutionary profiles. Our general approach follows the 2-stage protocol of Rost and Sander, with a number of Elman-type recurrent neural networks (NNs) combined into a consensus predictor. The RSA is predicted using our recently developed regression-based method that provides real-valued RSA, with the overall correlation coefficients between the actual and predicted RSA of about 0.66 in rigorous tests on independent control sets. Using the predicted RSA, we were able to improve the performance of our secondary structure prediction by up to 1.4% and achieved the overall per-residue accuracy between 77.0% and 78.4% for the 3-state classification problem on different control sets comprising, together, 603 proteins without homology to proteins included in the training. The effects of including solvent accessibility depend on the quality of RSA prediction. In the limit of perfect prediction (i.e., when using the actual RSA values derived from known protein structures), the accuracy of secondary structure prediction increases by up to 4%. We also observed that projecting real-valued RSA into 2 discrete classes with the commonly used threshold of 25% RSA decreases the classification accuracy for secondary structure prediction. While the level of improvement of secondary structure prediction may be different for prediction protocols that implicitly account for RSA in other ways, we conclude that an increase in the 3-state classification accuracy may be achieved when combining RSA with a state-of-theart protocol utilizing evolutionary profiles. The new method is available through a Web server at http://sable.cchme.org. (c) 2005 Wiley-Liss, Inc.				Meller, Jaroslaw/A-1971-2011; de Sousa, Miguel/A-3877-2009; Adamczak, Rafal/F-2965-2014	Meller, Jaroslaw/0000-0002-1162-8253; Porollo, Alexey/0000-0002-3202-5099												0887-3585					MAY 15	2005	59	3					467	475		10.1002/prot.20441						WOS:000228779200006	15768403	J	Yasui, Y; Pepe, M; Thompson, ML; Adam, BL; Wright, GL; Qu, YS; Potter, JD; Winget, M; Thornquist, M; Feng, ZD				Yasui, Y; Pepe, M; Thompson, ML; Adam, BL; Wright, GL; Qu, YS; Potter, JD; Winget, M; Thornquist, M; Feng, ZD			A data-analytic strategy for protein biomarker discovery: profiling of high-dimensional proteomic data for cancer detection	BIOSTATISTICS												With recent advances in mass spectrometry techniques, it is now possible to investigate proteins over a wide range of molecular weights in small biological specimens. This advance has generated data-analytic challenges in proteomics, similar to those created by microarray technologies in genetics, namely, discovery of 'signature' protein profiles specific to each pathologic state (e.g. normal vs. cancer) or differential profiles between experimental conditions (e.g. treated by a drug of interest vs. untreated) from high-dimensional data. We propose a data-analytic strategy for discovering protein biomarkers based on such high-dimensional mass spectrometry data. A real biomarker-discovery project on prostate cancer is taken as a concrete example throughout the paper: the project aims to identify proteins in serum that distinguish cancer, benign hyperplasia, and normal states of prostate using the Surface Enhanced Laser Desorption/Ionization (SELDI) technology, a recently developed mass spectrometry technique. Our data-analytic strategy takes properties of the SELDI mass spectrometer into account: the SELDI output of a specimen contains about 48 000 (x, y) points where x is the protein mass divided by the number of charges introduced by ionization and y is the protein intensity of the corresponding mass per charge value, x, in that specimen. Given high coefficients of variation and other characteristics of protein intensity measures (y values), we reduce the measures of protein intensities to a set of binary variables that indicate peaks in the y-axis direction in the nearest neighborhoods of each mass per charge point in the x-axis direction. We then account for a shifting (measurement error) problem of the x-axis in SELDI output. After this pre-analysis processing of data, we combine the binary predictors to generate classification rules for cancer, benign hyperplasia, and normal states of prostate. Our approach is to apply the boosting algorithm to select binary predictors and construct a summary classifier. We empirically evaluate sensitivity and specificity of the resulting summary classifiers with a test dataset that is independent from the training dataset used to construct the summary classifiers. The proposed method performed nearly perfectly in distinguishing cancer and benign hyperplasia from normal. In the classification of cancer vs. benign hyperplasia, however, an appreciable proportion of the benign specimens were classified incorrectly as cancer. We discuss practical issues associated with our proposed approach to the analysis of SELDI output and its application in cancer biomarker discovery.					Potter, John/0000-0001-5439-1500												1465-4644	1468-4357				JUL	2003	4	3					449	463		10.1093/biostatistics/4.3.449						WOS:000184100800011	12925511	J	Down, TA; Hubbard, TJP				Down, TA; Hubbard, TJP			Computational detection and location of transcription start sites in mammalian genomic DNA	GENOME RESEARCH												Transcription, the process whereby RNA copies are made from sections of the DNA genome, is directed by promoter regions. These define the transcription start site, and also the set of cellular conditions under which the promoter is active. At least in more complex species, it appears to be common for genes to have several different transcription start sites, which may be active under different conditions. Eukaryotic promoters are complex and fairly diffuse structures, which have proven hard to detect in sillco. We show that a novel hybrid machine-learning method is able to build useful models of promoters for >50% of human transcription start sites. We estimate specificity to be >70%, and demonstrate good positional accuracy. Based on the structure of our learned models, we conclude that a signal resembling the well known TATA box, together with flanking regions of C-G enrichment, are the most important sequence-based signals marking sites of transcriptional initiation at a large class of typical promoters.					Hubbard, Tim @timjph/0000-0002-1767-9318												1088-9051					MAR	2002	12	3					458	461		10.1101/gr.216102						WOS:000174171300012	11875034	J	Wang, F; Zhang, CS				Wang, Fei; Zhang, Changshui			Label propagation through linear Neighborhoods	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												In many practical data mining applications such as text classification, unlabeled training examples are readily available, but labeled ones are fairly expensive to obtain. Therefore, semisupervised learning algorithms have aroused considerable interests from the data mining and machine learning fields. In recent years, graph-based semisupervised learning has been becoming one of the most active research areas in the semisupervised learning community. In this paper, a novel graph- based semisupervised learning approach is proposed based on a linear neighborhood model, which assumes that each data point can be linearly reconstructed from its neighborhood. Our algorithm, named Linear Neighborhood Propagation ( LNP), can propagate the labels from the labeled points to the whole data set using these linear neighborhoods with sufficient smoothness. A theoretical analysis of the properties of LNP is presented in this paper. Furthermore, we also derive an easy way to extend LNP to out-of-sample data. Promising experimental results are presented for synthetic data, digit, and text classification tasks.					Wang, Fei/0000-0001-9459-9461												1041-4347					JAN	2008	20	1					55	67		10.1109/TKDE.2007.190672						WOS:000251003300005		J	Lo, HK				Lo, HK			Insecurity of quantum secure computations	PHYSICAL REVIEW A												It had been widely claimed that quantum mechanics can protect private information during public decision in, for example, the so-called two-party secure computation. If this were the case, quantum smart-cards, storing confidential information accessible only to a proper reader, could prevent fake teller machines from learning the PIN (personal identification number) from the customers' input. Although such optimism has been challenged by the recent surprising discovery of the insecurity of the so-called quantum bit commitment, the security of quantum two-party computation itself remains unaddressed. Here I answer this question directly by showing that all one-sided two-party computations (which allow only one of the two parties to learn the result) are necessarily insecure. As corollaries to my results, quantum one-way oblivious password identification and the so-called quantum one-out-of-two oblivious transfer are impossible. I also construct a class of functions that cannot be computed securely in any two-sided two-party computation. Nevertheless, quantum cryptography remains useful in key distribution and can still provide partial security in ''quantum money'' proposed by Wiesner.																	1050-2947					AUG	1997	56	2					1154	1162		10.1103/PhysRevA.56.1154						WOS:A1997XQ61800019		J	Huang, GB				Huang, Guang-Bin			An Insight into Extreme Learning Machines: Random Neurons, Random Features and Kernels	COGNITIVE COMPUTATION												Extreme learning machines (ELMs) basically give answers to two fundamental learning problems: (1) Can fundamentals of learning (i.e., feature learning, clustering, regression and classification) be made without tuning hidden neurons (including biological neurons) even when the output shapes and function modeling of these neurons are unknown? (2) Does there exist unified framework for feedforward neural networks and feature space methods? ELMs that have built some tangible links between machine learning techniques and biological learning mechanisms have recently attracted increasing attention of researchers in widespread research areas. This paper provides an insight into ELMs in three aspects, viz: random neurons, random features and kernels. This paper also shows that in theory ELMs (with the same kernels) tend to outperform support vector machine and its variants in both regression and classification applications with much easier implementation.				Huang, Guang-Bin/A-5035-2011	Huang, Guang-Bin/0000-0002-2480-4965												1866-9956	1866-9964				SEP	2014	6	3			SI		376	390		10.1007/s12559-014-9255-2						WOS:000341593600009		J	Zhang, ML; Zhou, ZH				Zhang, Min-Ling; Zhou, Zhi-Hua			A Review on Multi-Label Learning Algorithms	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING												Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.																	1041-4347	1558-2191				AUG	2014	26	8					1819	1837		10.1109/TKDE.2013.39						WOS:000341570800001		J	Wang, M; Hua, XS; Tang, JH; Hong, RC				Wang, Meng; Hua, Xian-Sheng; Tang, Jinhui; Hong, Richang			Beyond Distance Measurement: Constructing Neighborhood Similarity for Video Annotation	IEEE TRANSACTIONS ON MULTIMEDIA												In the past few years, video annotation has benefited a lot from the progress of machine learning techniques. Recently, graph-based semi-supervised learning has gained much attention in this domain. However, as a crucial factor of these algorithms, the estimation of pairwise similarity has not been sufficiently studied. Generally, the similarity of two samples is estimated based on the Euclidean distance between them. But we will show that the similarity between two samples is not merely related to their distance but also related to the distribution of surrounding samples and labels. It is shown that the traditional distance-based similarity measure may lead to high classification error rates even on several simple datasets. To address this issue, we propose a novel neighborhood similarity measure, which explores the local sample and label distributions. We show that the neighborhood similarity between two samples simultaneously takes into account three characteristics: 1) their distance; 2) the distribution difference of the surrounding samples; and 3) the distribution difference of surrounding labels. Extensive experiments have demonstrated the superiority of neighborhood similarity over the existing distance-based similarity.																	1520-9210	1941-0077				APR	2009	11	3					465	476		10.1109/TMM.2009.2012919						WOS:000264632300012		J	Ghosh-Dastidar, S; Adeli, H				Ghosh-Dastidar, Samanwoy; Adeli, Hojjat			Improved spiking neural networks for EEG classification and epilepsy and seizure detection	INTEGRATED COMPUTER-AIDED ENGINEERING												The goal of this research is to develop an efficient SNN model for epilepsy and epileptic seizure detection using electroencephalograms (EEGs), a complicated pattern recognition problem. Three training algorithms are investigated: SpikeProp (using both incremental and batch processing), QuickProp, and RProp. Since the epilepsy and epileptic seizure detection problem requires a large training dataset the efficacy of these algorithms is investigated by first applying them to the XOR and Fisher iris benchmark problems. Three measures of performance are investigated: number of convergence epochs, computational efficiency, and classification accuracy. Extensive parametric analysis is performed to identify heuristic rules and optimum parameter values that increase the computational efficiency and classification accuracy. The result is a remarkable increase in computational efficiency. For the XOR problem, the computational efficiency of SpikeProp, QuickProp, and RProp is increased by a factor of 588, 82, and 75, respectively, compared with the results reported in the literature. EEGs from three different subject groups are analyzed: (a) healthy subjects, (b) epileptic subjects during a seizure-free interval, and (c) epileptic subjects during a seizure. It is concluded that RProp is the best training algorithm because it has the highest classification accuracy among all training algorithms specially for large size training datasets with about the same computational efficiency provided by SpikeProp. The SNN model for EEG classification and epilepsy and seizure detection uses RProp as training algorithm. This model yields a high classification accuracy of 92.5%.				Ghosh-Dastidar, Samanwoy/A-5232-2008; adeli, hojjat/D-1430-2010	adeli, hojjat/0000-0001-5718-1453												1069-2509						2007	14	3					187	212								WOS:000248283800001		J	Gilboa, G; Osher, S				Gilboa, Guy; Osher, Stanley			Nonlocal linear image regularization and supervised segmentation	MULTISCALE MODELING & SIMULATION												A nonlocal quadratic functional of weighted di. erences is examined. The weights are based on image features and represent the a. nity between di. erent pixels in the image. By prescribing di. erent formulas for the weights, one can generalize many local and nonlocal linear denoising algorithms, including the nonlocal means. lter and the bilateral. lter. In this framework one can easily show that continuous iterations of the generalized. lter obey certain global characteristics and converge to a constant solution. The linear operator associated with the Euler-Lagrange equation of the functional is closely related to the graph Laplacian. We can thus interpret the steepest descent for minimizing the functional as a nonlocal di. usion process. This formulation allows a convenient framework for nonlocal variational minimizations, including variational denoising, Bregman iterations, and the recently proposed inverse scale space. It is also demonstrated how the steepest descent. ow can be used for segmentation. Following kernel based methods in machine learning, the generalized di. usion process is used to propagate sporadic initial user's information to the entire image. Unlike classical variational segmentation methods, the process is not explicitly based on a curve length energy and thus can cope well with highly nonconvex shapes and corners. Reasonable robustness to noise is still achieved.																	1540-3459	1540-3467					2007	6	2					595	630		10.1137/060669358						WOS:000249318500011		J	Kim, KI; Jung, K; Kim, HJ				Kim, KI; Jung, K; Kim, HJ			Face recognition using kernel principal component analysis	IEEE SIGNAL PROCESSING LETTERS												A kernel principal component analysis (PCA) was recently proposed as a nonlinear extension of a PCA. The basic idea is to first map the input space into a feature space via nonlinear mapping and then compute the principal components in that feature space. This letter adopts the kernel PCA as a mechanism for extracting facial features. Through adopting a polynomial kernel, the principal components can be computed within the space spanned by high-order correlations of input pixels making up a facial image, thereby producing a good performance.																	1070-9908					FEB	2002	9	2					40	42	PII S1070-9908(02)03400-4							WOS:000174493300002		J	Chandrashekar, G; Sahin, F				Chandrashekar, Girish; Sahin, Ferat			A survey on feature selection methods	COMPUTERS & ELECTRICAL ENGINEERING												Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques. (C) 2013 Elsevier Ltd. All rights reserved.																	0045-7906	1879-0755				JAN	2014	40	1					16	28		10.1016/j.compeleceng.2013.11.024						WOS:000331922800003		J	Mohabatkar, H; Beigi, MM; Esmaeili, A				Mohabatkar, Hassan; Beigi, Majid Mohammad; Esmaeili, Abolghasem			Prediction of GABA(A) receptor proteins using the concept of Chou's pseudo-amino acid composition and support vector machine	JOURNAL OF THEORETICAL BIOLOGY												The amino acid gamma-aminobutyric-acid receptors (GABA(A)Rs) belong to the ligand-gated ion channels (LGICs) superfamily. GABAARs are highly diverse in the central nervous system. These channels play a key role in regulating behavior. As a result, the prediction of GABAARs from the amino acid sequence would be helpful for research on these receptors. We have developed a method to predict these proteins using the features obtained from Chou's pseudo-amino acid composition concept and support vector machine as a powerful machine learning approach. The predictor efficiency was assessed by five-fold cross-validation. This method achieved an overall accuracy and Matthew's correlation coefficient (MCC) of 94.12% and 0.88, respectively. Furthermore, to evaluate the effect and power of each feature, the minimum Redundancy and Maximum Relevance (mRMR) feature selection method was implemented. An interesting finding in this study is the presence of all six characters (hydrophobicity, hydrophilicity, side chain mass, pK1, pK2 and pI) or combination of the characters among the 5 higher ranked features (pk2 and pI, hydrophobicity and mass, pk1, hydrophilicity and mass) obtained from the mRMR feature selection method. The results show a biologically justifiable ranked attributes of pk2 and pI; hydrophobicity, hydrophilicity and mass; mass and pk1; pk2 and mass. Based on our results, using the concept of Chou's pseudo-amino acid composition and support vector machine is an effective approach for the prediction of GABAARs. (C) 2011 Published by Elsevier Ltd.																	0022-5193					JUL 21	2011	281	1					18	23		10.1016/j.jtbi.2011.04.017						WOS:000291913700003	21536049	J	Liu, WF; Pokharel, PP; Principe, JC				Liu, Weifeng; Pokharel, Puskal P.; Principe, Jose C.			The kernel least-mean-square algorithm	IEEE TRANSACTIONS ON SIGNAL PROCESSING												The combination of the famed kernel trick and the least-mean-square (LMS) algorithm provides an interesting sample-by-sample update for an adaptive filter in reproducing kernel Hilbert spaces (RKHS), which is named in this paper the KLMS. Unlike the accepted view in kernel methods, this paper shows that in the finite training data case, the KLMS algorithm is well posed in RKHS without the addition of an extra regularization term to penalize solution norms as was suggested by Kivinen [Kivinen, Smola and Williamson, "Online Learning With Kernels," IEEE Transactions on Signal Processing, vol. 52, no. 8, pp. 2165-2176, Aug. 2004] and Smale [Smale and Yao, "Online Learning Algorithms," Foundations in Computational Mathematics, vol. 6, no. 2, pp. 145-176, 2006]. This result is the main contribution of the paper and enhances the present understanding of the LMS algorithm with a machine learning perspective. The effect of the KLMS step size is also studied from the viewpoint of regularization. Two experiments are presented to support our conclusion that with finite data the KLMS algorithm can be readily used in high dimensional spaces and particularly in RKHS to derive nonlinear, stable algorithms with comparable performance to batch, regularized solutions.																	1053-587X	1941-0476				FEB	2008	56	2					543	554		10.1109/TSP.2007.907881						WOS:000252575200009		J	Kim, KI; Jung, K; Park, SH; Kim, HJ				Kim, KI; Jung, K; Park, SH; Kim, HJ			Support vector machines for texture classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												This paper investigates the application of support vector machines (SVMs) in texture classification. Instead of relying on an external feature extractor, the SVM receives the gray-level values of the raw pixels, as SVMs can generalize well even in high-dimensional spaces. Furthermore, it is shown that SVMs can incorporate conventional texture feature extraction methods within their own architecture, while also providing solutions to problems inherent in these methods. One-against-others decomposition is adopted to apply binary SVMs to multitexture classification, plus a neural network is used as an arbitrator to make final classifications from several one-against-others SVM outputs. Experimental results demonstrate the effectiveness of SVMs in texture classification.																	0162-8828					NOV	2002	24	11					1542	1550								WOS:000178846400012		J	Suykens, JAK; Vandewalle, J				Suykens, JAK; Vandewalle, J			Recurrent least squares support vector machines	IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I-FUNDAMENTAL THEORY AND APPLICATIONS												The method of support vector machines (SVM's) has been developed for solving classification and static function approximation problems. In this paper we introduce SVM's within the context of recurrent neural networks. Instead of Vapnik's epsilon insensitive loss function, we consider a least squares version related to a cost function with equality constraints for a recurrent network, Essential features of SVM's remain, such as Mercer's condition and the fact that the output weights are a Lagrange multiplier weighted sum of the data points, The solution to recurrent least squares (LS-SVM's) is characterized by a set of nonlinear equations. Due to its high computational complexity, we focus on a limited case of assigning the squared error an infinitely large penalty factor with early stopping as a form of regularization, The effectiveness of the approach is demonstrated on trajectory learning of the double scroll attractor in Chua's circuit.				Suykens, Johan/C-9781-2014													1057-7122					JUL	2000	47	7					1109	1114		10.1109/81.855471						WOS:000088504900020		J	Caselli, F				Caselli, F			Technological revolutions	AMERICAN ECONOMIC REVIEW												In skill-biased (de-skilling) technological revolutions learning investments required by new machines are greater (smaller) than those required by preexisting machines. Skill-biased (de-skilling) revolutions trigger reallocations of capital from slow- (fast-) to fast- (slow-) learning workers, thereby reducing the relative and absolute wages of the former. The model of skill-biased (de-skilling) revolutions provides insight into developments since the mid-1970's (in the 1910's). The empirical work documents a large increase in the interindustry dispersion of capital-labor ratios since 1975. Changes in industry capital intensity are related to the skill composition of the labor force. (JEL E23 J31 O33).																	0002-8282					MAR	1999	89	1					78	102		10.1257/aer.89.1.78						WOS:000079411900004		J	Thelwall, M; Buckley, K; Paltoglou, G				Thelwall, Mike; Buckley, Kevan; Paltoglou, Georgios			Sentiment Strength Detection for the Social Web	JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY												Sentiment analysis is concerned with the automatic extraction of sentiment-related information from text. Although most sentiment analysis addresses commercial tasks, such as extracting opinions from product reviews, there is increasing interest in the affective dimension of the social web, and Twitter in particular. Most sentiment analysis algorithms are not ideally suited to this task because they exploit indirect indicators of sentiment that can reflect genre or topic instead. Hence, such algorithms used to process social web texts can identify spurious sentiment patterns caused by topics rather than affective phenomena. This article assesses an improved version of the algorithm SentiStrength for sentiment strength detection across the social web that primarily uses direct indications of sentiment. The results from six diverse social web data sets (MySpace, Twitter, YouTube, Digg, Runners World, BBC Forums) indicate that SentiStrength 2 is successful in the sense of performing better than a baseline approach for all data sets in both supervised and unsupervised cases. SentiStrength is not always better than machine-learning approaches that exploit indirect indicators of sentiment, however, and is particularly weaker for positive sentiment in news-related discussions. Overall, the results suggest that, even unsupervised, SentiStrength is robust enough to be applied to a wide variety of different social web contexts.				Thelwall, Mike/C-1449-2013	Thelwall, Mike/0000-0001-6065-205X												1532-2882					JAN	2012	63	1					163	173		10.1002/asi.21662						WOS:000302158800014		J	Shalev-Shwartz, S; Singer, Y; Srebro, N; Cotter, A				Shalev-Shwartz, Shai; Singer, Yoram; Srebro, Nathan; Cotter, Andrew			Pegasos: primal estimated sub-gradient solver for SVM	MATHEMATICAL PROGRAMMING												We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy epsilon is (O) over tilde (1/epsilon), where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require Omega(1/epsilon(2)) iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/lambda, where. is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is (O) over tilde (d/(lambda epsilon)), where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods.																	0025-5610					MAR	2011	127	1			SI		3	30		10.1007/s10107-010-0420-4						WOS:000288371400002		J	Brosch, M; Yu, L; Hubbard, T; Choudhary, J				Brosch, Markus; Yu, Lu; Hubbard, Tim; Choudhary, Jyoti			Accurate and Sensitive Peptide Identification with Mascot Percolator	JOURNAL OF PROTEOME RESEARCH												Sound scoring methods for sequence database search algorithms such as Mascot and Sequest are essential for sensitive and accurate peptide and protein identifications from proteomic tandem mass spectrometry data. In this paper, we present a software package that interfaces Mascot with Percolator, a well performing machine learning method for rescoring database search results, and demonstrate it to be amenable for both low and high accuracy mass spectrometry data, outperforming all available Mascot scoring schemes as well as providing reliable significance measures. Mascot Percolator can be readily used as a stand alone tool or integrated into existing data analysis pipelines.				Hubbard, Tim/C-2567-2008	Hubbard, Tim/0000-0002-1767-9318												1535-3893					JUN	2009	8	6					3176	3181		10.1021/pr800982s						WOS:000266719400053	19338334	J	Inza, I; Larranaga, P; Blanco, R; Cerrolaza, AJ				Inza, I; Larranaga, P; Blanco, R; Cerrolaza, AJ			Filter versus wrapper gene selection approaches in DNA microarray domains	ARTIFICIAL INTELLIGENCE IN MEDICINE												DNA microarray experiments generating thousands of gene expression measurements, are used to collect information from tissue and cell samples regarding gene expression differences that could be useful for diagnosis disease, distinction of the specific tumor type, etc. One important application of gene expression microarray data is the classification of samples into known categories. As DNA microarray technology measures the gene expression en masse, this has resulted in data with the number of features (genes) far exceeding the number of samples. As the predictive accuracy of supervised classifiers that try to discriminate between the classes of the problem decays with the existence of irrelevant and redundant features, the necessity of a dimensionality reduction process is essential. We propose the application of a gene selection process, which also enables the biology researcher to focus on promising gene candidates that actively contribute to classification in these large scale microarrays. Two basic approaches for feature selection appear in machine learning and pattern recognition literature: the filter and wrapper techniques. Filter procedures are used in most of the works in the area of DNA microarrays. In this work, a comparison between a group of different filter metrics and a wrapper sequential search procedure is carried out. The comparison is performed in two well-known DNA microarray datasets by the use of four classic supervised classifiers. The study is carried out over the original-continuous and three-intervals discretized gene expression data. White two well-known filter metrics are proposed for continuous data, four classic filter measures are used over discretized data. The same wrapper approach is used for both continuous and discretized data. The application of fitter and wrapper gene selection procedures leads to considerably better accuracy results in comparison to the non-gene selection approach, coupled with interesting and notable dimensionality reductions. Although the wrapper approach mainly shows a more accurate behavior than fitter metrics, this improvement is coupled with considerable computer-load necessities. We note that most of the genes selected by proposed fitter and wrapper procedures in discrete and continuous microarray data appear in the lists of retevant-informative genes detected by previous studies over these datasets. The aim of this work is to make contributions in the field of the gene selection task in DNA microarray datasets. By an extensive comparison with more popular fitter techniques, we would like to make contributions in the expansion and study of the wrapper approach in this type of domains. (C) 2004 Published by Elsevier Ltd.				Larranaga, Pedro/F-9293-2013	Larranaga, Pedro/0000-0003-0652-9872; INZA CANO, INAKI/0000-0003-4674-1755												0933-3657					JUN	2004	31	2					91	103		10.1016/j.artmed.2004.01.007						WOS:000223503500002	15219288	J	Thottan, M; Ji, C				Thottan, M; Ji, C			Anomaly detection in IP networks	IEEE TRANSACTIONS ON SIGNAL PROCESSING												Network anomaly detection is a vibrant research area. Researchers have approached this problem using various techniques such as artificial intelligence, machine learning, and state machine modeling. In this paper, we first review these anomaly detection methods and then describe in detail a statistical signal processing technique based on abrupt change detection. We show that this signal processing technique is effective at detecting several network anomalies. Case studies from real network data that demonstrate the power of the signal, processing approach to network anomaly detection are presented. The application of signal processing techniques to this area is still in its infancy, and we believe that it has great potential to enhance the field, and thereby improve the reliability of IP networks.																	1053-587X	1941-0476				AUG	2003	51	8					2191	2204		10.1109/TSP.2003.814797						WOS:000184090200015		J	Guo, GD; Li, SZ; Chan, KL				Guo, GD; Li, SZ; Chan, KL			Support vector machines for face recognition	IMAGE AND VISION COMPUTING												Support vector machines (SVMs) have been recently proposed as a new learning network for bipartite pattern recognition. In this paper, SVMs incorporated with a binary tree recognition strategy are proposed to tackle the multi-class face recognition problem. The binary tree extends naturally, the pairwise discrimination capability of the SVMs to the multi-class scenario. Two face databases are used to evaluate the proposed method. The performance of the SVMs based face recognition is compared with the standard eigenface approach, and also the more recently proposed algorithm called the nearest feature line (NFL). (C) 2001 Elsevier Science B.V. All rights reserved.				Chan , Kap Luk/A-5150-2011	Guo, Guodong/0000-0001-9583-0055												0262-8856					AUG 1	2001	19	9-10					631	638		10.1016/S0262-8856(01)00046-4						WOS:000170745500005		J	Werner, JJ; Knights, D; Garcia, ML; Scalfone, NB; Smith, S; Yarasheski, K; Cummings, TA; Beers, AR; Knight, R; Angenent, LT				Werner, Jeffrey J.; Knights, Dan; Garcia, Marcelo L.; Scalfone, Nicholas B.; Smith, Samual; Yarasheski, Kevin; Cummings, Theresa A.; Beers, Allen R.; Knight, Rob; Angenent, Largus T.			Bacterial community structures are unique and resilient in full-scale bioenergy systems	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA												Anaerobic digestion is the most successful bioenergy technology worldwide with, at its core, undefined microbial communities that have poorly understood dynamics. Here, we investigated the relationships of bacterial community structure (> 400,000 16S rRNA gene sequences for 112 samples) with function (i.e., bioreactor performance) and environment (i.e., operating conditions) in a year-long monthly time series of nine full-scale bioreactor facilities treating brewery wastewater (> 20,000 measurements). Each of the nine facilities had a unique community structure with an unprecedented level of stability. Using machine learning, we identified a small subset of operational taxonomic units (OTUs; 145 out of 4,962), which predicted the location of the facility of origin for almost every sample (96.4% accuracy). Of these 145 OTUs, syntrophic bacteria were systematically overrepresented, demonstrating that syntrophs rebounded following disturbances. This indicates that resilience, rather than dynamic competition, played an important role in maintaining the necessary syntrophic populations. In addition, we explained the observed phylogenetic differences between all samples on the basis of a subset of environmental gradients (using constrained ordination) and found stronger relationships between community structure and its function rather than its environment. These relationships were strongest for two performance variables-methanogenic activity and substrate removal efficiency-both of which were also affected by microbial ecology because these variables were correlated with community evenness (at any given time) and variability in phylogenetic structure (over time), respectively. Thus, we quantified relationships between community structure and function, which opens the door to engineer communities with superior functions.				Garcia, Marcelo/H-1940-2012; Yarasheski, Kevin/A-3025-2008; Knight, Rob/D-1299-2010	Garcia, Marcelo/0000-0002-6002-3840; Yarasheski, Kevin/0000-0001-5436-2451												0027-8424					MAR 8	2011	108	10					4158	4163		10.1073/pnas.1015676108						WOS:000288120400064	21368115	J	Hu, PZ; Janga, SC; Babu, M; Diaz-Mejia, JJ; Butland, G; Yang, WH; Pogoutse, O; Guo, XH; Phanse, S; Wong, P; Chandran, S; Christopoulos, C; Nazarians-Armavil, A; Nasseri, NK; Musso, G; Ali, M; Nazemof, N; Eroukova, V; Golshani, A; Paccanaro, A; Greenblatt, JF; Moreno-Hagelsieb, G; Emili, A				Hu, Pingzhao; Janga, Sarath Chandra; Babu, Mohan; Diaz-Mejia, J. Javier; Butland, Gareth; Yang, Wenhong; Pogoutse, Oxana; Guo, Xinghua; Phanse, Sadhna; Wong, Peter; Chandran, Shamanta; Christopoulos, Constantine; Nazarians-Armavil, Anaies; Nasseri, Negin Karimi; Musso, Gabriel; Ali, Mehrab; Nazemof, Nazila; Eroukova, Veronika; Golshani, Ashkan; Paccanaro, Alberto; Greenblatt, Jack F.; Moreno-Hagelsieb, Gabriel; Emili, Andrew			Global Functional Atlas of Escherichia coli Encompassing Previously Uncharacterized Proteins	PLOS BIOLOGY												One-third of the 4,225 protein-coding genes of Escherichia coli K-12 remain functionally unannotated (orphans). Many map to distant clades such as Archaea, suggesting involvement in basic prokaryotic traits, whereas others appear restricted to E. coli, including pathogenic strains. To elucidate the orphans' biological roles, we performed an extensive proteomic survey using affinity-tagged E. coli strains and generated comprehensive genomic context inferences to derive a high-confidence compendium for virtually the entire proteome consisting of 5,993 putative physical interactions and 74,776 putative functional associations, most of which are novel. Clustering of the respective probabilistic networks revealed putative orphan membership in discrete multiprotein complexes and functional modules together with annotated gene products, whereas a machine-learning strategy based on network integration implicated the orphans in specific biological processes. We provide additional experimental evidence supporting orphan participation in protein synthesis, amino acid metabolism, biofilm formation, motility, and assembly of the bacterial cell envelope. This resource provides a "systems-wide" functional blueprint of a model microbe, with insights into the biological and evolutionary significance of previously uncharacterized proteins.				cheepala, satish/H-6173-2011	Hu, Pingzhao/0000-0002-9546-2245; Janga, Sarath Chandra/0000-0001-7351-6268												1545-7885					APR	2009	7	4					929	947	e1000096	10.1371/journal.pbio.1000096						WOS:000266500000018	19402753	J	Jones, TR; Carpenter, AE; Lamprecht, MR; Moffat, J; Silver, SJ; Grenier, JK; Castoreno, AB; Eggert, US; Root, DE; Golland, P; Sabatini, DM				Jones, Thouis R.; Carpenter, Anne E.; Lamprecht, Michael R.; Moffat, Jason; Silver, Serena J.; Grenier, Jennifer K.; Castoreno, Adam B.; Eggert, Ulrike S.; Root, David E.; Golland, Polina; Sabatini, David M.			Scoring diverse cellular morphologies in image-based screens with iterative feedback and machine learning	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA												Many biological pathways were first uncovered by identifying mutants with visible phenotypes and by scoring every sample in a screen via tedious and subjective visual inspection. Now, automated image analysis can effectively score many phenotypes. In practical application, customizing an image-analysis algorithm or finding a sufficient number of example cells to train a machine learning algorithm can be infeasible, particularly when positive control samples are not available and the phenotype of interest is rare. Here we present a supervised machine learning approach that uses iterative feedback to readily score multiple subtle and complex morphological phenotypes in high-throughput, image-based screens. First, automated cytological profiling extracts hundreds of numerical descriptors for every cell in every image. Next, the researcher generates a rule (i.e., classifier) to recognize cells with a phenotype of interest during a short, interactive training session using iterative feedback. Finally, all of the cells in the experiment are automatically classified and each sample is scored based on the presence of cells displaying the phenotype. By using this approach, we successfully scored images in RNA interference screens in 2 organisms for the prevalence of 15 diverse cellular morphologies, some of which were previously intractable.				Carpenter, Anne/C-4982-2008	Carpenter, Anne/0000-0003-1555-8261												0027-8424					FEB 10	2009	106	6					1826	1831		10.1073/pnas.0808843106						WOS:000263252500029	19188593	J	Qi, YJ; Bar-Joseph, Z; Klein-Seetharaman, J				Qi, YJ; Bar-Joseph, Z; Klein-Seetharaman, J			Evaluation of different biological data and computational classification methods for use in protein interaction prediction	PROTEINS-STRUCTURE FUNCTION AND BIOINFORMATICS												Protein-protein interactions play a key role in many biological systems. High-throughput methods can directly detect the set of interacting proteins in yeast, but the results are often incomplete and exhibit high false-positive and false-negative rates. Recently, many different research groups independently suggested using supervised learning methods to integrate direct and indirect biological data sources for the protein interaction prediction task. However, the data sources, approaches, and implementations varied. Furthermore, the protein interaction prediction task itself can be subdivided into prediction of (1) physical interaction, (2) co-complex relationship, and (3) pathway co-membership. To investigate systematically the utility of different data sources and the way the data is encoded as features for predicting each of these types of protein interactions, we assembled a large set of biological features and varied their encoding for use in each of the three prediction tasks. Six different classifiers were used to assess the accuracy in predicting interactions, Random Forest (RF), RF similarity-based k-Nearest-Neighbor, Naive Bayes, Decision Tree, Logistic Regression, and Support Vector Machine. For all classifiers, the three prediction tasks had different success rates, and co-complex prediction appears to be an easier task than the other two. Independently of prediction task, however, the RF classifier consistently ranked as one of the top two classifiers for all combinations of feature sets. Therefore, we used this classifier to study the importance of different biological datasets. First, we used the splitting function of the RF tree structure, the Gini index, to estimate feature importance. Second, we determined classification accuracy when only the top-ranking features were used as an input in the classifier. We find that the importance of different features depends on the specific prediction task and the way they are encoded. Strikingly, gene expression is consistently the most important feature for all three prediction tasks, while the protein interactions identified using the yeast-2-hybrid system were not among the top-ranking features under any condition.					Bar-Joseph, Ziv/0000-0003-3430-6051												0887-3585					MAY 15	2006	63	3					490	500		10.1002/prot.20865						WOS:000236946200008	16450363	J	Rokach, L; Maimon, O				Rokach, L; Maimon, O			Top-down induction of decision trees classifiers - A survey	IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART C-APPLICATIONS AND REVIEWS												Decision trees are considered to be one of the most popular approaches for representing classifiers. Researchers from various disciplines such as statistics, machine learning, pattern recognition, and data mining considered the issue of growing a decision tree from available data. This paper presents an updated survey of current methods for constructing decision tree classifiers in a top-down manner. The paper suggests a unified algorithmic framework for presenting these algorithms and describes the various splitting criteria and pruning methodologies.				Rokach, Lior/F-8247-2010													1094-6977	1558-2442				NOV	2005	35	4					476	487		10.1109/TSMCC.2004.843247						WOS:000232846700003		J	Breazeal, C; Scassellati, B				Breazeal, C; Scassellati, B			Robots that imitate humans	TRENDS IN COGNITIVE SCIENCES												The study of social learning in robotics has been motivated by both scientific interest in the learning process and practical desires to produce machines that are useful, flexible, and easy to use. In this review, we introduce the social and task-oriented aspects of robot imitation. We focus on methodologies for addressing two fundamental problems. First, how does the robot know what to imitate? And second, how does the robot map that perception onto its own action repertoire to replicate it? In the future, programming humanoid robots to perform new tasks might be as simple as showing them.				Galantucci, Bruno/E-5770-2010													1364-6613					NOV	2002	6	11					481	487		10.1016/S1364-6613(02)02016-8						WOS:000179170000013		J	Billsus, D; Pazzani, MJ				Billsus, D; Pazzani, MJ			User modeling for adaptive news access	USER MODELING AND USER-ADAPTED INTERACTION												We present a framework for adaptive news access, based on machine learning techniques specifically designed for this task. First, we focus on the system's general functionality and system architecture. We then describe the interface and design of two deployed news agents that are part of the described architecture. While the first agent provides personalized news through a web-based interface, the second system is geared towards wireless information devices such as PDAs (personal digital assistants) and cell phones. Based on implicit and explicit user feedback, our agents use a machine learning algorithm to induce individual user models. Motivated by general shortcomings of other user modeling systems for Information Retrieval applications, as well as the specific requirements of news classification, we propose the induction of hybrid user models that consist of separate models for short-term and long-term interests. Furthermore, we illustrate how the described algorithm can be used to address an important issue that has thus far received little attention in the Information Retrieval community: a user's information need changes as a direct result of interaction with information. We empirically evaluate the system's performance based on data collected from regular system users. The goal of the evaluation is not only to understand the performance contributions of the algorithm's individual components, but also to assess the overall utility of the proposed user modeling techniques from a user perspective. Our results provide empirical evidence for the utility of the hybrid user model, and suggest that effective personalization can be achieved without requiring any extra effort from the user.																	0924-1868						2000	10	2-3					147	180		10.1023/A:1026501525781						WOS:000166528800004		J	Lee, J; Lee, D				Lee, J; Lee, D			An improved cluster labeling method for support vector clustering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE												The support vector clustering (SVC) algorithm is a recently emerged unsupervised learning method inspired by support vector machines. One key step involved in the SVC algorithm is the cluster assignment of each data point. A new cluster labeling method for SVC is developed based on some invariant topological properties of a trained kernel radius function. Benchmark results show that the proposed method outperforms previously reported labeling techniques.				Lee, Jaewook/A-8862-2012; Lee, Jaewook/A-7355-2013													0162-8828					MAR	2005	27	3					461	464		10.1109/TPAMI.2005.47						WOS:000226300200014	15747800	J	Im, J; Jensen, JR; Tullis, JA				Im, J.; Jensen, J. R.; Tullis, J. A.			Object-based change detection using correlation image analysis and image segmentation	INTERNATIONAL JOURNAL OF REMOTE SENSING												This study introduces change detection based on object/neighbourhood correlation image analysis and image segmentation techniques. The correlation image analysis is based on the fact that pairs of brightness values from the same geographic area (e.g. an object) between bi-temporal image datasets tend to be highly correlated when little change occurres, and uncorrelated when change occurs. Five different change detection methods were investigated to determine how new contextual features could improve change classification results, and if an object-based approach could improve change classification when compared with per-pixel analysis. The five methods examined include (1) object-based change classification incorporating object correlation images (OCIs), (2) object-based change classification incorporating neighbourhood correlation images (NCIs), (3) object-based change classification without contextual features, (4) per-pixel change classification incorporating NCIs, and (5) traditional per-pixel change classification using only bi-temporal image data. Two different classification algorithms (i.e. a machine-learning decision tree and nearest-neighbour) were also investigated. Comparison between the OCI and the NCI variables was evaluated. Object-based change classifications incorporating the OCIs or the NCIs produced more accurate change detection classes (Kappa approximated 90%) than other change detection results (Kappa ranged from 80 to 85%).				anzhi, yue/A-8609-2012; Ma, Lei/I-4597-2014	Im, Jungho/0000-0002-4506-6877												0143-1161						2008	29	2					399	423		10.1080/01431160601075582						WOS:000252346500006		J	Ekins, S; Mestres, J; Testa, B				Ekins, S.; Mestres, J.; Testa, B.			In silico pharmacology for drug discovery: methods for virtual ligand screening and profiling	BRITISH JOURNAL OF PHARMACOLOGY												Pharmacology over the past 100 years has had a rich tradition of scientists with the ability to form qualitative or semiquantitative relations between molecular structure and activity in cerebro. To test these hypotheses they have consistently used traditional pharmacology tools such as in vivo and in vitro models. Increasingly over the last decade however we have seen that computational (in silico) methods have been developed and applied to pharmacology hypothesis development and testing. These in silico methods include databases, quantitative structure-activity relationships, pharmacophores, homology models and other molecular modeling approaches, machine learning, data mining, network analysis tools and data analysis tools that use a computer. In silico methods are primarily used alongside the generation of in vitro data both to create the model and to test it. Such models have seen frequent use in the discovery and optimization of novel molecules with affinity to a target, the clarification of absorption, distribution, metabolism, excretion and toxicity properties as well as physicochemical characterization. The aim of this review is to illustrate some of the in silico methods for pharmacology that are used in drug discovery. Further applications of these methods to specific targets and their limitations will be discussed in the second accompanying part of this review.				Mestres, Jordi/B-3673-2009; Gasull, Martina/A-6630-2013	Mestres, Jordi/0000-0002-5202-4501; Ekins, Sean/0000-0002-5691-5790												0007-1188					SEP	2007	152	1					9	20		10.1038/sj.bjp.0707305						WOS:000249069600003	17549047	J	Chesler, EJ; Wilson, SG; Lariviere, WR; Rodriguez-Zas, SL; Mogil, JS				Chesler, EJ; Wilson, SG; Lariviere, WR; Rodriguez-Zas, SL; Mogil, JS			Identification and ranking of genetic and laboratory environment factors influencing a behavioral trait, thermal nociception, via computational analysis of a large data archive	NEUROSCIENCE AND BIOBEHAVIORAL REVIEWS												Laboratory conditions in biobehavioral experiments are commonly assumed to be 'controlled', having little impact on the outcome. However, recent studies have illustrated that the laboratory environment has a robust effect on behavioral traits. Given that environmental factors can interact with trait-relevant genes, some have questioned the reliability and generalizability of behavior genetic research designed to identify those genes. This problem might be alleviated by the identification of the most relevant environmental factors, but the task is hindered by the large number of factors that typically vary between and within laboratories. We used a computational approach to retrospectively identify and rank sources of variability in nociceptive responses as they occurred in a typical research laboratory over several years. A machine-learning algorithm was applied to an archival data set of 8034 independent observations of baseline thermal nociceptive sensitivity. This analysis revealed that a factor even more important than mouse genotype was the experimenter performing the test, and that nociception can be affected by many additional laboratory factors including season/humidity, cage density, time of day, sex and within-cage order of testing. The results were confirmed by linear modeling in a subset of the data, and in confirmatory experiments, in which we were able to partition the variance of this complex trait among genetic (27%), environmental (42%) and genetic X environmental (18%) sources. (C) 2003 Published by Elsevier Science Ltd.																	0149-7634					DEC	2002	26	8					907	923	PII S0149-7634(02)00103-3	10.1016/S0149-7634(02)00103-3						WOS:000182201500003	12667496	J	Simpson, A; Tan, VYF; Winn, J; Svensen, M; Bishop, CM; Heckerman, DE; Buchan, I; Custovic, A				Simpson, Angela; Tan, Vincent Y. F.; Winn, John; Svensen, Markus; Bishop, Christopher M.; Heckerman, David E.; Buchan, Iain; Custovic, Adnan			Beyond Atopy Multiple Patterns of Sensitization in Relation to Asthma in a Birth Cohort Study	AMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE												Rationale: The pattern of IgE response (over time or to specific allergens) may reflect different atopic vulnerabilities which are related to the presence of asthma in a fundamentally different way from current definition of atopy. Objectives: To redefine the atopic phenotype by identifying latent structure within a complex dataset, taking into account the timing and type of sensitization to specific allergens, and relating these novel phenotypes to asthma. Methods: In a population-based birth cohort in which multiple skin and IgE tests have been taken throughout childhood, we used a machine learning approach to cluster children into multiple atopic classes in an unsupervised way. We then investigated the relation between these classes and asthma (symptoms, hospitalizations, lung function and airway reactivity). Measurements and Main Results: A five-class model indicated a complex latent structure, in which children with atopic vulnerability were clustered into four distinct classes (Multiple Early [112/1053, 10.6%]; Multiple Late [171/1053, 16.2%]; Dust Mite [47/1053,4.5%]; and Non-dust Mite [100/1053, 9.5%]), with a fifth class describing children with No Latent Vulnerability (623/1053, 59.2%). The association with asthma was considerably stronger for Multiple Early compared with other classes and conventionally defined atopy (odds ratio [95% Cl]: 29.3 [11.1-77.2] versus 12.4 [4.8-32.2] versus 11.6 [4.8-27.9] for Multiple Early class versus Ever Atopic versus Atopic age 8). Lung function and airway reactivity were significantly poorer among children in Multiple Early class. Cox regression demonstrated a highly significant increase in risk of hospital admissions for wheeze/asthma after age 3 yr only among children in the Multiple Early class (HR 9.2 [3.5-24.0], P < 0.001). Conclusions: IgE antibody responses do not reflect a single phenotype of atopy, but several different atopic vulnerabilities which differ in their relation with asthma presence and severity.				Custovic, Adnan/A-2435-2012	Custovic, Adnan/0000-0001-5218-7071; Simpson, Angela/0000-0003-2733-6666; Buchan, Iain/0000-0003-3392-1650												1073-449X					JUN 1	2010	181	11					1200	1206		10.1164/rccm.200907-1101OC						WOS:000278663600009	20167852	J	Archer, KJ; Kirnes, RV				Archer, Kelfie J.; Kirnes, Ryan V.			Empirical characterization of random forest variable importance measures	COMPUTATIONAL STATISTICS & DATA ANALYSIS												Microarray studies yield data sets consisting of a large number of candidate predictors (genes) on a small number of observations (samples). When interest lies in predicting phenotypic class using gene expression data, often the goals are both to produce an accurate classifier and to uncover the predictive structure of the problem. Most machine learning methods, such as k-nearest neighbors, support vector machines, and neural networks, are useful for classification. However, these methods provide no insight regarding the covariates that best contribute to the predictive structure. Other methods, such as linear discriminant analysis, require the predictor space be substantially reduced prior to deriving the classifier. A recently developed method, random forests (RF), does not require reduction of the predictor space prior to classification. Additionally, RF yield variable importance measures for each candidate predictor. This study examined the effectiveness of RF variable importance measures in identifying the true predictor among a large number of candidate predictors. An extensive simulation study was conducted using 20 levels of correlation among the predictor variables and 7 levels of association between the true predictor and the dichotomous response. We conclude that the RF methodology is attractive for use in classification problems when the goals of the study are to produce an accurate classifier and to provide insight regarding the discriminative ability of individual predictor variables. Such goals are common among microarray studies, and therefore application of the RF methodology for the purpose of obtaining variable importance measures is demonstrated on a microarray data set.. (c) 2007 Elsevier B.V. All rights reserved.																	0167-9473					JAN 10	2008	52	4					2249	2260		10.1016/j.csda.2007.08.015						WOS:000253283500033		J	Jebara, T; Kondor, R; Howard, A				Jebara, T; Kondor, R; Howard, A			Probability product kernels	JOURNAL OF MACHINE LEARNING RESEARCH												The advantages of discriminative learning algorithms and kernel machines are combined with generative modeling using a novel kernel between distributions. In the probability product kernel, data points in the input space are mapped to distributions over the sample space and a general inner product is then evaluated as the integral of the product of pairs of distributions. The kernel is straightforward to evaluate for all exponential family models such as multinomials and Gaussians and yields interesting nonlinear kernels. Furthermore, the kernel is computable in closed form for latent distributions such as mixture models, hidden Markov models and linear dynamical systems. For intractable models, such as switching linear dynamical systems, structured mean-field approximations can be brought to bear on the kernel evaluation. For general distributions, even if an analytic expression for the kernel is not feasible, we show a straightforward sampling method to evaluate it. Thus, the kernel permits discriminative learning methods, including support vector machines, to exploit the properties, metrics and invariances of the generative models we infer from each datum. Experiments are shown using multinomial models for text, hidden Markov models for biological data sets and linear dynamical systems for time series data.																	1532-4435					JUL	2004	5						819	844								WOS:000236327800004		J	Doan, A; Madhavan, J; Dhamankar, R; Domingos, P; Halevy, A				Doan, A; Madhavan, J; Dhamankar, R; Domingos, P; Halevy, A			Learning to match ontologies on the Semantic Web	VLDB JOURNAL												On the Semantic Web, data will inevitably come from many different ontologies, and information processing across ontologies is not possible without knowing the semantic mappings between them. Manually finding such mappings is tedious, error-prone, and clearly not possible on the Web scale. Hence the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web. We describe GLUE, a system that employs machine learning techniques to find such mappings. Given two ontologies, for each concept in one ontology GLUE finds the most similar concept in the other ontology. We give well-founded probabilistic definitions to several practical similarity measures and show that GLUE can work with all of them. Another key feature of GLUE is that it uses multiple learning strategies, each of which exploits well a different type of information either in the data instances or in the taxonomic structure of the ontologies. To further improve matching accuracy, we extend GLUE to incorporate commonsense knowledge and domain constraints into the matching process. Our approach is thus distinguished in that it works with a variety of well-defined similarity notions and that it efficiently incorporates multiple types of knowledge. We describe a set of experiments on several real-world domains and show that GLUE proposes highly accurate semantic mappings. Finally, we extend GLUE to find complex mappings between ontologies and describe experiments that show the promise of the approach.																	1066-8888	0949-877X				NOV	2003	12	4					303	319		10.1007/s00778-003-0104-2						WOS:000186588400003		J	Buhlmann, P; Wyner, AJ				Buhlmann, P; Wyner, AJ			Variable length Markov chains	ANNALS OF STATISTICS												We study estimation in the class of stationary variable length Markov chains (VLMC) on a finite space. The processes in this class are still Markovian of high order, but with memory of variable length yielding a much bigger and structurally richer class of models than ordinary high-order Markov chains. From an algorithmic view, the VLMC model class has attracted interest in information theory and machine learning, but statistical properties have not yet been explored. Provided that good estimation is available, the additional structural richness of the model class enhances predictive power by finding a better trade-off between model bias and variance and allowing better structural description which can be of specific interest. The latter is exemplified with some DNA data. A version of the tree-structured context algorithm, proposed by Rissanen in an information theoretical set-up is shown to have new good asymptotic properties for estimation in the class of VLMCs. This remains true even when the underlying model increases in dimensionality. Furthermore, consistent estimation of minimal state spaces and mixing properties of fitted models are given. We also propose a new bootstrap scheme based on fitted VLMCs. We show its validity for quite general stationary categorical time series and for a broad range of statistical procedures.				Buhlmann, Peter/A-2107-2013	Buhlmann, Peter/0000-0002-1782-6015												0090-5364					APR	1999	27	2					480	513								WOS:000082687200004		J	Bredensteiner, EJ; Bennett, KP				Bredensteiner, EJ; Bennett, KP			Multicategory classification by support vector machines	COMPUTATIONAL OPTIMIZATION AND APPLICATIONS												We examine the problem of how to discriminate between objects of three or more classes. Specifically, we investigate how two-class discrimination methods can be extended to the multiclass case. We show how the linear programming (LP) approaches based on the work of Mangasarian and quadratic programming (QP) approaches based on Vapnik's Support Vector Machine (SVM) can be combined to yield two new approaches to the multiclass problem. In LP multiclass discrimination, a single linear program is used to construct a piecewise-linear classification function. In our proposed multiclass SVM method, a single quadratic program is used to construct a piecewise-nonlinear classification function. Each piece of this function can take the form of a polynomial, a radial basis function, or even a neural network. For the k > 2-class problems, the SVM method as originally proposed required the construction of a two-class SVM to separate each class from the remaining classes. Similarily, k two-class linear programs can be used for the multiclass problem. We performed an empirical study of the original LP method, the proposed k LP method, the proposed single QP method and the original k QP methods. We discuss the advantages and disadvantages of each approach.																	0926-6003					JAN	1999	12	1-3					53	79		10.1023/A:1008663629662						WOS:000080941900005		J	Ghosh-Dastidar, S; Adeli, H				Ghosh-Dastidar, Samanwoy; Adeli, Hojjat			SPIKING NEURAL NETWORKS	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS												Most current Artificial Neural Network (ANN) models are based on highly simplified brain dynamics. They have been used as powerful computational tools to solve complex pattern recognition, function estimation, and classification problems. ANNs have been evolving towards more powerful and more biologically realistic models. In the past decade, Spiking Neural Networks (SNNs) have been developed which comprise of spiking neurons. Information transfer in these neurons mimics the information transfer in biological neurons, i.e., via the precise timing of spikes or a sequence of spikes. To facilitate learning in such networks, new learning algorithms based on varying degrees of biological plausibility have also been developed recently. Addition of the temporal dimension for information encoding in SNNs yields new insight into the dynamics of the human brain and could result in compact representations of large neural networks. As such, SNNs have great potential for solving complicated time-dependent pattern recognition problems because of their inherent dynamic representation. This article presents a state-of-the-art review of the development of spiking neurons and SNNs, and provides insight into their evolution as the third generation neural networks.				Ghosh-Dastidar, Samanwoy/A-5232-2008; adeli, hojjat/D-1430-2010	adeli, hojjat/0000-0001-5718-1453												0129-0657					AUG	2009	19	4					295	308								WOS:000269527600006	19731402	J	Fan, Y; Resnick, SM; Wu, XY; Davatzikos, C				Fan, Yong; Resnick, Susan M.; Wu, Xiaoying; Davatzikos, Christos			Structural and functional biomarkers of prodromal Alzheimer's disease: A high-dimensional pattern classification study	NEUROIMAGE												This work builds upon previous studies that reported high sensitivity and specificity in classifying individuals with mild cognitive impairment (MCI), which is often a prodromal phase of Alzheimer's disease (AD), via pattern classification of MRI scans. The current study integrates MRI and PET O-15 water scans from 30 participants in the Baltimore Longitudinal Study of Aging, and tests the hypothesis that joint evaluation of structure and function can yield higher classification accuracy than either alone. Classification rates of up to 100% accuracy were achieved via leave-one-out cross-validation, whereas conservative estimates of generalization performance in new scans, evaluated via bagging cross-validation, yielded an area under the receiver operating characteristic (ROC) curve equal to 0.978 (97.8%), indicating excellent diagnostic accuracy. Spatial maps of regions determined to contribute the most to the classification implicated many temporal, prefrontal, orbitofrontal, and parietal regions. Detecting complex patterns of brain abnormality in early stages of cognitive impairment has pivotal importance for the detection and management of AD. (c) 2008 Elsevier Inc. All rights reserved.					Fan, Yong/0000-0001-9869-4685												1053-8119	1095-9572				JUN	2008	41	2					277	285		10.1016/j.neuroimage.2008.02.043						WOS:000256271100011	18400519	J	SRINIVASAN, K; FISHER, D				SRINIVASAN, K; FISHER, D			MACHINE LEARNING APPROACHES TO ESTIMATING SOFTWARE-DEVELOPMENT EFFORT	IEEE TRANSACTIONS ON SOFTWARE ENGINEERING												Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data.																	0098-5589					FEB	1995	21	2					126	137		10.1109/32.345828						WOS:A1995QG40700008		S	Gray, D; Tao, H		Forsyth, D; Torr, P; Zisserman, A		Gray, Douglas; Tao, Hai			Viewpoint Invariant Pedestrian Recognition with an Ensemble of Localized Features	COMPUTER VISION - ECCV 2008, PT I, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE				10th European Conference on Computer Vision (ECCV 2008)	OCT 12-18, 2008	Marseille, FRANCE	INRIA, Ville Marseille, Reg Province Alpes-Cote Azur, Deutsch Telekom Lab, Microsoft Res, Orange, INRIA, Microsoft Res, EADS, TOSHIBA, Springer				Viewpoint invariant pedestrian recognition is an import yet under-addressed problem computer vision. This is likely due to the difficulty in matching two objects With unknown viewpoint and pose. This paper presents a method of performing viewpoint invariant pedestrian recognition using an efficiently and intelligently designed object representation, the ensemble of localized features (ELF). Instead of designing a specific feature by, hand to solve the problem, we define a feature space using our intuition about the problem and let a machine learning algorithm find the best, representation. We show how both an object class specific representation and a discriminative recognition model can be learned using the AdaBoost algorithm. This approach allows many different kinds of simple features to be combined into a single similarity function. The method is evaluated using a viewpoint invariant pedestrian recognition dataset and the results are shown to be superior to all previous benchmarks for both recognition and reacquisition of pedestrians.																	0302-9743		978-3-540-88681-5				2008	5302		1				262	275								WOS:000260656000020		J	Ellis, DI; Broadhurst, D; Kell, DB; Rowland, JJ; Goodacre, R				Ellis, DI; Broadhurst, D; Kell, DB; Rowland, JJ; Goodacre, R			Rapid and quantitative detection of the microbial spoilage of meat by Fourier transform infrared spectroscopy and machine learning	APPLIED AND ENVIRONMENTAL MICROBIOLOGY												Fourier transform infrared (FT-IR) spectroscopy is a rapid, noninvasive technique with considerable potential for application in the food and related industries. We show here that this technique can be used directly on the surface of food to produce biochemically interpretable "fingerprints." Spoilage in meat is the result of decomposition and the formation of metabolites caused by the growth and enzymatic activity of microorganisms. FT-IR was exploited to measure biochemical changes within the meat substrate, enhancing and accelerating the detection of microbial spoilage. Chicken breasts were purchased from a national retailer, comminuted for 10 s, and left to spoil at room temperature for 24 h. Every hour, FT-IR measurements were taken directly from the meat surface using attenuated total reflectance, and the total viable counts were obtained by classical plating methods. Quantitative interpretation of FT-IR spectra was possible using partial least-squares regression and allowed accurate estimates of bacterial loads to be calculated directly from the meat surface in 60 s. Genetic programming was used to derive rules showing that at levels of 10(7) bacteria.g(-1) the main biochemical indicator of spoilage was the onset of proteolysis. Thus, using FT-IR we were able to acquire a metabolic snapshot and quantify, noninvasively, the microbial loads of food samples accurately and rapidly in 60 s, directly from the sample surface. We believe this approach will aid in the Hazard Analysis Critical Control Point process for the assessment of the microbiological safety of food at the production, processing, manufacturing, packaging, and storage levels.				Kell, Douglas/E-8318-2011; Goodacre, Roy/J-1600-2012; Ellis, David/D-8603-2016	Kell, Douglas/0000-0001-5838-7963; Goodacre, Roy/0000-0003-2230-645X; Ellis, David/0000-0002-7633-7019												0099-2240					JUN	2002	68	6					2822	2828		10.1128/AEM.68.6.2822-2828.2002						WOS:000176030100026	12039738	J	DORNDORF, U; PESCH, E				DORNDORF, U; PESCH, E			EVOLUTION BASED LEARNING IN A JOB-SHOP SCHEDULING ENVIRONMENT	COMPUTERS & OPERATIONS RESEARCH												A class of approximation algorithms is described for solving the minimum makespan problem of job shop scheduling. A common basis of these algorithms is the underlying genetic algorithm that serves as a meta-strategy to guide an optimal design of local decision rule sequences. We consider sequences of dispatching rules for job assignment as well as sequences of one machine solutions in the sense of the shifting bottleneck procedure of Adams et al. Computational experiments show that our algorithm can find shorter makespans than the shifting bottleneck heuristic or a simulated annealing approach with the same running time.																	0305-0548					JAN	1995	22	1					25	40		10.1016/0305-0548(93)E0016-M						WOS:A1995PV15500005		J	GILES, CL; MILLER, CB; CHEN, D; CHEN, HH; SUN, GZ; LEE, YC				GILES, CL; MILLER, CB; CHEN, D; CHEN, HH; SUN, GZ; LEE, YC			LEARNING AND EXTRACTING FINITE STATE AUTOMATA WITH 2ND-ORDER RECURRENT NEURAL NETWORKS	NEURAL COMPUTATION												We show that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples. We present simulations that show the effect of initial conditions, training set size and order, and neural network architecture. All simulations were performed with random initial weight strengths and usually converge after approximately a hundred epochs of training. We discuss a quantization algorithm for dynamically extracting finite state automata during and after training. For a well-trained neural net, the extracted automata constitute an equivalence class of state machines that are reducible to the minimal machine of the inferred grammar. We then show through simulations that many of the neural net state machines are dynamically stable, that is, they correctly classify many long unseen strings. In addition, some of these extracted automata actually outperform the trained neural network for classification of unseen strings.																	0899-7667					MAY	1992	4	3					393	405		10.1162/neco.1992.4.3.393						WOS:A1992HZ47600008		J	Duchi, JC; Agarwal, A; Wainwright, MJ				Duchi, John C.; Agarwal, Alekh; Wainwright, Martin J.			Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling	IEEE TRANSACTIONS ON AUTOMATIC CONTROL												The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. It arises in various application domains, including distributed tracking and localization, multi-agent coordination, estimation in sensor networks, and large-scale machine learning. We develop and analyze distributed algorithms based on dual subgradient averaging, and we provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis allows us to clearly separate the convergence of the optimization algorithm itself and the effects of communication dependent on the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network, and confirm this prediction's sharpness both by theoretical lower bounds and simulations for various networks. Our approach includes the cases of deterministic optimization and communication, as well as problems with stochastic optimization and/or communication.																	0018-9286					MAR	2012	57	3					592	606		10.1109/TAC.2011.2161027						WOS:000300845400004		J	Galar, M; Fernandez, A; Barrenechea, E; Bustince, H; Herrera, F				Galar, Mikel; Fernandez, Alberto; Barrenechea, Edurne; Bustince, Humberto; Herrera, Francisco			An overview of ensemble methods for binary classifiers in multi-class problems: Experimental study on one-vs-one and one-vs-all schemes	PATTERN RECOGNITION												Classification problems involving multiple classes can be addressed in different ways. One of the most popular techniques consists in dividing the original data set into two-class subsets, learning a different binary model for each new subset. These techniques are known as binarization strategies. In this work, we are interested in ensemble methods by binarization techniques; in particular, we focus on the well-known one-vs-one and one-vs-all decomposition strategies, paying special attention to the final step of the ensembles, the combination of the outputs of the binary classifiers. Our aim is to develop an empirical analysis of different aggregations to combine these outputs. To do so, we develop a double study: first, we use different base classifiers in order to observe the suitability and potential of each combination within each classifier. Then, we compare the performance of these ensemble techniques with the classifiers' themselves. Hence, we also analyse the improvement with respect to the classifiers that handle multiple classes inherently. We carry out the experimental study with several well-known algorithms of the literature such as Support Vector Machines, Decision Trees, Instance Based Learning or Rule Based Systems. We will show, supported by several statistical analyses, the goodness of the binarization techniques with respect to the base classifiers and finally we will point out the most robust techniques within this framework. (C) 2011 Elsevier Ltd. All rights reserved.				Barrenechea, Edurne/H-4815-2011; Galar, Mikel/H-4846-2011; Bustince, Humberto/H-4868-2011; Herrera, Francisco/C-6856-2008; Fernandez, Alberto/G-3827-2014	Barrenechea, Edurne/0000-0001-6657-948X; Galar, Mikel/0000-0003-2865-6549; Bustince, Humberto/0000-0002-1279-6195; Herrera, Francisco/0000-0002-7283-312X; 												0031-3203	1873-5142				AUG	2011	44	8					1761	1776		10.1016/j.patcog.2011.01.017						WOS:000290054200017		J	Muller, KR; Tangermann, M; Dornhege, G; Krauledat, M; Curio, G; Blankertz, B				Mueller, Klaus-Robert; Tangermann, Michael; Dornhege, Guido; Krauledat, Matthias; Curio, Gabriel; Blankertz, Benjamin			Machine learning for real-time single-trial EEG-analysis: From brain-computer interfacing to mental state monitoring	JOURNAL OF NEUROSCIENCE METHODS												Machine learning methods are an excellent choice for compensating the high variability in EEG when analyzing single-trial data in real-time. This paper briefly reviews preprocessing and classification techniques for efficient EEG-based brain-computer interfacing (BCI) and mental state monitoring applications. More specifically, this paper gives an outline of the Berlin brain-computer interface (BBCI), which can be operated with minimal subject training. Also, spelling with the novel BBCI-based Hex-o-Spell text entry system, which gains communication speeds of 6-8 letters per minute, is discussed. Finally the results of a real-time arousal monitoring experiment are presented. (C) 2007 Elsevier B.V. All rights reserved.				Muller, Klaus/C-3196-2013													0165-0270					JAN 15	2008	167	1					82	90		10.1016/j.jneumeth.2007.09.022						WOS:000252164300009	18031824	J	Patel, S; Dawood, A; Ford, TP; Whaites, E				Patel, S.; Dawood, A.; Ford, T. Pitt; Whaites, E.			The potential applications of cone beam computed tomography in the management of endodontic problems	INTERNATIONAL ENDODONTIC JOURNAL												Aim To provide core information on cone beam computed tomography (CBCT) technology and its potential applications in endodontic practice. Summary CBCT has been specifically designed to produce undistorted three-dimensional information of the maxillofacial skeleton as well as three-dimensional images of the teeth and their surrounding tissues. This is usually achieved with a substantially lower effective dose compared with conventional medical computed tomography (CT). Periapical disease may be detected sooner using CBCT compared with periapical views, and the true size, extent, nature and position of periapical and resorptive lesions can be assessed. Root fractures, root canal anatomy and the true nature of the alveolar bone topography around teeth may be assessed. CBCT scans are desirable to assess posterior teeth prior to periapical surgery, as the thickness of the cortical and cancellous bone can be accurately determined as can the inclination of roots in relation to the surrounding jaw. The relationship of anatomical structures such as the maxillary sinus and inferior dental nerve to the root apices may also be clearly visualized. CBCT has a low effective dose in the same order of magnitude as conventional dental radiographs. Key learning points CBCT has a low effective dose in the same order of magnitude as conventional dental radiographs. CBCT has numerous potential applications in the management of endodontic problems.					patel, shanon/0000-0003-0614-6951												0143-2885					OCT	2007	40	10					818	830		10.1111/j.1365-2591.2007.01299.x						WOS:000249429100010	17697108	J	Craddock, RC; Holtzheimer, PE; Hu, XPP; Mayberg, HS				Craddock, R. Cameron; Holtzheimer, Paul E., III; Hu, Xiaoping P.; Mayberg, Helen S.			Disease State Prediction From Resting State Functional Connectivity	MAGNETIC RESONANCE IN MEDICINE												The application of multivoxel pattern analysis methods has attracted increasing attention, particularly for brain state prediction and real-time functional MRI applications. Support vector classification is the most popular of these techniques, owing to reports that it has better prediction accuracy and is less sensitive to noise. Support vector classification was applied to learn functional connectivity patterns that distinguish patients with depression from healthy volunteers. In addition, two feature selection algorithms were implemented (one filter method, one wrapper method) that incorporate reliability information into the feature selection process. These reliability feature selections methods were compared to two previously proposed feature selection methods. A support vector classifier was trained that reliably distinguishes healthy volunteers from clinically depressed patients. The reliability feature selection methods outperformed previously utilized methods. The proposed framework for applying support vector classification to functional connectivity data is applicable to other disease states beyond major depression. Magn Reson Med 62: 1619-1628, 2009. (C) 2009 Wiley-Liss, Inc.				Holtzheimer, Paul/B-6212-2015	Holtzheimer, Paul/0000-0002-3552-3296; Craddock, Cameron/0000-0002-4950-1303												0740-3194					DEC	2009	62	6					1619	1628		10.1002/mrm.22159						WOS:000272067600028	19859933	J	Eagle, N; Pentland, AS				Eagle, Nathan; Pentland, Alex Sandy			Eigenbehaviors: identifying structure in routine	BEHAVIORAL ECOLOGY AND SOCIOBIOLOGY												Longitudinal behavioral data generally contains a significant amount of structure. In this work, we identify the structure inherent in daily behavior with models that can accurately analyze, predict, and cluster multimodal data from individuals and communities within the social network of a population. We represent this behavioral structure by the principal components of the complete behavioral dataset, a set of characteristic vectors we have termed eigenbehaviors. In our model, an individual's behavior over a specific day can be approximated by a weighted sum of his or her primary eigenbehaviors. When these weights are calculated halfway through a day, they can be used to predict the day's remaining behaviors with 79% accuracy for our test subjects. Additionally, we demonstrate the potential for this dimensionality reduction technique to infer community affiliations within the subjects' social network by clustering individuals into a "behavior space" spanned by a set of their aggregate eigenbehaviors. These behavior spaces make it possible to determine the behavioral similarity between both individuals and groups, enabling 96% classification accuracy of community affiliations within the population-level social network. Additionally, the distance between individuals in the behavior space can be used as an estimate for relational ties such as friendship, suggesting strong behavioral homophily amongst the subjects. This approach capitalizes on the large amount of rich data previously captured during the Reality Mining study from mobile phones continuously logging location, proximate phones, and communication of 100 subjects at MIT over the course of 9 months. As wearable sensors continue to generate these types of rich, longitudinal datasets, dimensionality reduction techniques such as eigenbehaviors will play an increasingly important role in behavioral research.																	0340-5443					MAY	2009	63	7					1057	1066		10.1007/s00265-009-0739-0						WOS:000265215700010		J	Gopnik, A; Sobel, DM; Schulz, LE; Glymour, C				Gopnik, A; Sobel, DM; Schulz, LE; Glymour, C			Causal learning mechanisms in very young children: Two-, three-, and four-year-olds infer causal relations from patterns of variation and covariation	DEVELOPMENTAL PSYCHOLOGY												Three studies investigated whether young children make accurate causal inferences on the basis of patterns of variation and covariation. Children were presented with a new causal relation by means of a machine called the "blicket detector." Some objects, but not others, made the machine light up and play music. In the first 2 experiments, children were told that "blickets make the machine go" and were then asked to identify which objects were "blickets." Two-, 3-, and 4-year-old children were shown various patterns of variation and covariation between two different objects and the activation of the machine. All 3 age groups took this information into account in their causal judgments about which objects were blickets. In a 3rd experiment, 3- and 4-year-old children used the information when they were asked to make the machine stop: These results are related to Bayes-net causal graphical models of causal learning.																	0012-1649					SEP	2001	37	5					620	629		10.1037//0012-1649.37.5.620						WOS:000170776800006	11552758	J	FARMER, JD; SIDOROWICH, JJ				FARMER, JD; SIDOROWICH, JJ			OPTIMAL SHADOWING AND NOISE-REDUCTION	PHYSICA D												The shadowing problem is that of finding a deterministic orbit as close as possible to a given noisy orbit. We present an optimal solution to this problem in the sense of least-mean-squares, which also provides an effective and convenient numerical method for noise reduction for data generated by a dynamical system. Given a noisy orbit y and a dynamical system f, we derive a set of nonlinear equations whose solution x is the deterministic orbit with the smallest possible Euclidean distance to y. We present a numerical method for solving these equations. The quality of the solution depends on the initial noise level. When f is known exactly, the noise can be reduced to machine precision over long trajectory segments; with higher noise levels there are regions where the algorithm has difficulty, but significant overall noise reductions are still achieved. If f must be learned from the data the noise reduction is limited by the accuracy of the learning algorithm and the number of available data points, but large reductions are still possible in some cases.																	0167-2789					JAN	1991	47	3					373	392								WOS:A1991EX65200004		J	Saulnier, DM; Riehle, K; Mistretta, TA; Diaz, MA; Mandal, D; Raza, S; Weidler, EM; Qin, X; Coarfa, C; Milosavljevic, A; Petrosino, JF; Highlander, S; Gibbs, R; Lynch, SV; Shulman, RJ; Versalovic, J				Saulnier, Delphine M.; Riehle, Kevin; Mistretta, Toni-Ann; Diaz, Maria-Alejandra; Mandal, Debasmita; Raza, Sabeen; Weidler, Erica M.; Qin, Xiang; Coarfa, Cristian; Milosavljevic, Aleksandar; Petrosino, Joseph F.; Highlander, Sarah; Gibbs, Richard; Lynch, Susan V.; Shulman, Robert J.; Versalovic, James			Gastrointestinal Microbiome Signatures of Pediatric Patients With Irritable Bowel Syndrome	GASTROENTEROLOGY												BACKGROUND & AIMS: The intestinal microbiomes of healthy children and pediatric patients with irritable bowel syndrome (IBS) are not well defined. Studies in adults have indicated that the gastrointestinal microbiota could be involved in IBS. METHODS: We analyzed 71 samples from 22 children with IBS (pediatric Rome III criteria) and 22 healthy children, ages 7-12 years, by 16S ribosomal RNA gene sequencing, with an average of 54,287 reads/stool sample (average 454 read length = 503 bases). Data were analyzed using phylogenetic-based clustering (Unifrac), or an operational taxonomic unit (OTU) approach using a supervised machine learning tool (randomForest). Most samples were also hybridized to a microarray that can detect 8741 bacterial taxa (16S rRNA PhyloChip). RESULTS: Microbiomes associated with pediatric IBS were characterized by a significantly greater percentage of the class gamma-proteobacteria (0.07% vs 0.89% of total bacteria, respectively; P<.05); 1 prominent component of this group was Haemophilus parainfluenzae. Differences highlighted by 454 sequencing were confirmed by high-resolution PhyloChip analysis. Using supervised learning techniques, we were able to classify different subtypes of IBS with a success rate of 98.5%, using limited sets of discriminant bacterial species. A novel Ruminococcus-like microbe was associated with IBS, indicating the potential utility of microbe discovery for gastrointestinal disorders. A greater frequency of pain correlated with an increased abundance of several bacterial taxa from the genus Alistipes. CONCLUSIONS: Using16S metagenomics by PhyloChip DNA hybridization and deep 454 pyrosequencing, we associated specific microbiome signatures with pediatric IBS. These findings indicate the important association between gastrointestinal microbes and IBS in children; these approaches might be used in diagnosis of functional bowel disorders in pediatric patients.					Diaz, Maria Alejandra/0000-0001-5537-5510												0016-5085					NOV	2011	141	5					1782	1791		10.1053/j.gastro.2011.06.072						WOS:000296512200043	21741921	J	Verma, N; Shoeb, A; Bohorquez, J; Dawson, J; Guttag, J; Chandrakasan, AP				Verma, Naveen; Shoeb, Ali; Bohorquez, Jose; Dawson, Joel; Guttag, John; Chandrakasan, Anantha P.			A Micro-Power EEG Acquisition SoC With Integrated Feature Extraction Processor for a Chronic Seizure Detection System	IEEE JOURNAL OF SOLID-STATE CIRCUITS					Symposium on VLSI Circuits	JUN 16-18, 2009	Kyoto, JAPAN	Japan Soc Appl Phys, IEEE Solid State Circuits Soc				This paper presents a low-power SoC that performs EEG acquisition and feature extraction required for continuous detection of seizure onset in epilepsy patients. The SoC corresponds to one EEG channel, and, depending on the patient, up to 18 channels may be worn to detect seizures as part of a chronic treatment system. The SoC integrates an instrumentation amplifier, ADC, and digital processor that streams features-vectors to a central device where seizure detection is performed via a machine-learning classifier. The instrumentation-amplifier uses chopper-stabilization in a topology that achieves high input-impedance and rejects large electrode-offsets while operating at 1 V; the ADC employs power-gating for low energy-per-conversion while using static-biasing for comparator precision; the EEG feature extraction processor employs low-power hardware whose parameters are determined through validation via patient data. The integration of sensing and local processing lowers system power by 14x by reducing the rate of wireless EEG data transmission. Feature vectors are derived at a rate of 0.5 Hz, and the complete one-channel SoC operates from a 1 V supply, consuming 9 mu J per feature vector.																	0018-9200	1558-173X				APR	2010	45	4					804	816		10.1109/JSSC.2010.2042245						WOS:000275995000012		J	Tuia, D; Ratle, F; Pacifici, F; Kanevski, MF; Emery, WJ				Tuia, Devis; Ratle, Frederic; Pacifici, Fabio; Kanevski, Mikhail F.; Emery, William J.			Active Learning Methods for Remote Sensing Image Classification	IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING												In this paper, we propose two active learning algorithms for semiautomatic definition of training samples in remote sensing image classification. Based on predefined heuristics, the classifier ranks the unlabeled pixels and automatically chooses those that are considered the most valuable for its improvement. Once the pixels have been selected, the analyst labels them manually and the process is iterated. Starting with a small and nonoptimal training set, the model itself builds the optimal set of samples which minimizes the classification error. We have applied the proposed algorithms to a variety of remote sensing data, including very high resolution and hyperspectral images, using support vector machines. Experimental results confirm the consistency of the methods. The required number of training samples can be reduced to 10% using the methods proposed, reaching the same level of accuracy as larger data sets. A comparison with a state-of-the-art active learning method, margin sampling, is provided, highlighting advantages of the methods proposed. The effect of spatial resolution and separability of the classes on the quality of the selection of pixels is also discussed.				Tuia, Devis/J-2239-2015	Tuia, Devis/0000-0003-0374-2459; Emery, William/0000-0002-7598-9082												0196-2892					JUL	2009	47	7					2218	2232		10.1109/TGRS.2008.2010404						WOS:000267437300012		J	Peters, J; Schaal, S				Peters, Jan; Schaal, Stefan			Reinforcement learning of motor skills with policy gradients	NEURAL NETWORKS												Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g., by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm. (C) 2008 Elsevier Ltd. All rights reserved.				Peters, Jan/D-5068-2009	Peters, Jan/0000-0002-5266-8091												0893-6080	1879-2782				MAY	2008	21	4					682	697		10.1016/j.neunet.2008.02.003						WOS:000257639800012	18482830	J	Vaughan, TM; McFarland, DJ; Schalk, G; Sarnacki, WA; Krusienski, DJ; Sellers, EW; Wolpaw, JR				Vaughan, Theresa M.; McFarland, Dennis J.; Schalk, Gerwin; Sarnacki, William A.; Krusienski, Dean J.; Sellers, Eric W.; Wolpaw, Jonathan R.			The wadsworth BCI research and development program: At home with BCI	IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING					3rd International Meeting on Brain-Computer Interface Technology	JUN, 2005	Rensselaerville Inst, Rensselaerville, NY		Rensselaerville Inst			The ultimate goal of brain-computer interface (BCI) technology is to provide communication and control capacities to people with severe motor disabilities. BCI research at the Wadsworth Center focuses primarily on noninvasive, electroencephalography (EEG)-based BCI methods. We have shown that people, including those with severe motor disabilities, can learn to use sensorimotor rhythms (SMRs) to move a cursor rapidly and accurately in one or two dimensions. We have also improved P300-based BCI operation. We are now translating this laboratory-proven BCI technology into a system that can be used by severely disabled people in their homes with minimal ongoing technical oversight. To accomplish this, we have: improved our general-purpose BCI software (BCI2000); improved online adaptation and feature translation for SMR-based BCI operation; improved the accuracy and bandwidth of P300-based BCI operation; reduced the complexity of system hardware and software and begun to evaluate home system use in appropriate users. These developments have resulted in prototype systems for every day use in people's homes.																	1534-4320	1558-0210				JUN	2006	14	2					229	233		10.1109/TNSRE.2006.875577						WOS:000238394700027	16792301	J	Lebedev, MA; Carmena, JM; O'Doherty, JE; Zacksenhouse, M; Henriquez, CS; Principe, JC; Nicolelis, MAL				Lebedev, MA; Carmena, JM; O'Doherty, JE; Zacksenhouse, M; Henriquez, CS; Principe, JC; Nicolelis, MAL			Cortical ensemble adaptation to represent velocity of an artificial actuator controlled by a brain-machine interface	JOURNAL OF NEUROSCIENCE												Monkeys can learn to directly control the movements of an artificial actuator by using a brain - machine interface ( BMI) driven by the activity of a sample of cortical neurons. Eventually, they can do so without moving their limbs. Neuronal adaptations underlying the transition from control of the limb to control of the actuator are poorly understood. Here, we show that rapid modifications in neuronal representation of velocity of the hand and actuator occur in multiple cortical areas during the operation of a BMI. Initially, monkeys controlled the actuator by moving a hand- held pole. During this period, the BMI was trained to predict the actuator velocity. As the monkeys started using their cortical activity to control the actuator, the activity of individual neurons and neuronal populations became less representative of the animal's hand movements while representing the movements of the actuator. As a result of this adaptation, the animals could eventually stop moving their hands yet continue to control the actuator. These results show that, during BMI control, cortical ensembles represent behaviorally significant motor parameters, even if these are not associated with movements of the animal's own limb.				O'Doherty, Joseph/D-5116-2009; Lebedev, Mikhail/H-5066-2016	O'Doherty, Joseph/0000-0001-8175-5699; Lebedev, Mikhail/0000-0003-0355-8723												0270-6474					MAY 11	2005	25	19					4681	4693		10.1523/JNEUROSCI.4088-04.2005						WOS:000229038300002	15888644	J	Donnes, P; Elofsson, A				Donnes, P; Elofsson, A			Prediction of MHC class I binding peptides, using SVMHC	BMC BIOINFORMATICS												Background: T-cells are key players in regulating a specific immune response. Activation of cytotoxic T-cells requires recognition of specific peptides bound to Major Histocompatibility Complex (MHC) class I molecules. MHC-peptide complexes are potential tools for diagnosis and treatment of pathogens and cancer, as well as for the development of peptide vaccines. Only one in 100 to 200 potential binders actually binds to a certain MHC molecule, therefore a good prediction method for MHC class I binding peptides can reduce the number of candidate binders that need to be synthesized and tested. Results: Here, we present a novel approach, SVMHC, based on support vector machines to predict the binding of peptides to MHC class I molecules. This method seems to perform slightly better than two profile based methods, SYFPEITHI and HLA_BIND. The implementation of SVMHC is quite simple and does not involve any manual steps, therefore as more data become available it is trivial to provide prediction for more MHC types. SVMHC currently contains prediction for 26 MHC class I types from the MHCPEP database or alternatively 6 MHC class I types from the higher quality SYFPEITHI database. The prediction models for these MHC types are implemented in a public web service available at [http://www.sbc.su.se/svmhc/]. Conclusions: Prediction of MHC class I binding peptides using Support Vector Machines, shows high performance and is easy to apply to a large number of MHC class I types. As more peptide data are put into MHC databases, SVMHC can easily be updated to give prediction for additional MHC class I types. We suggest that the number of binding peptides needed for SVM training is at least 20 sequences.																	1471-2105						2002	3								25	10.1186/1471-2105-3-25						WOS:000181476800025	12225620	J	Duro, DC; Franklin, SE; Dube, MG				Duro, Dennis C.; Franklin, Steven E.; Dube, Monique G.			A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery	REMOTE SENSING OF ENVIRONMENT												Pixel-based and object-based image analysis approaches for classifying broad land cover classes over agricultural landscapes are compared using three supervised machine learning algorithms: decision tree (DT), random forest (RF), and the support vector machine (SVM). Overall classification accuracies between pixel-based and object-based classifications were not statistically significant (p > 0.05) when the same machine learning algorithms were applied. Using object-based image analysis, there was a statistically significant difference in classification accuracy between maps produced using the DT algorithm compared to maps produced using either RF (p = 0.0116) or SVM algorithms (p = 0.0067). Using pixel-based image analysis. there was no statistically significant difference (p > 0.05) between results produced using different classification algorithms. Classifications based on RF and SVM algorithms provided a more visually adequate depiction of wetland, riparian, and crop land cover types when compared to DT based classifications, using either object-based or pixel-based image analysis. In this study, pixel-based classifications utilized fewer variables (15 vs. 300), achieved similar classification accuracies, and required less time to produce than object-based classifications. Object-based classifications produced a visually appealing generalized appearance of land cover classes. Based exclusively on overall accuracy reports, there was no advantage to preferring one image analysis approach over another for the purposes of mapping broad land cover types in agricultural environments using medium spatial resolution earth observation imagery. (C) 2011 Elsevier Inc. All rights reserved.				Ma, Lei/I-4597-2014	Duro, Dennis/0000-0003-4314-6635												0034-4257	1879-0704				MAR 15	2012	118						259	272		10.1016/j.rse.2011.11.020						WOS:000300517700024		J	Sowa, Y; Berry, RM				Sowa, Yoshiyuki; Berry, Richard M.			Bacterial flagellar motor	QUARTERLY REVIEWS OF BIOPHYSICS												The bacterial flagellar motor is a reversible rotary nano-machine, about 45 nm in diameter, embedded in the bacterial cell envelope. It is powered by the flux of H+ or Na+ ions across the cytoplasmic membrane driven by an electrochemical gradient. the proton-motive force or the sodium-motive force. Each motor rotates a helical filament at several hundreds of revolutions per second (hertz). In many species, the motor switches direction stochastically, with the switching rates controlled by a network of sensory and signalling proteins, The bacterial flagellar motor was confirmed as a rotary motor in the early 1970s, the first direct observation of the function of a single molecular motor. However, because of the large size and complexity of the motor, much remains to be discovered, in particular, the structural details of the torque-generating mechanism. This review outlines what has been learned about the structure and function of the motor using a combination of genetics, single-molecule and biophysical techniques, with a focus on recent results and single-molecule techniques.																	0033-5835					MAY	2008	41	2					103	132		10.1017/S0033583508004691						WOS:000260202500001	18812014	J	Agranoff, D; Fernandez-Reyes, D; Papadopoulos, MC; Rojas, SA; Herbster, M; Loosemore, A; Tarelli, E; Sheldon, J; Schwenk, A; Pollak, R; Rayner, CFJ; Krishna, S				Agranoff, Dan; Fernandez-Reyes, Delmiro; Papadopoulos, Marios C.; Rojas, Sergio A.; Herbster, Mark; Loosemore, Alison; Tarelli, Edward; Sheldon, Jo; Schwenk, Achim; Pollak, Richard; Rayner, Charlotte F. J.; Krishna, Sarjeev			Identification of diagnostic markers for tuberculosis by proteomic fingerprinting of serum	LANCET												Background We investigated the potential of proteomic fingerprinting with mass spectrometric serum profiling, coupled with pattern recognition methods, to identify biomarkers that could improve diagnosis of tuberculosis. Methods We obtained serum proteomic profiles from patients with active tuberculosis and controls by surface-enhanced laser desorption ionisation time of flight mass spectrometry. A supervised machine-learning approach based on the support vector machine (SVM) was used to obtain a classifier that distinguished between the groups in two independent test sets. We used k-fold cross validation and random sampling of the SVM classifier to assess the classifier further. Relevant mass peaks were selected by correlational analysis and assessed with SVM We tested the diagnostic potential of candidate biomarkers, identified by peptide mass fingerprinting, by conventional immunoassays and SVM classifiers trained on these data. Findings Our SVM classifier discriminated the proteomic profile of patients with active tuberculosis from that of controls with overlapping clinical features. Diagnostic accuracy was 94% (sensitivity 93.5%, specificity 94.9%) for patients with tuberculosis and was unaffected by HIV status. A classifier trained on the 20 most informative peaks achieved diagnostic accuracy of 90%. From these peaks, two peptides (serum amyloid A protein and transthyretin) were identified and quantitated by immunoassay. Because these peptides reflect inflammatory states, we also quantitated neopterin and C reactive protein. Application of an SVM classifier using combinations of these values gave diagnostic accuracies of up to 84% for tuberculosis. Validation on a second, prospectively collected testing set gave similar accuracies using the whole proteomic signature and the 20 selected peaks. Using combinations of the four biomarkers, we achieved diagnostic accuracies of up to 78%. Interpretation The potential biomarkers for tuberculosis that we identified through proteomic fingerprinting and pattern recognition have a plausible biological connection with the disease and could be used to develop new diagnostic tests.					Krishna, Sanjeev/0000-0003-0066-0634; Fernandez-Reyes, Delmiro/0000-0001-5070-9198												0140-6736	1474-547X				SEP 16	2006	368	9540					1012	1021		10.1016/S0140-6736(06)69342-2						WOS:000240698700030	16980117	J	Hong, TP; Lee, CY				Hong, TP; Lee, CY			Induction of fuzzy rules and membership functions from training examples	FUZZY SETS AND SYSTEMS												Most fuzzy controllers and fuzzy expert systems must predefine membership functions and fuzzy inference rules to map numeric data into linguistic variable terms and to make fuzzy reasoning work. In this paper, we propose a general learning method as a framework for automatically deriving membership functions and fuzzy if-then rules from a set of given training examples to rapidly build a prototype fuzzy expert system. Based on the membership functions and the fuzzy rules derived, a corresponding fuzzy inference procedure to process inputs is also developed.																	0165-0114					NOV 25	1996	84	1					33	47		10.1016/0165-0114(95)00305-3						WOS:A1996VU55300003		J	Burke, EK; Gendreau, M; Hyde, M; Kendall, G; Ochoa, G; Ozcan, E; Qu, R				Burke, Edmund K.; Gendreau, Michel; Hyde, Matthew; Kendall, Graham; Ochoa, Gabriela; Oezcan, Ender; Qu, Rong			Hyper-heuristics: a survey of the state of the art	JOURNAL OF THE OPERATIONAL RESEARCH SOCIETY												Hyper-heuristics comprise a set of approaches that are motivated (at least in part) by the goal of automating the design of heuristic methods to solve hard computational search problems. An underlying strategic research challenge is to develop more generally applicable search methodologies. The term hyperheuristic is relatively new; it was first used in 2000 to describe heuristics to choose heuristics in the context of combinatorial optimisation. However, the idea of automating the design of heuristics is not new; it can be traced back to the 1960s. The definition of hyper-heuristics has been recently extended to refer to a search method or learning mechanism for selecting or generating heuristics to solve computational search problems. Two main hyper-heuristic categories can be considered: heuristic selection and heuristic generation. The distinguishing feature of hyper-heuristics is that they operate on a search space of heuristics (or heuristic components) rather than directly on the search space of solutions to the underlying problem that is being addressed. This paper presents a critical discussion of the scientific literature on hyper-heuristics including their origin and intellectual roots, a detailed account of the main types of approaches, and an overview of some related areas. Current research trends and directions for future research are also discussed.				Ozcan, Ender/B-4241-2008	Ozcan, Ender/0000-0003-0276-1391; Kendall, Graham/0000-0003-2006-5103; Ochoa, Gabriela/0000-0001-7649-5669												0160-5682	1476-9360				DEC	2013	64	12					1695	1724		10.1057/jors.2013.71						WOS:000327720200001		J	Xiao, X; Wu, ZC; Chou, KC				Xiao, Xuan; Wu, Zhi-Cheng; Chou, Kuo-Chen			iLoc-Virus: A multi-label learning classifier for identifying the subcellular localization of virus proteins with both single and multiple sites	JOURNAL OF THEORETICAL BIOLOGY												In the last two decades or so, although many computational methods were developed for predicting the subcellular locations of proteins according to their sequence information, it is still remains as a challenging problem, particularly when the system concerned contains both single- and multiple-location proteins. Also, among the existing methods, very few were developed specialized for dealing with viral proteins, those generated by viruses. Actually, knowledge of the subcellular localization of viral proteins in a host cell or virus-infected cell is very important because it is closely related to their destructive tendencies and consequences. In this paper, by introducing the "multi-label scale" and by hybridizing the gene ontology information with the sequential evolution information, a predictor called iLoc-Virus is developed. It can be utilized to identify viral proteins among the following six locations: (1) viral capsid, (2) host cell membrane, (3) host endoplasmic reticulum, (4) host cytoplasm, (5) host nucleus, and (6) secreted. The iLoc-Virus predictor not only can more accurately predict the location sites of viral proteins in a host cell, but also have the capacity to deal with virus proteins having more than one location. As a user-friendly web-server, iLoc-Virus is freely accessible to the public at http:// icpr.jci.edu.cn/bioinfo/iLoc-Virus. Meanwhile, a step-by-step guide is provided on how to use the web-server to get the desired results. Furthermore, for the user's convenience, the iLoc-Virus web-server also has the function to accept the batch job submission. It is anticipated that iLoc-Virus may become a useful high throughput tool for both basic research and drug development. (c) 2011 Elsevier Ltd. All rights reserved.				Chou, Kuo-Chen/A-8340-2009													0022-5193					SEP 7	2011	284	1					42	51		10.1016/j.jtbi.2011.06.005						WOS:000293672600006	21684290	J	Dudek, AZ; Arodz, T; Galvez, J				Dudek, AZ; Arodz, T; Galvez, J			Computational methods in developing quantitative structure-activity relationships (QSAR): A review	COMBINATORIAL CHEMISTRY & HIGH THROUGHPUT SCREENING												Virtual filtering and screening of combinatorial libraries have recently gained attention as methods complementing the high-throughput screening and combinatorial chemistry. These chemoinformatic techniques rely heavily on quantitative structure-activity relationship (QSAR) analysis, a field with established methodology and successful history. In this review, we discuss the computational methods for building QSAR models. We start with outlining their usefulness in high-throughput screening and identifying the general scheme of a QSAR model. Following, we focus on the methodologies in constructing three main components of QSAR model, namely the methods for describing the molecular structure of compounds. for selection of informative descriptors and for activity prediction. We present both the well-established methods as well as techniques recently introduced into the QSAR domain.																	1386-2073	1875-5402				MAR	2006	9	3					213	228		10.2174/138620706776055539						WOS:000235763400009	16533155	